{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0891387-8f77-4fbc-b9f8-48d61ce46335",
   "metadata": {},
   "source": [
    "### Datafiles preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8135d1f-4064-44e9-b31c-5fb093576d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fa557-55f3-4003-9399-094a92d81fc7",
   "metadata": {},
   "source": [
    "### Remove unwanted chars from the trial and test sentences to prevent the code from crashing\n",
    "and store the results in the files:\n",
    "- tsar2022_en_trial_none_no_noise.tsv \n",
    "- tsar2022_en_trial_gold_no_noise.tsv \n",
    "- tsar2022_en_test_none_no_noise.tsv\n",
    "- tsar2022_en_test_gold_no_noise.tsv\n",
    "\n",
    "These files will be used in all further processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ab214-4634-4c55-a627-e1b6871cb48d",
   "metadata": {},
   "source": [
    "##### trial dataset (for none and gold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96e1ffa-66a8-400e-a3a4-2302c4c621c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_none.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "\n",
    "# remove character combinations starting with # and quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# save the updated df to a new tsv\n",
    "new_filename = \"./data/trial/tsar2022_en_trial_none_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb77717-13ba-4d9d-a6d3-df5e113795c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_gold.tsv\"\n",
    "\n",
    "# define column names \n",
    "col_names = [\"sentence\", \"complex_word\"] + [f\"extra_col{i}\" for i in range(1, 27)]\n",
    "\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=col_names)\n",
    "\n",
    "# remove character combinations starting with # and quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# save the updated df to a new tsv\n",
    "new_filename = \"./data/trial/tsar2022_en_trial_gold_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a5930-4ad7-4800-ba2e-e9ed4fc580a5",
   "metadata": {},
   "source": [
    "##### test dataset (for none and gold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a637e89b-632d-4acc-b2c4-2fa06d4ad037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_none.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "\n",
    "# remove character combinations starting with # and quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# write the updated df to a new tsv\n",
    "new_filename = \"./data/test/tsar2022_en_test_none_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63233c91-570f-47ce-8556-d4b04f11d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_gold.tsv\"\n",
    "\n",
    "# define column names\n",
    "col_names = [\"sentence\", \"complex_word\"] + [f\"extra_col{i}\" for i in range(1, 27)]\n",
    "\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=col_names)\n",
    "\n",
    "# remove character combinations starting with # and quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# write the data to a new tsv\n",
    "new_filename = \"./data/test/tsar2022_en_test_gold_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea73248-cc1f-4e5a-b815-f253fa2e4360",
   "metadata": {},
   "source": [
    "### EFLLEX file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a8ed0-f198-4e99-b7e4-b66fd369dbb8",
   "metadata": {},
   "source": [
    "#### remove unwanted headers and columns from EFLLEX file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a8d03b-875d-4a88-9158-694a382e3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cefr_efllex/EFLLex_NLP4J_ORIG.tsv', 'r', encoding='utf-8') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    next(tsv_reader, None)  # skip the headers\n",
    "    rows = [row[:-106] for row in tsv_reader]  # remove the last 105 columns\n",
    "\n",
    "\n",
    "with open('./cefr_efllex/EFLLex_trimmed.tsv', 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    tsv_writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05acda9-d708-4920-9790-52ac88168946",
   "metadata": {},
   "source": [
    "#### Option 1: assign most frequent cefr level to word in EFLLEX dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fe86464-e0fe-4180-be14-9640fd4033ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define column names\n",
    "column_names = ['word', 'pos tag', 'A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# load tsv file\n",
    "df = pd.read_csv('./cefr_efllex/EFLLex_trimmed.tsv', sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# define CEFR levels\n",
    "cefr_levels = ['A1', 'A2', 'B1', 'B2', 'C1',]\n",
    "\n",
    "# extract column names with the highest values and add the result to a new column\n",
    "df['Highest CEFR'] = df[cefr_levels].idxmax(axis=1)\n",
    "\n",
    "# create a new df with the needed columns\n",
    "df_new = df[['word', 'pos tag', 'mostfreq CEFR']]\n",
    "\n",
    "\n",
    "# write the new df to a new tsv \n",
    "df_new.to_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header = False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5746a-9001-4817-a20b-d3d24a42b61d",
   "metadata": {},
   "source": [
    "#### Option 2: take weighted average to assign cefr level to word in efflex dataset.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "985af180-dee5-41c2-a075-b0bf998cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define column names\n",
    "column_names = ['word', 'pos tag', 'A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# load tsv file\n",
    "df = pd.read_csv('./cefr_efllex/EFLLex_trimmed.tsv', sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# define CEFR levels \n",
    "cefr_levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5}\n",
    "\n",
    "# create a new df for the weighted frequencies\n",
    "df_weighted = pd.DataFrame()\n",
    "\n",
    "# calculate the weighted frequencies for each CEFR level\n",
    "for level in cefr_levels:\n",
    "    df_weighted[level] = df[level] * mapping[level]\n",
    "\n",
    "# sum the weighted frequencies across the CEFR levels for each word\n",
    "df['Weighted Sum'] = df_weighted.sum(axis=1)\n",
    "\n",
    "# sum the frequencies across the CEFR levels for each word\n",
    "df['Frequency Sum'] = df[cefr_levels].sum(axis=1)\n",
    "\n",
    "# calculate the weighted average for each word\n",
    "df['Weighted CEFR'] = df['Weighted Sum'] / df['Frequency Sum']\n",
    "\n",
    "# create a new df with the needed columns\n",
    "df_new = df[['word', 'pos tag', 'Weighted CEFR']]\n",
    "\n",
    "# write the new df to a new tsv\n",
    "df_new.to_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header = False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f232bf-c817-48de-8e21-dc43bdb2691e",
   "metadata": {},
   "source": [
    "### CEFRJ file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043da03-ae7f-4a38-93aa-1ea0d88a8497",
   "metadata": {},
   "source": [
    "#### Concatenate cefrj files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706caba-1d5a-4cff-8d5d-f78ada38b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv files and remove the header row\n",
    "df1 = pd.read_csv('./cefrj/cefrj_1.csv', skiprows=1, header=None)\n",
    "df2 = pd.read_csv('./cefrj/cefrj_2.csv', skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79c199-805d-4861-af31-d6d746d52d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first three columns\n",
    "df1 = df1.iloc[:, :3]\n",
    "df2 = df2.iloc[:, :3]\n",
    "\n",
    "# concatenate the df's\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# write the combined data to a new tsv \n",
    "df.to_csv('./cefrj/cefrj_all.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e88e1-24e3-466a-8ea8-6f99da4021eb",
   "metadata": {},
   "source": [
    "#### Map existing pos tags to Treebank tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e1117-98c4-4a6b-aece-ee4cb39748c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# map existing PoS tags to Treebank tags\n",
    "def map_to_treebank_tag(existing_tag):\n",
    "    if existing_tag == 'adjective':\n",
    "        return 'JJ'  # Adjective\n",
    "    elif simple_tag == 'verb':\n",
    "        return 'VB'  # Verb\n",
    "    elif simple_tag == 'noun':\n",
    "        return 'NN'  # Noun\n",
    "    elif simple_tag == 'adverb':\n",
    "        return 'RB'  # Adverb\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# open existing cefrj file\n",
    "with open('./cefrj/cefrj_all.tsv', 'r') as infile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "\n",
    "    # open new file\n",
    "    with open('./cefrj/cefrj_all_treebank.tsv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "        # loop over rows in the existing cefrj file\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            pos_tag = row[1]\n",
    "            cefr_level = row[2]\n",
    "\n",
    "            # map the PoS tag to the treebank PoS tags\n",
    "            treebank_pos = map_to_treebank_tag(pos_tag)\n",
    "\n",
    "            # if the tag is one of the four Treebank tags, write to the new file\n",
    "            if treebank_pos is not None:\n",
    "                writer.writerow([word, treebank_pos, cefr_level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e445058-c8e4-4d7b-8cc3-327d595e3ef8",
   "metadata": {},
   "source": [
    "#### Map cefr levels to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09088a4e-a93a-4306-b78e-f7c0a685105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the data\n",
    "df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None)\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# apply the mapping to the third column\n",
    "df[2] = df[2].map(cefr_level_mapping)\n",
    "\n",
    "# write the updated df to a new tsv\n",
    "df.to_csv('./cefr_all/cefrj_num.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0847b-2dca-4802-bfc8-1ab5f9d79a2b",
   "metadata": {},
   "source": [
    "### Uchida file (CEFR_LS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c49af-03a7-4d0b-a2f9-6fb6efe3e750",
   "metadata": {},
   "source": [
    "#### Remove redundant columns from the uchida file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "306e3789-fc54-40e2-85d4-c536ad930d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the df\n",
    "df = pd.read_csv('./cefr_ls/uchida.tsv', sep='\\t', header=None, usecols=range(24))\n",
    "\n",
    "# remove redundant columns \n",
    "remove_cols = [5, 8, 11, 14, 17, 20, 23]\n",
    "df.drop(df.columns[remove_cols], axis=1, inplace=True)\n",
    "\n",
    "# write the df to a new tsv\n",
    "df.to_csv('./cefr_ls/uchida_trimmed.tsv', sep='\\t', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d0f8-e705-41ba-b0b4-63cb365b3f58",
   "metadata": {},
   "source": [
    "#### Parse the sentences to get the pos tags of the words (for example, this is needed in case of same words with different CEFR levels based on their pos in the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bffeb6b-a3ea-443f-92ca-c2421d7e0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert NLTK POS tags \n",
    "def convert_pos_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'JJ'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'VB'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'NN'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'RB'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# read the tsv file\n",
    "df = pd.read_csv('./cefr_ls/uchida_trimmed.tsv', sep='\\t', header=None)\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define a new column for the POS tag\n",
    "df[17] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row[0]\n",
    "    target_word = row[1]\n",
    "\n",
    "    # tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # get PoS tags for the words\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # lemmatize words and compare with the target word\n",
    "    for word, tag in tagged_words:\n",
    "        tag = convert_pos_tag(tag)  # convert the POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')  # 'n' is used as a default PoS if the word cannot be pos tagged\n",
    "        if lemma == target_word and tag:\n",
    "            df.at[index, 17] = tag\n",
    "\n",
    "# filter the df to only include rows where the new POS column is not empty\n",
    "df = df[df[17] != '']\n",
    "\n",
    "# remove the first column\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "# insert the values from the last column to a new column after the first one\n",
    "df.insert(1, 'POS', df[17])\n",
    "\n",
    "# iterate through the positions of the columns and insert PoS columns \n",
    "insert_positions = [4, 7, 10, 13, 16, 19, 22]\n",
    "for i, pos in enumerate(insert_positions):\n",
    "    df.insert(pos, f'POS_copy_{i+1}', df['POS'])\n",
    "\n",
    "# remove the last column\n",
    "df = df.drop(columns=[17])\n",
    "\n",
    "# write the df to a new tsv\n",
    "df.to_csv('./cefr_ls/uchida_trimmed_parsed.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a5dc6-5654-4f57-a653-e3b4c571a87a",
   "metadata": {},
   "source": [
    "#### Standardize format into three colums: word, pos, cefr level, and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ebf6a9e-8ac8-4801-af54-f51c64359e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the tsv file\n",
    "df = pd.read_csv('./cefr_ls/uchida_trimmed_parsed.tsv', sep='\\t', header=None)\n",
    "\n",
    "# create an empty df for the standardized format\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# iterate over each three columns\n",
    "for i in range(0, df.shape[1], 3):\n",
    "    # select the three columns\n",
    "    threecols_df = df.iloc[:, i:i+3]\n",
    "    \n",
    "    # reset the column names \n",
    "    threecols_df.columns = range(3)\n",
    "    \n",
    "    # append to the final df\n",
    "    final_df = pd.concat([final_df, threecols_df])\n",
    "\n",
    "# reset the index of the final df\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# remove rows where less than two cells are filled (e.g., as a word without a CEFR level or the other way around is useless)\n",
    "final_df = final_df.dropna(thresh=2)\n",
    "\n",
    "# remove duplicates based on the first and second column\n",
    "final_df = final_df.drop_duplicates(subset=[0, 1])\n",
    "\n",
    "# write the final df to a new tsv\n",
    "final_df.to_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d8bf3-9924-4a31-baec-0ad2e387d2a3",
   "metadata": {},
   "source": [
    "#### Map cefr levels to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b74c2f29-45e0-4e63-93e9-0d9c2812a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the data\n",
    "df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None)\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# apply the mapping to the third column\n",
    "df[2] = df[2].map(cefr_level_mapping)\n",
    "\n",
    "# save the updated df to a new tsv\n",
    "df.to_csv('./cefr_all/uchida_num.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2f54c-e70c-42a9-b2f4-9b0d8fe51d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cbdd2c3-d7f0-418a-94dc-a4bcde7ba26a",
   "metadata": {},
   "source": [
    "### Concatenate all CEFR datasets to get a higher coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6df41d7-876c-4381-8f79-d0679dedd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the data\n",
    "df1 = pd.read_csv('./cefr_all/uchida_num.tsv', sep='\\t', header=None)\n",
    "df2 = pd.read_csv('./cefr_all/cefrj_num.tsv', sep='\\t', header=None)\n",
    "df3 = pd.read_csv('./cefr_all/EFLLex_weighted.tsv', sep='\\t', header=None)\n",
    "\n",
    "# concatenate the df's\n",
    "result_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# save the concatenated df to a new tsv\n",
    "result_df.to_csv('./cefr_all/cefr_all.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83251834-baa1-4849-a297-74d29c3a7773",
   "metadata": {},
   "source": [
    "#### Remove 'full' duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed2312d-a737-4728-99cf-2d92ad587797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the combined data\n",
    "df = pd.read_csv('./cefr_all/cefr_all.tsv', sep='\\t', header=None)\n",
    "\n",
    "# remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# save the resulting df to a new tsv\n",
    "df.to_csv('./cefr_all/cefr_all_no_duplicates.tsv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e568b-af72-4fc4-861a-f8abf955ce7b",
   "metadata": {},
   "source": [
    "#### Group the data by the 'word' and 'pos' columns, and then calculate the average of the cefr_level_mapping column for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bd560f8-3987-438c-aca3-d5f7b5b69e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the data without 'full' duplicates\n",
    "df = pd.read_csv('./cefr_all/cefr_all_no_duplicates.tsv', sep='\\t', header=None)\n",
    "\n",
    "# group by 'word' and 'pos' columns, and then calculate the average of the cefr_level_mapping column for each group\n",
    "df_grouped = df.groupby([0, 1])[2].mean().reset_index()\n",
    "\n",
    "# save the resulting df to a new tsv\n",
    "df_grouped.to_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
