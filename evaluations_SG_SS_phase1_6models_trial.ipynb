{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations for SG and SS step phase 1, all 6 models, trial dataset.\n",
    " This file corresponds to the results presented in sections 4.1 and 4.2 of the thesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'bert-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'bert-base-uncased'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertbase_maskedsentenceonly exported to csv in path './predictions/trial/SG_bertbase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertbase_maskedsentenceonly.tsv --output_file ./output/trial/SG_bertbase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertbase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertbase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.0777\n",
    "MAP@5 = 0.0546\n",
    "MAP@10 = 0.0327\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.3\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_bertbase exported to csv in path './predictions/trial/SG_incl_orig_sentence_bertbase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_incl_orig_sentence_bertbase.tsv --output_file ./output/trial/SG_incl_orig_sentence_bertbase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_bertbase.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_bertbase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2243\n",
    "MAP@10 = 0.1289\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_bertbase exported to csv in path './predictions/trial/SS_phase1_bertbase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_bertbase.tsv --output_file ./output/trial/SS_phase1_bertbase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_bertbase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_bertbase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2253\n",
    "MAP@10 = 0.1298\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'bert-large':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'bert-large-uncased'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertlarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_bertlarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertlarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_bertlarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertlarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertlarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0666\n",
    "MAP@5 = 0.044\n",
    "MAP@10 = 0.0259\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_bertlarge exported to csv in path './predictions/trial/SG_incl_orig_sentence_bertlarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_incl_orig_sentence_bertlarge.tsv --output_file ./output/trial/SG_incl_orig_sentence_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_bertlarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3055\n",
    "MAP@5 = 0.1873\n",
    "MAP@10 = 0.1278\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_bertlarge exported to csv in path './predictions/trial/SS_phase1_bertlarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_bertlarge.tsv --output_file ./output/trial/SS_phase1_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_bertlarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2026\n",
    "MAP@10 = 0.1355\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'roberta-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'roberta-base'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertabase_maskedsentenceonly exported to csv in path './predictions/trial/SG_robertabase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertabase_maskedsentenceonly.tsv --output_file ./output/trial/SG_robertabase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertabase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertabase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0666\n",
    "MAP@5 = 0.0709\n",
    "MAP@10 = 0.0434\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.7\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_robertabase exported to csv in path './predictions/trial/SG_incl_orig_sentence_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.24\n",
    "MAP@10 = 0.1411\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_robertabase exported to csv in path './predictions/trial/SS_phase1_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_robertabase.tsv --output_file ./output/trial/SS_phase1_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.2449\n",
    "MAP@10 = 0.1546\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'roberta-large':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'roberta-large'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertalarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_robertalarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertalarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_robertalarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertalarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertalarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2999\n",
    "MAP@5 = 0.218\n",
    "MAP@10 = 0.1177\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_robertalarge exported to csv in path './predictions/trial/SG_incl_orig_sentence_robertalarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_incl_orig_sentence_robertalarge.tsv --output_file ./output/trial/SG_incl_orig_sentence_robertalarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_robertalarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_robertalarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2944\n",
    "MAP@5 = 0.2016\n",
    "MAP@10 = 0.1067\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_robertalarge exported to csv in path './predictions/trial/SS_phase1_robertalarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_robertalarge.tsv --output_file ./output/trial/SS_phase1_robertalarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_robertalarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_robertalarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2944\n",
    "MAP@5 = 0.2056\n",
    "MAP@10 = 0.1159\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'electra-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'google/electra-base-generator'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electrabase_maskedsentenceonly exported to csv in path './predictions/trial/SG_electrabase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electrabase_maskedsentenceonly.tsv --output_file ./output/trial/SG_electrabase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electrabase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_electrabase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0888\n",
    "MAP@5 = 0.0533\n",
    "MAP@10 = 0.0398\n",
    "\n",
    "Potential@3 = 0.4\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.1\n",
    "Accuracy@3@top_gold_1 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_electrabase exported to csv in path './predictions/trial/SG_incl_orig_sentence_electrabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_incl_orig_sentence_electrabase.tsv --output_file ./output/trial/SG_incl_orig_sentence_electrabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_electrabase.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_electrabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3\n",
    "MAP@5 = 0.233\n",
    "MAP@10 = 0.148\n",
    "\n",
    "Potential@3 = 0.5\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_electrabase exported to csv in path './predictions/trial/SS_phase1_electrabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_electrabase.tsv --output_file ./output/trial/SS_phase1_electrabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_electrabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_electrabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3\n",
    "MAP@5 = 0.233\n",
    "MAP@10 = 0.148\n",
    "\n",
    "Potential@3 = 0.5\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For model 'electralarge':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'google/electra-large-generator'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electralarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_electralarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electralarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_electralarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electralarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_electralarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0444\n",
    "MAP@5 = 0.0306\n",
    "MAP@10 = 0.0254\n",
    "\n",
    "Potential@3 = 0.2\n",
    "Potential@5 = 0.3\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.1\n",
    "Accuracy@3@top_gold_1 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_incl_orig_sentence_electralarge exported to csv in path './predictions/trial/SG_incl_orig_sentence_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_incl_orig_sentence_electralarge.tsv --output_file ./output/trial/SG_incl_orig_sentence_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_incl_orig_sentence_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_incl_orig_sentence_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2576\n",
    "MAP@10 = 0.1546\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_electralarge exported to csv in path './predictions/trial/SS_phase1_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_electralarge.tsv --output_file ./output/trial/SS_phase1_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2686\n",
    "MAP@10 = 0.1616\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accumulated scores, the best 2 models proceed to SS phase 2 (refer to table 4.4 in section 4.2 in the thesis):\n",
    "1. SS_phase1_robertabase (accum. score 5.1161).\n",
    "2. SS_phase1_electralarge (accum. score 4.9413)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
