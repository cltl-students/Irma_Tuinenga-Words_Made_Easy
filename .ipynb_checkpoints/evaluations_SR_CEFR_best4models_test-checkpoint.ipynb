{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## TEST set: Substitute Ranking (SR) step with CEFR levels:\n",
    "#### Performed on best 3 models after SS step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b67d20-7b08-4c5b-8967-82e833aee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f341d72-bd0a-431f-82d9-8a073b23c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde89e63-5e5e-46f4-b5cc-abe282ed9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map spaCy PoS tags to WordNet PoS tags\n",
    "def map_pos_spacy_wordnet(pos_spacy):\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    return pos_map.get(pos_spacy, wn.NOUN) # default to NOUN if pos_spacy does not exist in the dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7a2d-bd38-4e24-82a2-f56942a528b4",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option2bHyps2first_robertabase (No. 1 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e892ae-2302-44cf-b3e9-8a056d30cd82",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9584832e-ea68-4e27-acc6-d1f6da9ca3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "\n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "            \n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    # print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b7e9b-8fa6-4662-aa1a-03216fd6bc5d",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "976c7f4a-2781-4cbd-be67-06f8003be519",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4408\n",
    "\n",
    "MAP@3 = 0.2918\n",
    "MAP@5 = 0.2316\n",
    "MAP@10 = 0.1565\n",
    "\n",
    "Potential@3 = 0.7419\n",
    "Potential@5 = 0.8655\n",
    "Potential@10 = 0.9327\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1854\n",
    "Accuracy@2@top_gold_1 = 0.3145\n",
    "Accuracy@3@top_gold_1 = 0.387"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af888b06-2f22-4677-ae56-f35736772843",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cc1099-766b-4e58-a3e9-5792dac5ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08f7fe-2b00-4fa7-942d-77ffe7e649f6",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99144a35-c82c-4534-bcff-a008c7d0740e",
   "metadata": {},
   "source": [
    "========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4327\n",
    "\n",
    "MAP@3 = 0.3057\n",
    "MAP@5 = 0.2414\n",
    "MAP@10 = 0.1591\n",
    "\n",
    "Potential@3 = 0.7526\n",
    "Potential@5 = 0.8575\n",
    "Potential@10 = 0.9327\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1747\n",
    "Accuracy@2@top_gold_1 = 0.2795\n",
    "Accuracy@3@top_gold_1 = 0.3763"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2708c-33c5-46df-9143-86aa638756f8",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f21881-970b-4e66-991d-451700f9edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed32c6-dacf-4417-9098-dca1a237a71a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e59c3d4-287e-4b7f-8980-28a34973e34c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4005\n",
    "\n",
    "MAP@3 = 0.2864\n",
    "MAP@5 = 0.2305\n",
    "MAP@10 = 0.1548\n",
    "\n",
    "Potential@3 = 0.7526\n",
    "Potential@5 = 0.8655\n",
    "Potential@10 = 0.9327\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1639\n",
    "Accuracy@2@top_gold_1 = 0.2983\n",
    "Accuracy@3@top_gold_1 = 0.4086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e961e-5933-4f47-8784-2ef53842f53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a62fd6-9ed7-4b5f-941c-2807126b7737",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82b17c3-3b0a-4e9f-925e-f2512d0dcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "      # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fd602-9386-49b4-944f-fbbed6b37dab",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80911004-e7f8-4464-bae8-cb24ef6e2772",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.387\n",
    "\n",
    "MAP@3 = 0.2776\n",
    "MAP@5 = 0.2247\n",
    "MAP@10 = 0.153\n",
    "\n",
    "Potential@3 = 0.7338\n",
    "Potential@5 = 0.8629\n",
    "Potential@10 = 0.9327\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1424\n",
    "Accuracy@2@top_gold_1 = 0.2876\n",
    "Accuracy@3@top_gold_1 = 0.3978"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42eb75-8f45-4866-9ba7-66d726522757",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7905f849-1c41-429f-89d5-d8fe6239890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2eCEFR_all_robertabase'\n",
      "\n",
      "On average, there were 6.293010752688172 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no1_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no1_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf96e5-e9a2-4da4-b7c1-b9beb97412cf",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no1_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/test/SS_no1_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95c804ae-278b-41f5-b63a-5f219a841e9d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3978\n",
    "\n",
    "MAP@3 = 0.2656\n",
    "MAP@5 = 0.2205\n",
    "MAP@10 = 0.1512\n",
    "\n",
    "Potential@3 = 0.7311\n",
    "Potential@5 = 0.8575\n",
    "Potential@10 = 0.9327\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1478\n",
    "Accuracy@2@top_gold_1 = 0.2849\n",
    "Accuracy@3@top_gold_1 = 0.379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112ed4e-7146-4b11-9164-de8ec29fb22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f616167-c318-4cf9-a5ce-d9d6b3fcd128",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option1Synsfirst_robertabase (No. 2 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531ac67-bf07-4c89-8c88-6ea6e6304d2e",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bcbc3b-a13e-4db5-a7cc-c97fb4f76417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "        \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d75955-87e2-4694-bf1e-4cc13548c1c2",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebbf7dd3-5fbc-4190-a22d-046c8b645d92",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4865\n",
    "\n",
    "MAP@3 = 0.3203\n",
    "MAP@5 = 0.2554\n",
    "MAP@10 = 0.1709\n",
    "\n",
    "Potential@3 = 0.7661\n",
    "Potential@5 = 0.8844\n",
    "Potential@10 = 0.9543\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2123\n",
    "Accuracy@2@top_gold_1 = 0.3306\n",
    "Accuracy@3@top_gold_1 = 0.4327"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef6c70-b21a-49e3-9194-f1b4498f4e46",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3182cbb2-8877-4a24-a035-68b7be162cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "          # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cb519-02c0-41a5-9f40-1ef9d9b3ce1a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac470774-b059-41f5-a655-96ca4a873ad9",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4677\n",
    "\n",
    "MAP@3 = 0.3413\n",
    "MAP@5 = 0.2763\n",
    "MAP@10 = 0.1761\n",
    "\n",
    "Potential@3 = 0.7956\n",
    "Potential@5 = 0.9005\n",
    "Potential@10 = 0.9543\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2016\n",
    "Accuracy@2@top_gold_1 = 0.3413\n",
    "Accuracy@3@top_gold_1 = 0.4516"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfcbde-63a7-466c-9c50-dbb29900a3fc",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b12723c-3177-42b7-8871-b89a7683136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "         # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194f890-6138-4ff4-9caf-883e824e6f71",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/test/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a399c20-b120-4d78-abe8-f1716a9f220b",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4516\n",
    "\n",
    "MAP@3 = 0.3149\n",
    "MAP@5 = 0.2537\n",
    "MAP@10 = 0.1694\n",
    "\n",
    "Potential@3 = 0.7768\n",
    "Potential@5 = 0.8978\n",
    "Potential@10 = 0.9543\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1827\n",
    "Accuracy@2@top_gold_1 = 0.3118\n",
    "Accuracy@3@top_gold_1 = 0.4489"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec5807-b19a-4222-9037-92cdcd98c7b2",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37274071-fb1b-41ea-87aa-9522de899809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183f2f9-ab62-4de8-a7d1-3e8c9caa3b3e",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5739ebc6-54c9-4754-8254-20f4fd645aab",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4166\n",
    "\n",
    "MAP@3 = 0.2939\n",
    "MAP@5 = 0.2413\n",
    "MAP@10 = 0.165\n",
    "\n",
    "Potential@3 = 0.7607\n",
    "Potential@5 = 0.8897\n",
    "Potential@10 = 0.9543\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1612\n",
    "Accuracy@2@top_gold_1 = 0.293\n",
    "Accuracy@3@top_gold_1 = 0.4139"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83457260-d8e5-4720-902c-4e1a9d48dd05",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837d1adb-bd55-4869-bc8f-5fa4c5926447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "         # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no2_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no2_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f45d-0dda-4f25-8ea4-ce74c595e3d9",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no2_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/test/SS_no2_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bb9515c-b55e-45ea-8fb5-fe8a8a9f42c8",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4112\n",
    "\n",
    "MAP@3 = 0.2834\n",
    "MAP@5 = 0.2351\n",
    "MAP@10 = 0.1623\n",
    "\n",
    "Potential@3 = 0.7607\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9543\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1559\n",
    "Accuracy@2@top_gold_1 = 0.3064\n",
    "Accuracy@3@top_gold_1 = 0.4166\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f4e65-7218-4701-8da8-5082e88958ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48c0466-09ab-4b24-a751-8f091e626249",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_robertabase (No. 3 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e47a95-bf30-4b15-9a9c-f5b075bb6dcc",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fca0d04-155b-490c-984c-9f1c35f03896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "        \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7b543-daaa-4940-a548-7e948b71743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2541df8d-0b0d-4b74-a794-c12f6d55b831",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4838\n",
    "\n",
    "MAP@3 = 0.3437\n",
    "MAP@5 = 0.2704\n",
    "MAP@10 = 0.1824\n",
    "\n",
    "Potential@3 = 0.7822\n",
    "Potential@5 = 0.9032\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2069\n",
    "Accuracy@2@top_gold_1 = 0.3279\n",
    "Accuracy@3@top_gold_1 = 0.4274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f119e-6782-411e-a2a9-311880390877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656259c9-8c8b-4c2c-88f7-6b4812ce9386",
   "metadata": {},
   "source": [
    "#### for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f822a1c-269e-4508-af6a-f9bdd3f71571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "          # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "        \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955588b1-4d8a-43cf-973d-e6901edf9ed7",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "565a5d28-0f34-40ce-8dbe-182ef27c1715",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4919\n",
    "\n",
    "MAP@3 = 0.372\n",
    "MAP@5 = 0.2979\n",
    "MAP@10 = 0.1908\n",
    "\n",
    "Potential@3 = 0.8252\n",
    "Potential@5 = 0.9193\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1935\n",
    "Accuracy@2@top_gold_1 = 0.3387\n",
    "Accuracy@3@top_gold_1 = 0.4327\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226194b3-7739-4ec1-9607-57fdc65ceab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216606bf-07ba-490d-bee1-cf692564c177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c86be84-3a42-4c9a-9ec3-91ee39d1b888",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e27321-a677-4184-a3d3-6fe6d9b7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "         # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde1cd5-ed4c-4246-a04c-2c1a9c717041",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c2a61ed-3f0f-4ea1-ad61-50f0a45d8b75",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4543\n",
    "\n",
    "MAP@3 = 0.3322\n",
    "MAP@5 = 0.2678\n",
    "MAP@10 = 0.1804\n",
    "\n",
    "Potential@3 = 0.7661\n",
    "Potential@5 = 0.8951\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1666\n",
    "Accuracy@2@top_gold_1 = 0.301\n",
    "Accuracy@3@top_gold_1 = 0.4139"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56de14-b948-41e4-9724-055372c88fdc",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002bb78b-fc32-461e-b082-eb36d97e982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "        \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8152afe-4740-4d42-8c86-d70aae3e415c",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a2109b-4842-4bdc-a1f7-01d6a8740052",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4596\n",
    "\n",
    "MAP@3 = 0.324\n",
    "MAP@5 = 0.262\n",
    "MAP@10 = 0.1786\n",
    "\n",
    "Potential@3 = 0.7526\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1666\n",
    "Accuracy@2@top_gold_1 = 0.2822\n",
    "Accuracy@3@top_gold_1 = 0.387\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfb49f-5de4-4c0a-a5ba-7f8fe2c16b23",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d440af1-0a4a-4932-b23f-eec265dd2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "         # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "        \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no3_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/test/SS_no3_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66779f-6495-4e16-a1df-fc3ba813bd07",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no3_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/test/SS_no3_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4afe97fe-c8ac-45b5-9b6d-bf18631e62a0",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4327\n",
    "\n",
    "MAP@3 = 0.3101\n",
    "MAP@5 = 0.2553\n",
    "MAP@10 = 0.175\n",
    "\n",
    "Potential@3 = 0.7768\n",
    "Potential@5 = 0.9059\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.3064\n",
    "Accuracy@3@top_gold_1 = 0.3978\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad677a8f-11f5-445f-8b63-e3087f753db0",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_electralarge (No. 4 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489a5ed-eeaa-41e2-8786-ea96038bc67e",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9800c11-7cfe-4706-aff0-52a544668efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2aCEFR_J_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "\n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "            \n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    # print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2aCEFR_J_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bc1ab-5226-44f7-a308-a3f52c963f09",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv --output_file ./output/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40ebd1f-c00d-4af4-9c41-1af040e8ccd0",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SS_no4_SR_option2aCEFR_J_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4327\n",
    "\n",
    "MAP@3 = 0.3151\n",
    "MAP@5 = 0.2548\n",
    "MAP@10 = 0.1743\n",
    "\n",
    "Potential@3 = 0.801\n",
    "Potential@5 = 0.8844\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.2983\n",
    "Accuracy@3@top_gold_1 = 0.3978"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329afd4-77cd-497e-97ba-cd3ce723f3d7",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8577afe-2625-408c-88f2-5cafbb18fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2bCEFR_ls_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2bCEFR_ls_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae1b31-1ec8-44ad-8dec-a065400fa12a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv --output_file ./output/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80dceaa8-1fc5-4470-b195-054c9a166124",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SS_no4_SR_option2bCEFR_ls_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.465\n",
    "\n",
    "MAP@3 = 0.3457\n",
    "MAP@5 = 0.2843\n",
    "MAP@10 = 0.1838\n",
    "\n",
    "Potential@3 = 0.8064\n",
    "Potential@5 = 0.8897\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.3225\n",
    "Accuracy@3@top_gold_1 = 0.4005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac309f-d84b-46e9-96dc-1cc8052b581a",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "215c80a6-f08d-4634-9075-20213caea42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46e263-33d8-4e2c-ab7b-e5b17968f5e4",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv --output_file ./output/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d19ea9d7-c9a7-4694-8a3f-15039d36f48c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3951\n",
    "\n",
    "MAP@3 = 0.3112\n",
    "MAP@5 = 0.2571\n",
    "MAP@10 = 0.1742\n",
    "\n",
    "Potential@3 = 0.7876\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1505\n",
    "Accuracy@2@top_gold_1 = 0.3037\n",
    "Accuracy@3@top_gold_1 = 0.4032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c1054-9b80-4263-9992-25ec843b7498",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ab3964f-28f8-4f58-8a11-32d02cc14a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2dCEFR_efl_weigthed_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "      # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2dCEFR_efl_weigthed_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add26b5-46f0-48a8-b85c-9ed7097b569b",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv --output_file ./output/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bc9c2e7-4688-428f-b088-aba45efb0be1",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3709\n",
    "\n",
    "MAP@3 = 0.2919\n",
    "MAP@5 = 0.2442\n",
    "MAP@10 = 0.1695\n",
    "\n",
    "Potential@3 = 0.7715\n",
    "Potential@5 = 0.8817\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1317\n",
    "Accuracy@2@top_gold_1 = 0.2715\n",
    "Accuracy@3@top_gold_1 = 0.3655"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d50b50-79d4-40ad-81fb-a3d37a1e4366",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "816d52a4-cf13-45f2-bd33-a1a0b341ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2eCEFR_all_electralarge'\n",
      "\n",
      "On average, there were 5.830645161290323 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "        # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos_list = [token.pos_ for token in doc if token.text == substitute]  \n",
    "        if pos_list:  \n",
    "            pos = pos_list[0]  \n",
    "        else:  \n",
    "            pos = None  \n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_no4_SR_option2eCEFR_all_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/test/SS_no4_SR_option2eCEFR_all_electralarge'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a2e18-51ff-41e3-b8b1-2bfd882568df",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_no4_SR_option2eCEFR_all_electralarge.tsv --output_file ./output/test/SS_no4_SR_option2eCEFR_all_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b19d04d1-87d8-4c54-81fc-1807fb6aa66d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_no4_SR_option2eCEFR_all_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SS_no4_SR_option2eCEFR_all_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.387\n",
    "\n",
    "MAP@3 = 0.2821\n",
    "MAP@5 = 0.2379\n",
    "MAP@10 = 0.1674\n",
    "\n",
    "Potential@3 = 0.7715\n",
    "Potential@5 = 0.8763\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1478\n",
    "Accuracy@2@top_gold_1 = 0.2634\n",
    "Accuracy@3@top_gold_1 = 0.379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582c9f1-85f2-4c76-a3b3-6aeb82c4e276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82716000-1426-4336-9350-1a0bd0c9cd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461addbe-daa9-40be-b8e7-b0f878d307c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b93213-97d1-40af-a975-84d8a2e4caed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bcace-cb85-479b-be1e-4ba3aeac068a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
