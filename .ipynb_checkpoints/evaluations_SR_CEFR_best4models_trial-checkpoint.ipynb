{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## trial set: Substitute Ranking (SR) step with CEFR levels:\n",
    "#### Performed on best 3 models after SS step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b67d20-7b08-4c5b-8967-82e833aee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f341d72-bd0a-431f-82d9-8a073b23c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde89e63-5e5e-46f4-b5cc-abe282ed9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map spaCy PoS tags to WordNet PoS tags\n",
    "def map_pos_spacy_wordnet(pos_spacy):\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    return pos_map.get(pos_spacy, wn.NOUN) # default to NOUN if pos_spacy does not exist in the dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7a2d-bd38-4e24-82a2-f56942a528b4",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option2bHyps2first_robertabase (No. 1 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e892ae-2302-44cf-b3e9-8a056d30cd82",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9584832e-ea68-4e27-acc6-d1f6da9ca3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b7e9b-8fa6-4662-aa1a-03216fd6bc5d",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "976c7f4a-2781-4cbd-be67-06f8003be519",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2666\n",
    "MAP@5 = 0.253\n",
    "MAP@10 = 0.1532\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af888b06-2f22-4677-ae56-f35736772843",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cc1099-766b-4e58-a3e9-5792dac5ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08f7fe-2b00-4fa7-942d-77ffe7e649f6",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99144a35-c82c-4534-bcff-a008c7d0740e",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2999\n",
    "MAP@5 = 0.304\n",
    "MAP@10 = 0.1639\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2708c-33c5-46df-9143-86aa638756f8",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f21881-970b-4e66-991d-451700f9edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed32c6-dacf-4417-9098-dca1a237a71a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e59c3d4-287e-4b7f-8980-28a34973e34c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2506\n",
    "MAP@10 = 0.1516\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a62fd6-9ed7-4b5f-941c-2807126b7737",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b17c3-3b0a-4e9f-925e-f2512d0dcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fd602-9386-49b4-944f-fbbed6b37dab",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80911004-e7f8-4464-bae8-cb24ef6e2772",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2388\n",
    "MAP@5 = 0.2503\n",
    "MAP@10 = 0.1498\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42eb75-8f45-4866-9ba7-66d726522757",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7905f849-1c41-429f-89d5-d8fe6239890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf96e5-e9a2-4da4-b7c1-b9beb97412cf",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95c804ae-278b-41f5-b63a-5f219a841e9d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2222\n",
    "MAP@5 = 0.2323\n",
    "MAP@10 = 0.1424\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0e57d-ad78-4aba-ad75-15106cdc2d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f616167-c318-4cf9-a5ce-d9d6b3fcd128",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option1Synsfirst_robertabase (No. 2 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531ac67-bf07-4c89-8c88-6ea6e6304d2e",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74bcbc3b-a13e-4db5-a7cc-c97fb4f76417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d75955-87e2-4694-bf1e-4cc13548c1c2",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebbf7dd3-5fbc-4190-a22d-046c8b645d92",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2166\n",
    "MAP@5 = 0.1689\n",
    "MAP@10 = 0.1281\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef6c70-b21a-49e3-9194-f1b4498f4e46",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3182cbb2-8877-4a24-a035-68b7be162cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cb519-02c0-41a5-9f40-1ef9d9b3ce1a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac470774-b059-41f5-a655-96ca4a873ad9",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2256\n",
    "MAP@10 = 0.1416\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfcbde-63a7-466c-9c50-dbb29900a3fc",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b12723c-3177-42b7-8871-b89a7683136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194f890-6138-4ff4-9caf-883e824e6f71",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a399c20-b120-4d78-abe8-f1716a9f220b",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2111\n",
    "MAP@5 = 0.1666\n",
    "MAP@10 = 0.1265\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec5807-b19a-4222-9037-92cdcd98c7b2",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37274071-fb1b-41ea-87aa-9522de899809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183f2f9-ab62-4de8-a7d1-3e8c9caa3b3e",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5739ebc6-54c9-4754-8254-20f4fd645aab",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.1888\n",
    "MAP@5 = 0.1663\n",
    "MAP@10 = 0.1247\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83457260-d8e5-4720-902c-4e1a9d48dd05",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837d1adb-bd55-4869-bc8f-5fa4c5926447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f45d-0dda-4f25-8ea4-ce74c595e3d9",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bb9515c-b55e-45ea-8fb5-fe8a8a9f42c8",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.1722\n",
    "MAP@5 = 0.1483\n",
    "MAP@10 = 0.1173\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f4e65-7218-4701-8da8-5082e88958ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48c0466-09ab-4b24-a751-8f091e626249",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_robertabase (No. 3 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e47a95-bf30-4b15-9a9c-f5b075bb6dcc",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fca0d04-155b-490c-984c-9f1c35f03896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7b543-daaa-4940-a548-7e948b71743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2541df8d-0b0d-4b74-a794-c12f6d55b831",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.22\n",
    "MAP@10 = 0.1466\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.7\n",
    "Accuracy@3@top_gold_1 = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61019833-b5e3-4377-ba34-beef619e76d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f119e-6782-411e-a2a9-311880390877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656259c9-8c8b-4c2c-88f7-6b4812ce9386",
   "metadata": {},
   "source": [
    "#### for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f822a1c-269e-4508-af6a-f9bdd3f71571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955588b1-4d8a-43cf-973d-e6901edf9ed7",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "565a5d28-0f34-40ce-8dbe-182ef27c1715",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3444\n",
    "MAP@5 = 0.2326\n",
    "MAP@10 = 0.1529\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226194b3-7739-4ec1-9607-57fdc65ceab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216606bf-07ba-490d-bee1-cf692564c177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c86be84-3a42-4c9a-9ec3-91ee39d1b888",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e27321-a677-4184-a3d3-6fe6d9b7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde1cd5-ed4c-4246-a04c-2c1a9c717041",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c2a61ed-3f0f-4ea1-ad61-50f0a45d8b75",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2096\n",
    "MAP@10 = 0.1382\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56de14-b948-41e4-9724-055372c88fdc",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002bb78b-fc32-461e-b082-eb36d97e982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8152afe-4740-4d42-8c86-d70aae3e415c",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a2109b-4842-4bdc-a1f7-01d6a8740052",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3277\n",
    "MAP@5 = 0.2406\n",
    "MAP@10 = 0.1512\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfb49f-5de4-4c0a-a5ba-7f8fe2c16b23",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d440af1-0a4a-4932-b23f-eec265dd2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66779f-6495-4e16-a1df-fc3ba813bd07",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4afe97fe-c8ac-45b5-9b6d-bf18631e62a0",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.7\n",
    "\n",
    "MAP@3 = 0.3333\n",
    "MAP@5 = 0.242\n",
    "MAP@10 = 0.1569\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.7\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5441e24-7db2-4c87-95c7-a8d12821ad8f",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_electralarge (No. 4 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5ca8a-d24f-4252-b842-7efcc6c2f915",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52ce8d89-c398-44f3-8eb5-9058bc7d1ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2aCEFR_J_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2aCEFR_J_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a744e1-0993-4cd2-9a2e-d9ea6962357d",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ec66af8-2fc1-43a7-b28c-89ad65f024fb",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option2aCEFR_J_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3388\n",
    "MAP@5 = 0.2743\n",
    "MAP@10 = 0.1739\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d17618b-a221-468b-9341-d0dfbc0080bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e0c6c2-5b97-455a-a956-2056fa8c905b",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89aee503-0b5e-4f77-80be-225de8f049b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2bCEFR_ls_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2bCEFR_ls_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2ffb6-9166-4155-81a4-0b5ed3deffd6",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75ce8363-781e-4d93-9f20-6d56ab82c8ef",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option2bCEFR_ls_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.35\n",
    "MAP@5 = 0.2739\n",
    "MAP@10 = 0.1758\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8f53d4c-3636-4e36-a7dd-2e8b8a66c395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e34f205-fd1c-4ebe-bcb0-a8d908d95fa2",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2389220a-a1e5-4b97-bcd7-c5e89b3e996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bdbcd-7532-4d65-a874-3e5cdb52c8cc",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1962cc3-8fee-47b2-bc40-df07319bb9fa",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option2cCEFR_efl_mostfreq_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2277\n",
    "MAP@5 = 0.2186\n",
    "MAP@10 = 0.148\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ca4a-e395-471e-911f-46545a810518",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa0e7c33-c1ce-4636-baae-8b06a2c8e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option2dCEFR_efl_weigthed_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2dCEFR_efl_weigthed_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f546f-b36c-4d64-b8b6-e693d48d5849",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8c028dc-f351-4cb0-84b9-688d7ea54206",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option2dCEFR_efl_weigthed_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2833\n",
    "MAP@5 = 0.246\n",
    "MAP@10 = 0.16\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d42b5f-322d-473e-b899-ab12b8532e2b",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a1bd82-ea8c-4a33-a7ed-dca21d0d7c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substitutes_cefr ranked on weighted average: [('free', 1.9211746218154655), ('necessary', 3.097985119380673), ('illegal', 3.154739395588201), ('essential', 3.961447810323761), ('optional', 4.0), ('voluntary', 4.515403311860254), ('mandatory', 5.0), ('required', 7), ('mandated', 7), ('prohibited', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('infused', 7), ('fed', 7), ('blessed', 7), ('impressed', 7), ('captured', 7), ('reinforced', 7), ('packed', 7), ('stunned', 7), ('surprised', 7), ('cultivated', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('freaks', 7), ('thugs', 7), ('nazis', 7), ('monsters', 7), ('devotees', 7), ('minions', 7), ('criminals', 7), ('operators', 7), ('mania', 7), ('machines', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('police', 2.411472933295504), ('monitors', 7), ('diplomats', 7), ('experts', 7), ('observations', 7), ('reporters', 7), ('witnesses', 7), ('officials', 7), ('specialists', 7), ('analysts', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('small', 1.8822950571267991), ('multiple', 3.508020212648811), ('minor', 3.7055185312725776), ('severe', 3.962990031597852), ('superficial', 5.0), ('bullet', 7), ('stab', 7), ('gunshot', 7), ('knife', 7), ('stabbing', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('masked', 7), ('concealed', 7), ('clothed', 7), ('posing', 7), ('dressed', 7), ('clad', 7), ('wrapped', 7), ('hidden', 7), ('dressing', 7), ('hiding', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('ally', 3.751615447623093), ('aspect', 3.81663333369369), ('extension', 4.5), ('echo', 6.0), ('assimilation', 6.0), ('affiliate', 7), ('outpost', 7), ('amalgamation', 7), ('adjunct', 7), ('analogue', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('musical', 2.423428458423256), ('classical', 2.7399554872362746), ('orchestral', 4.0), ('symphony', 7), ('melodic', 7), ('operatic', 7), ('concerto', 7), ('gothic', 7), ('choral', 7), ('concert', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('send', 2.578189995534637), ('commit', 3.609821412298311), ('dispatch', 4.0), ('employ', 4.160696730300728), ('activate', 5.0), ('install', 5.0), ('utilize', 5.5), ('station', 7), ('use', 7), ('launch', 7)]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('government', 3.1161180306175127), ('officials', 7), ('investigators', 7), ('prosecutors', 7), ('police', 7), ('magistrates', 7), ('officers', 7), ('agencies', 7), ('courts', 7), ('prisons', 7)]\n",
      "\n",
      "SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e8566-5ece-46d7-b998-0259fc34541e",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa893923-f5a3-45e3-897a-1116ab9c24d0",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3055\n",
    "MAP@5 = 0.2493\n",
    "MAP@10 = 0.164\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6704cbe-587a-4a34-87c4-1742a4456771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#averaging cefr levels across all substitutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fbe2485-3e56-4c5f-9ddc-99c74839d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substitutes_cefr ranked on weighted average: [('free', 1.9211746218154655), ('necessary', 3.097985119380673), ('illegal', 3.154739395588201), ('essential', 3.961447810323761), ('optional', 4.0), ('voluntary', 4.515403311860254), ('mandatory', 5.0), ('required', 7), ('mandated', 7), ('prohibited', 7)]\n",
      "\n",
      "sorted_cefr_levels: [1.9211746218154655, 3.097985119380673, 3.154739395588201, 3.961447810323761, 4.0, 4.515403311860254, 5.0]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('infused', 7), ('fed', 7), ('blessed', 7), ('impressed', 7), ('captured', 7), ('reinforced', 7), ('packed', 7), ('stunned', 7), ('surprised', 7), ('cultivated', 7)]\n",
      "\n",
      "sorted_cefr_levels: []\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('freaks', 7), ('thugs', 7), ('nazis', 7), ('monsters', 7), ('devotees', 7), ('minions', 7), ('criminals', 7), ('operators', 7), ('mania', 7), ('machines', 7)]\n",
      "\n",
      "sorted_cefr_levels: []\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('police', 2.411472933295504), ('monitors', 7), ('diplomats', 7), ('experts', 7), ('observations', 7), ('reporters', 7), ('witnesses', 7), ('officials', 7), ('specialists', 7), ('analysts', 7)]\n",
      "\n",
      "sorted_cefr_levels: [2.411472933295504]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('small', 1.8822950571267991), ('multiple', 3.508020212648811), ('minor', 3.7055185312725776), ('severe', 3.962990031597852), ('superficial', 5.0), ('bullet', 7), ('stab', 7), ('gunshot', 7), ('knife', 7), ('stabbing', 7)]\n",
      "\n",
      "sorted_cefr_levels: [1.8822950571267991, 3.508020212648811, 3.7055185312725776, 3.962990031597852, 5.0]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('masked', 7), ('concealed', 7), ('clothed', 7), ('posing', 7), ('dressed', 7), ('clad', 7), ('wrapped', 7), ('hidden', 7), ('dressing', 7), ('hiding', 7)]\n",
      "\n",
      "sorted_cefr_levels: []\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('ally', 3.751615447623093), ('aspect', 3.81663333369369), ('extension', 4.5), ('echo', 6.0), ('assimilation', 6.0), ('affiliate', 7), ('outpost', 7), ('amalgamation', 7), ('adjunct', 7), ('analogue', 7)]\n",
      "\n",
      "sorted_cefr_levels: [3.751615447623093, 3.81663333369369, 4.5, 6.0, 6.0]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('musical', 2.423428458423256), ('classical', 2.7399554872362746), ('orchestral', 4.0), ('symphony', 7), ('melodic', 7), ('operatic', 7), ('concerto', 7), ('gothic', 7), ('choral', 7), ('concert', 7)]\n",
      "\n",
      "sorted_cefr_levels: [2.423428458423256, 2.7399554872362746, 4.0]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('send', 2.578189995534637), ('commit', 3.609821412298311), ('dispatch', 4.0), ('employ', 4.160696730300728), ('activate', 5.0), ('install', 5.0), ('utilize', 5.5), ('station', 7), ('use', 7), ('launch', 7)]\n",
      "\n",
      "sorted_cefr_levels: [2.578189995534637, 3.609821412298311, 4.0, 4.160696730300728, 5.0, 5.0, 5.5]\n",
      "\n",
      "substitutes_cefr ranked on weighted average: [('government', 3.1161180306175127), ('officials', 7), ('investigators', 7), ('prosecutors', 7), ('police', 7), ('magistrates', 7), ('officers', 7), ('agencies', 7), ('courts', 7), ('prisons', 7)]\n",
      "\n",
      "sorted_cefr_levels: [3.1161180306175127]\n",
      "\n",
      "SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge'\n",
      "\n",
      "On average, there were 7.1 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n",
      "Average CEFR level over all rows: 3.8730174455392214\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "\n",
    "total_cefr_levels = []  # to put all cefr levels across all rows in\n",
    "\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    cefr_levels = []  # to put the cefr levels for the current row in\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            cefr_level = cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]\n",
    "            substitutes_cefr.append((original, cefr_level))\n",
    "            cefr_levels.append(cefr_level)\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "    sorted_cefr_levels = sorted(cefr_levels)\n",
    "    print(f\"sorted_cefr_levels: {sorted_cefr_levels}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "    total_cefr_levels.extend(cefr_levels)  # add cefr levels of the current row to the total cefr levels\n",
    "    \n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no4_SR_option2eCEFR_all_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")\n",
    "\n",
    "# calculate average CEFR level over all rows\n",
    "if total_cefr_levels:\n",
    "    average_cefr_level_total = sum(total_cefr_levels) / len(total_cefr_levels)\n",
    "else:\n",
    "    average_cefr_level_total = None\n",
    "\n",
    "print(f\"Average CEFR level over all rows: {average_cefr_level_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369e9f3-d35a-4ba0-b088-4d25379e6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#averaging cefr levels across all complex words (however, pos tag not taken into account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "344b6dec-9226-4fac-9b79-fe38a2d9cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CEFR level for complex words: 4.75\n",
      "Average CEFR level for substitutes (except level 7): 3.8730174455392214\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "cefr_levels = []  # list to store CEFR levels of all substitutes (except those with level 7)\n",
    "complex_word_cefr_levels = []  # list to store CEFR levels of the complex words\n",
    "\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    \n",
    "    \n",
    "    # get the CEFR level of the complex word and append it to complex_word_cefr_levels\n",
    "    if complex_word in cefr_df['word'].values:\n",
    "        complex_word_cefr = cefr_df[cefr_df['word'] == complex_word]['Weighted CEFR'].values[0]\n",
    "        complex_word_cefr_levels.append(complex_word_cefr)\n",
    "\n",
    " \n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  \n",
    "\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "    # collect the CEFR levels of the substitutes, except those with level 7\n",
    "    cefr_levels.extend(cefr for sub, cefr in ranked_cefr_subs if cefr < 7)\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "              \n",
    "# calculate average CEFR level of the complex words\n",
    "if complex_word_cefr_levels:\n",
    "    average_complex_word_cefr = sum(complex_word_cefr_levels) / len(complex_word_cefr_levels)\n",
    "    print(f\"Average CEFR level for complex words: {average_complex_word_cefr}\")\n",
    "else:\n",
    "    print(\"No CEFR level found for the complex words.\")\n",
    "\n",
    "# calculate average CEFR level of the substitutes (except those with level 7)\n",
    "if cefr_levels:\n",
    "    average_cefr_level = sum(cefr_levels) / len(cefr_levels)\n",
    "    print(f\"Average CEFR level for substitutes (except level 7): {average_cefr_level}\")\n",
    "else:\n",
    "    print(\"No CEFR level found for the substitutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c717e-b15d-4330-8168-4a4a430d0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#averaging cefr levels across all complex words (with pos tag taken into account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26c2bc84-ebc6-4b69-8e11-f9833858432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CEFR level for complex words: 4.75\n",
      "Average CEFR level for substitutes (except level 7): 3.8730174455392214\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "cefr_levels = []  # list to store CEFR levels of all substitutes (except those with level 7)\n",
    "complex_word_cefr_levels = []  # list to store CEFR levels of the complex words\n",
    "\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "\n",
    "    # get the POS of the complex word and its lemma\n",
    "    complex_word_pos = dict(pos_tag(word_tokenize(sentence))).get(complex_word)\n",
    "    complex_word_lemma = nlp(complex_word)[0].lemma_ if complex_word in [token.text for token in nlp(complex_word)] else complex_word\n",
    "\n",
    "    # get the CEFR level of the complex word and append it to complex_word_cefr_levels\n",
    "    if complex_word_lemma in cefr_df['word'].values and cefr_df[cefr_df['word'] == complex_word_lemma]['pos'].values[0] == complex_word_pos:\n",
    "        complex_word_cefr = cefr_df[cefr_df['word'] == complex_word_lemma]['Weighted CEFR'].values[0]\n",
    "        complex_word_cefr_levels.append(complex_word_cefr)\n",
    "\n",
    "\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  \n",
    "            \n",
    "\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "    # collect the CEFR levels of the substitutes, except those with level 7\n",
    "    cefr_levels.extend(cefr for sub, cefr in ranked_cefr_subs if cefr < 7)\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no4_SR_option2eCEFR_all_electralarge.tsv', sep='\\t', index=False, header=False)\n",
    "              \n",
    "# calculate average CEFR level of the complex words\n",
    "if complex_word_cefr_levels:\n",
    "    average_complex_word_cefr = sum(complex_word_cefr_levels) / len(complex_word_cefr_levels)\n",
    "    print(f\"Average CEFR level for complex words: {average_complex_word_cefr}\")\n",
    "else:\n",
    "    print(\"No CEFR level found for the complex words.\")\n",
    "\n",
    "# calculate average CEFR level of the substitutes (except those with level 7)\n",
    "if cefr_levels:\n",
    "    average_cefr_level = sum(cefr_levels) / len(cefr_levels)\n",
    "    print(f\"Average CEFR level for substitutes (except level 7): {average_cefr_level}\")\n",
    "else:\n",
    "    print(\"No CEFR level found for the substitutes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96baaa16-f559-4e02-b662-d1df3f19251d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
