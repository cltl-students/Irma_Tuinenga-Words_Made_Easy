{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0891387-8f77-4fbc-b9f8-48d61ce46335",
   "metadata": {},
   "source": [
    "### Datafiles preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fa557-55f3-4003-9399-094a92d81fc7",
   "metadata": {},
   "source": [
    "#### Removing unwanted chars from the trial and test sentences to prevent the code from crashing\n",
    "and storing the results in the files:\n",
    "- tsar2022_en_trial_none_no_noise.tsv \n",
    "- tsar2022_en_trial_gold_no_noise.tsv \n",
    "- tsar2022_en_test_none_no_noise.tsv\n",
    "- tsar2022_en_test_gold_no_noise.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ab214-4634-4c55-a627-e1b6871cb48d",
   "metadata": {},
   "source": [
    "##### trial dataset (for none and gold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96e1ffa-66a8-400e-a3a4-2302c4c621c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_none.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "\n",
    "# remove character combinations starting with # and optional quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# save the updated DataFrame to a new file\n",
    "new_filename = \"./data/trial/tsar2022_en_trial_none_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb77717-13ba-4d9d-a6d3-df5e113795c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_gold.tsv\"\n",
    "\n",
    "# define column names. Adjust this list depending on the actual column names in your file\n",
    "col_names = [\"sentence\", \"complex_word\"] + [f\"extra_col{i}\" for i in range(1, 27)]\n",
    "\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=col_names)\n",
    "\n",
    "# eemove character combinations starting with # and optional quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# save the updated DataFrame to a new file\n",
    "new_filename = \"./data/trial/tsar2022_en_trial_gold_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a5930-4ad7-4800-ba2e-e9ed4fc580a5",
   "metadata": {},
   "source": [
    "##### test dataset (for none and gold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a637e89b-632d-4acc-b2c4-2fa06d4ad037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_none.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "\n",
    "# remove character combinations starting with # and optional quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# save the updated dataFrame to a new file\n",
    "new_filename = \"./data/test/tsar2022_en_test_none_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63233c91-570f-47ce-8556-d4b04f11d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_gold.tsv\"\n",
    "\n",
    "# define column names. Adjust this list depending on the actual column names in your file\n",
    "col_names = [\"sentence\", \"complex_word\"] + [f\"extra_col{i}\" for i in range(1, 27)]\n",
    "\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=col_names)\n",
    "\n",
    "# remove character combinations starting with # and optional quotes\n",
    "pattern = r'#\\d+-\\d+(?: \")?'\n",
    "data['sentence'] = data['sentence'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# save the updated DataFrame to a new file\n",
    "new_filename = \"./data/test/tsar2022_en_test_gold_no_noise.tsv\"\n",
    "data.to_csv(new_filename, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea73248-cc1f-4e5a-b815-f253fa2e4360",
   "metadata": {},
   "source": [
    "### EFLLEX file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a8ed0-f198-4e99-b7e4-b66fd369dbb8",
   "metadata": {},
   "source": [
    "#### remove unwanted headers and columns from EFLLEX file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a8d03b-875d-4a88-9158-694a382e3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./cefr_efllex/EFLLex_NLP4J_ORIG.tsv', 'r', encoding='utf-8') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    next(tsv_reader, None)  # skip the headers\n",
    "    rows = [row[:-106] for row in tsv_reader]  # remove the last 105 columns\n",
    "\n",
    "\n",
    "with open('./cefr_efllex/EFLLex_trimmed.tsv', 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    tsv_writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05acda9-d708-4920-9790-52ac88168946",
   "metadata": {},
   "source": [
    "#### Option 1: assign most frequent cefr level to word in efflex dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fe86464-e0fe-4180-be14-9640fd4033ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define column names\n",
    "column_names = ['word', 'pos tag', 'A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# load tsv file\n",
    "df = pd.read_csv('./cefr_efllex/EFLLex_trimmed.tsv', sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# define CEFR levels for future reference\n",
    "cefr_levels = ['A1', 'A2', 'B1', 'B2', 'C1',]\n",
    "\n",
    "# extract column names of the max values and add the result to a new column\n",
    "df['Highest CEFR'] = df[cefr_levels].idxmax(axis=1)\n",
    "\n",
    "# create new dataframe with only the required columns\n",
    "df_new = df[['word', 'pos tag', 'Highest CEFR']]\n",
    "\n",
    "\n",
    "# write the new dataframe to a tsv file\n",
    "df_new.to_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header = False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5746a-9001-4817-a20b-d3d24a42b61d",
   "metadata": {},
   "source": [
    "#### Option 2: take weighted average to assign cefr level to word in efflex dataset.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "985af180-dee5-41c2-a075-b0bf998cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define column names\n",
    "column_names = ['word', 'pos tag', 'A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# load tsv file\n",
    "df = pd.read_csv('./cefr_efllex/EFLLex_trimmed.tsv', sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# define CEFR levels for future reference\n",
    "cefr_levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "# define mapping from CEFR levels to numbers\n",
    "mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5}\n",
    "\n",
    "# create a new dataframe to hold the weighted frequencies\n",
    "df_weighted = pd.DataFrame()\n",
    "\n",
    "# calculate the weighted frequencies for each CEFR level\n",
    "for level in cefr_levels:\n",
    "    df_weighted[level] = df[level] * mapping[level]\n",
    "\n",
    "# sum the weighted frequencies across the CEFR levels for each word\n",
    "df['Weighted Sum'] = df_weighted.sum(axis=1)\n",
    "\n",
    "# sum the frequencies across the CEFR levels for each word\n",
    "df['Frequency Sum'] = df[cefr_levels].sum(axis=1)\n",
    "\n",
    "# calculate the weighted average for each word\n",
    "df['Weighted CEFR'] = df['Weighted Sum'] / df['Frequency Sum']\n",
    "\n",
    "# create new dataframe with only the required columns\n",
    "df_new = df[['word', 'pos tag', 'Weighted CEFR']]\n",
    "\n",
    "# write the new dataframe to a tsv file\n",
    "df_new.to_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header = False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df998a96-13b7-47c5-92eb-18fbb5d6e199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f232bf-c817-48de-8e21-dc43bdb2691e",
   "metadata": {},
   "source": [
    "### CEFRJ file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043da03-ae7f-4a38-93aa-1ea0d88a8497",
   "metadata": {},
   "source": [
    "#### concatenate cefrj files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e02f0f-1b68-4465-9be6-d4c37cbe315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706caba-1d5a-4cff-8d5d-f78ada38b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv files, skipping the header row\n",
    "df1 = pd.read_csv('./cefrj/cefrj_1.csv', skiprows=1, header=None)\n",
    "df2 = pd.read_csv('./cefrj/cefrj_2.csv', skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79c199-805d-4861-af31-d6d746d52d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first three columns\n",
    "df1 = df1.iloc[:, :3]\n",
    "df2 = df2.iloc[:, :3]\n",
    "\n",
    "# concatenate the dataframes\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# write the combined data to a new tsv file\n",
    "df.to_csv('./cefrj/cefrj_all.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e88e1-24e3-466a-8ea8-6f99da4021eb",
   "metadata": {},
   "source": [
    "#### map existing pos tags to Treebank tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e1117-98c4-4a6b-aece-ee4cb39748c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# map simple POS tags to Treebank tags\n",
    "def map_to_treebank_tag(simple_tag):\n",
    "    if simple_tag == 'adjective':\n",
    "        return 'JJ'  # Adjective\n",
    "    elif simple_tag == 'verb':\n",
    "        return 'VB'  # Verb\n",
    "    elif simple_tag == 'noun':\n",
    "        return 'NN'  # Noun\n",
    "    elif simple_tag == 'adverb':\n",
    "        return 'RB'  # Adverb\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# open input file\n",
    "with open('./cefrj/cefrj_all.tsv', 'r') as infile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "\n",
    "    # open output file\n",
    "    with open('./cefrj/cefrj_all_treebank.tsv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "        # loop over rows in the input file\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            pos_tag = row[1]\n",
    "            cefr_level = row[2]\n",
    "\n",
    "            # map the POS tag to the Treebank format\n",
    "            treebank_pos = map_to_treebank_tag(pos_tag)\n",
    "\n",
    "            # if the tag was one of the four, write to the output file\n",
    "            if treebank_pos is not None:\n",
    "                writer.writerow([word, treebank_pos, cefr_level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0847b-2dca-4802-bfc8-1ab5f9d79a2b",
   "metadata": {},
   "source": [
    "### Uchida file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c49af-03a7-4d0b-a2f9-6fb6efe3e750",
   "metadata": {},
   "source": [
    "#### remove the redundant columns from the uchida file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "306e3789-fc54-40e2-85d4-c536ad930d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataframe\n",
    "df = pd.read_csv('./cefr/uchida.tsv', sep='\\t', header=None, usecols=range(24))\n",
    "\n",
    "# remove columns \n",
    "remove_cols = [5, 8, 11, 14, 17, 20, 23]\n",
    "df.drop(df.columns[remove_cols], axis=1, inplace=True)\n",
    "\n",
    "# write the dataframe to a new tsv file\n",
    "df.to_csv('./cefr/uchida_no_removed.tsv', sep='\\t', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d0f8-e705-41ba-b0b4-63cb365b3f58",
   "metadata": {},
   "source": [
    "#### parse the sentences to get the pos tags of the words (for example, this is needed in case of same words with different CEFR levels based on their pos in the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bffeb6b-a3ea-443f-92ca-c2421d7e0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# convert NLTK POS tags \n",
    "def convert_pos_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'JJ'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'VB'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'NN'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'RB'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# read the tsv file\n",
    "df = pd.read_csv('./cefr/uchida_no_removed.tsv', sep='\\t', header=None)\n",
    "\n",
    "# initialize a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define a new column for the POS tag\n",
    "df[17] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row[0]\n",
    "    target_word = row[1]\n",
    "\n",
    "    # tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # get POS tags for the words\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # lemmatize words and compare with the target word\n",
    "    for word, tag in tagged_words:\n",
    "        tag = convert_pos_tag(tag)  # convert the POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')  # 'n' is used as a default POS if the word cannot be pos tagged\n",
    "        if lemma == target_word and tag:\n",
    "            df.at[index, 17] = tag\n",
    "\n",
    "# filter the DataFrame to only include rows where the new POS column is not empty\n",
    "df = df[df[17] != '']\n",
    "\n",
    "# remove the first column\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "# insert the values from the last column to a new column after the first one\n",
    "df.insert(1, 'POS', df[17])\n",
    "\n",
    "# iterate through the positions and insert POS columns \n",
    "insert_positions = [4, 7, 10, 13, 16, 19, 22]\n",
    "for i, pos in enumerate(insert_positions):\n",
    "    df.insert(pos, f'POS_copy_{i+1}', df['POS'])\n",
    "\n",
    "# remove the last column\n",
    "df = df.drop(columns=[17])\n",
    "\n",
    "# write the df to a new file\n",
    "df.to_csv('./cefr/uchida_no_removed_parsed.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a5dc6-5654-4f57-a653-e3b4c571a87a",
   "metadata": {},
   "source": [
    "#### standardize format into three colums: word, pos, cefr level, and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ebf6a9e-8ac8-4801-af54-f51c64359e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the tsv file\n",
    "df = pd.read_csv('./cefr/uchida_no_removed_parsed.tsv', sep='\\t', header=None)\n",
    "\n",
    "# initialize an empty df\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# iterate over each three colums\n",
    "for i in range(0, df.shape[1], 3):\n",
    "    # select the three columns\n",
    "    threecols_df = df.iloc[:, i:i+3]\n",
    "    \n",
    "    # reset the column names \n",
    "    threecols_df.columns = range(3)\n",
    "    \n",
    "    # append to the final df\n",
    "    final_df = pd.concat([final_df, threecols_df])\n",
    "\n",
    "# reset the index of the final DataFrame\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# remove rows where less than two cells are filled (e.g., as a word without a CEFR level or the other way around is useless)\n",
    "final_df = final_df.dropna(thresh=2)\n",
    "\n",
    "# remove duplicates based on the first and second column\n",
    "final_df = final_df.drop_duplicates(subset=[0, 1])\n",
    "\n",
    "# write the final DataFrame to a new TSV file\n",
    "final_df.to_csv('./cefr/uchida_pos.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e2476-78ef-44fd-9db0-9965048d1ff9",
   "metadata": {},
   "source": [
    "### EVP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b095b5-2733-4220-a1e3-539ebdaefd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tsv\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('./cefr_evp/evp_american.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)  # remove the headers\n",
    "    rows = [row[:1] + row[2:-3] for row in csv_reader]  # remove the second column and last three columns\n",
    "\n",
    "\n",
    "with open('./cefr_evp/evp_american.tsv', 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    tsv_writer.writerows(rows)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c2ef2-4bb1-4fbe-82a8-b614798032d9",
   "metadata": {},
   "source": [
    "#### map evp pos tags to Treebank tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3f8a7f-36fd-4e23-bf20-97ec4a230ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# map simple POS tags to Treebank tags\n",
    "def map_to_treebank_tag(simple_tag):\n",
    "    if simple_tag == 'adjective':\n",
    "        return 'JJ'  # Adjective\n",
    "    elif simple_tag == 'verb':\n",
    "        return 'VB'  # Verb\n",
    "    elif simple_tag == 'noun':\n",
    "        return 'NN'  # Noun\n",
    "    elif simple_tag == 'adverb':\n",
    "        return 'RB'  # Adverb\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# open input file\n",
    "with open('./cefr_evp/evp_american.tsv', 'r') as infile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "\n",
    "    # open output file\n",
    "    with open('./cefr_evp/evp_american_treebank.tsv', 'w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "        # loop over rows in the input file\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            pos_tag = row[1]\n",
    "            cefr_level = row[2]\n",
    "\n",
    "            # map the POS tag to the Treebank format\n",
    "            treebank_pos = map_to_treebank_tag(pos_tag)\n",
    "\n",
    "            # if the tag was one of the four POS categories, write to the output file\n",
    "            if treebank_pos is not None:\n",
    "                writer.writerow([word, treebank_pos, cefr_level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb10c0f-0743-430e-b07f-bdcb58c5e813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
