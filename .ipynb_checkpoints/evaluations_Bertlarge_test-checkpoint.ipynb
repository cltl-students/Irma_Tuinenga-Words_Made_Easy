{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0085fb59-3709-4063-b22e-73a011dbb414",
   "metadata": {},
   "source": [
    "#### Evaluations BERTlarge for the TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683d06d-2380-4e93-bd91-fa8787d8f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_none_no_noise.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "# create an empty dataframe to store the substitutes for evaluation\n",
    "substitutes_df = pd.DataFrame(columns=[\"sentence\", \"complex_word\"] + [f\"substitute_{i+1}\" for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009449d-71e8-4be9-989d-722bbbd02202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc147290-d88e-4c2b-94bc-9e21f266759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c75960-7777-4ac2-a607-15725beaedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used when Bertscore is used in step SS\n",
    "import bert_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb261e-7d45-4254-aec6-7d6be569902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set the display.max_rows option to None to display all rows instead of limiting it to 50\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa3af3-857e-4db9-99b4-38c34eff6a23",
   "metadata": {},
   "source": [
    "## Bert-large-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f19cfd-81d2-4e7a-b664-c070d9df5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer and the model\n",
    "\n",
    "\n",
    "lm_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "lm_model = AutoModelForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "\n",
    "# Instantiate the fill-mask pipeline with the model\n",
    "fill_mask = pipeline(\"fill-mask\", lm_model, tokenizer = lm_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff42f3-d12f-4ef1-a972-5d5f6212043d",
   "metadata": {},
   "source": [
    "#### Substitute Generation including noise removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671ef0f-bef4-460a-b25b-40bfa07b6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "     \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation (SG) step a): initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\" Substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    #print(f\"Substitute Generation (SG) step b): substitute list without empty elements and unwanted characters: {substitutes}\\n\")\n",
    "        \n",
    "        \n",
    "    # limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = substitutes[:10]\n",
    "    #print(f\"Substitute Generation (SG) final step c): top-10 substitutes for the complex word '{complex_word}': {top_10_substitutes}\\n\")\n",
    "    \n",
    "    # # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SG_bertlarge exported to csv in path './predictions/test/SG_bertlarge.tsv'}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc98406-af44-410f-9533-a5a2978b3564",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_bertlarge.tsv --output_file ./output/test/SG_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ecaedf5-eacf-4645-a690-871d595c6818",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4919\n",
    "\n",
    "MAP@3 = 0.3297\n",
    "MAP@5 = 0.2533\n",
    "MAP@10 = 0.1556\n",
    "\n",
    "Potential@3 = 0.7392\n",
    "Potential@5 = 0.844\n",
    "Potential@10 = 0.9193\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.25\n",
    "Accuracy@2@top_gold_1 = 0.3736\n",
    "Accuracy@3@top_gold_1 = 0.4516"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0727fad-27a5-4bd5-bbbe-247db133f613",
   "metadata": {},
   "source": [
    "#### Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039b4f1-3fff-4647-9327-188e9d861160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    # print(f\"Sentence: {sentence}\")\n",
    "    # print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "     \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation (SG) step a): initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    #print(f\"Substitute Generation (SG) final step b): substitute list without empty elements and unwanted characters: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 2. Substitute Selection (SS) phase 1: remove duplicates, inflected forms, and antonyms of complex word:\n",
    "    \n",
    "    ## a) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    # print(f\"Substitute Selection (SS) phase 1, step a): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "    ## b) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    ## Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    # print(f\"Substitute Selection (SS) phase 1, step b): substitute list without duplicates nor inflected forms of the complex word '{complex_word}': {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "    ## c) remove antonyms of the complex word from the substitute list\n",
    "    ## get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    # print(f\"Substitute Selection (SS) phase 1, step c): substitute list without antonyms of the complex word '{complex_word}': {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "        \n",
    "    # limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = substitutes_no_antonyms[:10]\n",
    "    # print(f\"Substitute Selection (SS) phase 1, final step d): top-10 substitutes for the complex word '{complex_word}': {top_10_substitutes}\\n\")\n",
    "    \n",
    "    # # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SS_phase1_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_phase1_bertlarge exported to csv in path './predictions/test/SS_phase1_bertlarge.tsv'}\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36cf61-3f53-49e5-95b1-2bf60a36ae41",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_phase1_bertlarge.tsv --output_file ./output/test/SS_phase1_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4788fbe1-e53a-4cba-b0da-3b8594fbcc2d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_phase1_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SS_phase1_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.543\n",
    "\n",
    "MAP@3 = 0.3564\n",
    "MAP@5 = 0.2757\n",
    "MAP@10 = 0.17\n",
    "\n",
    "Potential@3 = 0.7715\n",
    "Potential@5 = 0.8602\n",
    "Potential@10 = 0.9247\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2768\n",
    "Accuracy@2@top_gold_1 = 0.387\n",
    "Accuracy@3@top_gold_1 = 0.4731"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21d113-c80c-4f84-90bb-b2d94324b520",
   "metadata": {},
   "source": [
    "#### Substitute Selection phase 2, option 1: duplicates that are synonyms of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a095e60-4873-4383-abff-62f19a2f59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "#     print(f\"Sentence: {sentence}\")\n",
    "#     print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "     \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation (SG) step a): initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    #print(f\"Substitute Generation (SG) final step b): substitute list without empty elements and unwanted characters: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 2. Substitute Selection (SS) phase 1: remove duplicates, inflected forms, and antonyms of complex word:\n",
    "    \n",
    "    ## a) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step a): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "    ## b) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    ## Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step b): substitute list without duplicates nor inflected forms of the complex word '{complex_word}': {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "    ## c) remove antonyms of the complex word from the substitute list\n",
    "    ## get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step c): substitute list without antonyms of the complex word '{complex_word}': {substitutes_no_antonyms}\\n\") \n",
    "    \n",
    "    \n",
    "    \n",
    "     # 3. Substitute Selection (SS) phase 2, option 1: sort the duplicates that are synonyms with the complex word first:\n",
    "    ## create two lists to hold synonyms and non-synonyms\n",
    "    synonyms = []\n",
    "    non_synonyms = []\n",
    "\n",
    "    ## iterate through each substitute\n",
    "    for substitute in substitutes_no_antonyms:\n",
    "        substitute_synsets = wn.synsets(substitute)\n",
    "        #print(f\"substitute_synsets for {substitute}: {substitute_synsets}\\n\")\n",
    "\n",
    "        # get all the lemmas for the substitute\n",
    "        substitute_syn_lemmas = [lemma.name() for syn in substitute_synsets for lemma in syn.lemmas()]\n",
    "        #print(f\"substitute_syn_lemmas for {substitute}: {substitute_syn_lemmas}\\n\")\n",
    "\n",
    "        # get all the lemmas for the complex word\n",
    "        complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "        #print(f\"complex_word_synsets for {complex_word}: {complex_word_synsets}\\n\")\n",
    "\n",
    "        complex_word_syn_lemmas = [lemma.name() for syn in complex_word_synsets for lemma in syn.lemmas()]\n",
    "        #print(f\"complex_word_syn_lemmas for {complex_word}: {complex_word_syn_lemmas}\\n\")\n",
    "\n",
    "        # find the intersection between the lemmas of the substitute and the complex word\n",
    "        intersection = set(complex_word_syn_lemmas).intersection(set(substitute_syn_lemmas))\n",
    "        #print(f\"intersection between lemmas of synsets of complex word {complex_word} and lemmas of synsets of substitute {substitute}: {intersection}\\n\")\n",
    "\n",
    "        if intersection:\n",
    "            #print(f\"Substitute {substitute} is a synonym of the complex word '{complex_word}'. Matching lemmas of their synsets: {intersection}\\n\")\n",
    "            # Add substitute to synonyms list\n",
    "            synonyms.append(substitute)\n",
    "        else:\n",
    "            # Add substitute to non_synonyms list\n",
    "            non_synonyms.append(substitute)\n",
    "\n",
    "        \n",
    "    ## print the lists of synonyms and non-synonyms\n",
    "    # print(f\"List of substitutes that are synonyms with the complex word '{complex_word}' in Wordnet: {synonyms}\\n\")\n",
    "    # print(f\"List of substitutes that are NO synonyms with the complex word '{complex_word}' in Wordnet: {non_synonyms}\\n\")\n",
    "\n",
    "    ## combine the lists with synonyms appearing first\n",
    "    final_list = synonyms + non_synonyms\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 1, step a): substitutes sorted on synonyms for the complex word '{complex_word}' first: {final_list}\\n\")\n",
    "\n",
    "        \n",
    "    ## limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = final_list[:10]\n",
    "#     print(f\"Substitute Selection (SS) phase 2, option 1, final step b): top 10 substitutes, sorted on synonyms for the complex word '{complex_word}' first: {top_10_substitutes}\\n\")\n",
    "    \n",
    "#     print('--------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    \n",
    "    # # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SS_phase2_option1Synsfirst_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_phase2_option1Synsfirst_bertlarge exported to csv in path './predictions/test/SS_phase2_option1Synsfirst_bertlarge.tsv'}\\n\")\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dbb1f-879e-467f-9e32-b6b2bb48751b",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_phase2_option1Synsfirst_bertlarge.tsv --output_file ./output/test/SS_phase2_option1Synsfirst_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e04d78b0-adca-4f12-af5a-b2e1edb605e5",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_phase2_option1Synsfirst_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SS_phase2_option1Synsfirst_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6102\n",
    "\n",
    "MAP@3 = 0.3921\n",
    "MAP@5 = 0.299\n",
    "MAP@10 = 0.1861\n",
    "\n",
    "Potential@3 = 0.8172\n",
    "Potential@5 = 0.8844\n",
    "Potential@10 = 0.9408\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2741\n",
    "Accuracy@2@top_gold_1 = 0.3709\n",
    "Accuracy@3@top_gold_1 = 0.4838\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e49b17-6308-4a0d-89d8-0311fbd465bd",
   "metadata": {},
   "source": [
    "#### Substitute Selection phase 2, option 2a: sort the duplicates that share their direct hypernyms with the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c4692-b6b7-43c9-853a-1bab19e0e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    # print(f\"Sentence: {sentence}\")\n",
    "    # print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation (SG) step a): initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    #print(f\"Substitute Generation (SG) final step b): substitute list without empty elements and unwanted characters: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 2. Substitute Selection (SS) phase 1: remove duplicates, inflected forms, and antonyms of complex word:\n",
    "    \n",
    "    ## a) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step a): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "    ## b) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    ## Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step b): substitute list without duplicates nor inflected forms of the complex word '{complex_word}': {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "    ## c) remove antonyms of the complex word from the substitute list\n",
    "    ## get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step c): substitute list without antonyms of the complex word '{complex_word}': {substitutes_no_antonyms}\\n\") \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. Substitute Selection (SS) phase 2, option 2-a: sort the duplicates that share their direct hypernyms (1 level up) with the complex word first:\n",
    "    \n",
    "    ## step a: get 1-level hypernym of complex word\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms = [syn.hypernyms() for syn in complex_word_synsets]\n",
    "    complex_word_hypernyms_lemmas = [lemma for sublist in complex_word_hypernyms for h in sublist for lemma in h.lemma_names()] # list of hypernyms' lemmas\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2, step a): complex_word_hypernyms_lemmas 1st level (direct hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_lemmas}\\n\")\n",
    "\n",
    "    ## step b: get 1-level hypernyms of the substitutes and get the intersection of shared hypernyms for complex word and the substitutes\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute in substitutes_no_antonyms:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        #print (f\"substitute synsets for substitute {substitute}: {substitute_synsets}\\n\")\n",
    "\n",
    "        # check 1-level hypernyms\n",
    "        substitute_hypernyms_1 = [syn.hypernyms() for syn in substitute_synsets]\n",
    "        substitute_hypernyms_1_lemmas = [lemma for sublist in substitute_hypernyms_1 for h in sublist for lemma in h.lemma_names()] # List of hypernyms' lemmas\n",
    "        #print(f\"Substitute_hypernyms_lemmas 1st level (direct hypernyms) for substitute {substitute}: {substitute_hypernyms_1_lemmas}\\n\")\n",
    "        \n",
    "\n",
    "        # check if the substitute has the same 1-level hypernyms as the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_lemmas).intersection(set(substitute_hypernyms_1_lemmas))\n",
    "        if intersection_1:\n",
    "            #print(f\"Substitute {substitute} has the same one-level hypernym as the complex word {complex_word} in Wordnet. Matching hypernym: {intersection_1}\\n\")\n",
    "            intersection_1_substitutes.append(substitute)\n",
    "        else:\n",
    "            other_substitutes.append(substitute)\n",
    "            \n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-a, step b): list of substitutes that share the same one-level hypernym with the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-a, step b): list of substitutes that DO NOT share the same one-level hypernym with the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")\n",
    "    \n",
    "\n",
    "    ## step c: create the final list, by putting the intersection first\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-a, step c): substitutes sorted on shared one-level hypernyms with the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # step d): limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = final_list[:10]\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-a, final step d): top 10 of substitutes, sorted on shared one-level hypernyms with the complex word '{complex_word}' in Wordnet first: {top_10_substitutes}\\n\")\n",
    "\n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    " \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SS_phase2_option2aHyps1first_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_phase2_option2aHyps1first_bertlarge exported to csv in path './predictions/test/SS_phase2_option2aHyps1first_bertlarge.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f63b4-54b1-48d7-a4bc-fe7516ad3c81",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_phase2_option2aHyps1first_bertlarge.tsv --output_file ./output/test/SS_phase2_option2aHyps1first_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "337749ce-b127-4a81-94a7-ca964cdb5fb6",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_phase2_option2aHyps1first_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SS_phase2_option2aHyps1first_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5698\n",
    "\n",
    "MAP@3 = 0.3511\n",
    "MAP@5 = 0.2686\n",
    "MAP@10 = 0.1734\n",
    "\n",
    "Potential@3 = 0.7634\n",
    "Potential@5 = 0.8602\n",
    "Potential@10 = 0.9301\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2553\n",
    "Accuracy@2@top_gold_1 = 0.3333\n",
    "Accuracy@3@top_gold_1 = 0.3978\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9147bc6-f497-444e-9af0-337ed0b80060",
   "metadata": {},
   "source": [
    "#### Substitute Selection phase 2, option 2b: sort the duplicates that share their indirect hypernyms (2 levels up) with the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed5d34-c457-4649-9237-ff15e2200748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "#     print(f\"Sentence: {sentence}\")\n",
    "#     print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation (SG) step a): initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    #print(f\"Substitute Generation (SG) final step b): substitute list without empty elements and unwanted characters: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 2. Substitute Selection (SS) phase 1: remove duplicates, inflected forms, and antonyms of complex word:\n",
    "    \n",
    "    ## a) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step a): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "    ## b) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    ## Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step b): substitute list without duplicates nor inflected forms of the complex word '{complex_word}': {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "    ## c) remove antonyms of the complex word from the substitute list\n",
    "    ## get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Substitute Selection (SS) phase 1, step c): substitute list without antonyms of the complex word '{complex_word}': {substitutes_no_antonyms}\\n\") \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. Substitute Selection (SS) phase 2, option 2-b: sort the duplicates that share their indirect (2 levels up) hypernyms with the complex word first:\n",
    "    \n",
    "    ## step a: get 2-level hypernym of complex word\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_2 = [h for syn in complex_word_synsets for h in syn.hypernyms() for h2 in h.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()] # List of two-level up hypernyms' lemmas\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-b, step a): complex_word_hypernyms_lemmas 2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "\n",
    "    ## step b: get 2-level hypernyms of the substitutes and get the intersection of shared 2-lvel hypernyms for complex word and the substitutes\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute in substitutes_no_antonyms:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        #print (f\"substitute synsets for substitute {substitute}: {substitute_synsets}\\n\")\n",
    "\n",
    "        # check 2-level hypernyms\n",
    "        substitute_hypernyms_2 = [h for syn in substitute_synsets for h in syn.hypernyms() for h2 in h.hypernyms()]\n",
    "        substitute_hypernyms_2_lemmas = [lemma for h in substitute_hypernyms_2 for lemma in h.lemma_names()] # List of two-level up hypernyms' lemmas\n",
    "        #print(f\"Substitute_hypernyms_lemmas 2nd level for substitute {substitute}: {substitute_hypernyms_2_lemmas}\\n\")\n",
    "        \n",
    "\n",
    "        # check if the substitute has the same 2-level hypernyms as the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_hypernyms_2_lemmas))\n",
    "        if intersection_2:\n",
    "            #print(f\"Substitute {substitute} has the same two-level hypernym as the complex word {complex_word} in Wordnet. Matching hypernym: {intersection_2}\\n\")\n",
    "            intersection_2_substitutes.append(substitute)\n",
    "        else:\n",
    "            other_substitutes.append(substitute)\n",
    "            \n",
    "#     print(f\"Substitute Selection (SS) phase 2, option 2-b, step b): list of substitutes that share the same two-level hypernym with the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Selection (SS) phase 2, option 2-b, step b): list of substitutes that DO NOT share the same two-level hypernym with the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")\n",
    "    \n",
    "\n",
    "    ## step c: create the final list, by putting the intersection first\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-b, step c): substitutes sorted on shared two-level hypernyms with the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # step d): limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = final_list[:10]\n",
    "    # print(f\"Substitute Selection (SS) phase 2, option 2-b, final step d): top 10 of substitutes, sorted on shared two-level hypernyms with the complex word '{complex_word}' in Wordnet first: {top_10_substitutes}\\n\")\n",
    "\n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    " \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SS_phase2_option2bHyps2first_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_phase2_option2bHyps2first_bertlarge exported to csv in path './predictions/test/SS_phase2_option2bHyps2first_bertlarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5b5c1-a983-4614-b3a5-9c5c82d28816",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_phase2_option2bHyps2first_bertlarge.tsv --output_file ./output/test/SS_phase2_option2bHyps2first_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fd33e0a-e8f5-4edf-978f-e9a8e3ca3128",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_phase2_option2bHyps2first_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SS_phase2_option2bHyps2first_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5591\n",
    "\n",
    "MAP@3 = 0.3546\n",
    "MAP@5 = 0.2749\n",
    "MAP@10 = 0.1745\n",
    "\n",
    "Potential@3 = 0.7688\n",
    "Potential@5 = 0.8575\n",
    "Potential@10 = 0.9274\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2526\n",
    "Accuracy@2@top_gold_1 = 0.336\n",
    "Accuracy@3@top_gold_1 = 0.4032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af66625-eb61-4710-af0b-37d0eb0c1fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c0057-3b87-42e6-a573-7ead6561c613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24de14-3dad-4b9c-826d-c6aeff6ab427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da65dd1b-491d-4a22-95ea-867f03caa541",
   "metadata": {},
   "source": [
    "#### Substitute Selection phase 2, option 3: duplicates ranked on their BertScores:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc036c5-6a1e-4e71-a184-b2201ac7baf7",
   "metadata": {},
   "source": [
    "#### Substitute Generation, Morphological Adaptation, and BertScore:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac04ed-1605-4908-9415-a91f714c56dc",
   "metadata": {},
   "source": [
    "##### BErtscore based on BERTbase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b87c-249f-41e7-8778-0935eac905e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result= fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms= []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='bert-base-uncased', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsBertbase_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SG_MA_SS_bsBertbase_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsBertbase_bertlarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d610f-b8ad-470e-90c6-dc81eb0a4c32",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsBertbase_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsBertbase_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8ba5902-2a2b-43be-a147-b5f6720a102e",
   "metadata": {},
   "source": [
    "========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsBertbase_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsBertbase_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5645\n",
    "\n",
    "MAP@3 = 0.3969\n",
    "MAP@5 = 0.296\n",
    "MAP@10 = 0.1818\n",
    "\n",
    "Potential@3 = 0.8306\n",
    "Potential@5 = 0.8924\n",
    "Potential@10 = 0.9408\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2338\n",
    "Accuracy@2@top_gold_1 = 0.3978\n",
    "Accuracy@3@top_gold_1 = 0.4704\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab9006-4a29-4410-a239-a2d309ce97e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde226f9-99a0-4ad5-89f4-8fcb6b8f5f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e243e1-e1f4-4de1-9952-cefd5ce8c6e1",
   "metadata": {},
   "source": [
    "##### BErtscore based on BERTlarge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504ebf4-576b-4370-8345-2492ac65fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result= fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms= []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='bert-large-uncased', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsBertlarge_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SG_MA_SS_bsBertlarge_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsBertlarge_bertlarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dbfa50-213d-4d26-afce-1df5ce5b539a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsBertlarge_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsBertlarge_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200eac4b-d783-4c11-a56e-61cbf22cc7c6",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsBertlarge_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsBertlarge_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5967\n",
    "\n",
    "MAP@3 = 0.412\n",
    "MAP@5 = 0.3038\n",
    "MAP@10 = 0.1889\n",
    "\n",
    "Potential@3 = 0.8413\n",
    "Potential@5 = 0.8978\n",
    "Potential@10 = 0.9435\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2526\n",
    "Accuracy@2@top_gold_1 = 0.3709\n",
    "Accuracy@3@top_gold_1 = 0.4838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f7737-482b-49e8-ae04-bcfb8737acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5638412-da86-4609-a24f-65f84ac07fde",
   "metadata": {},
   "source": [
    "#### bertscore based on electrabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e4995-2455-47f1-b81e-717f6c3baa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word : {substitutes_no_dupl_complex_word\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bertscores: \n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='google/electra-base-generator', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "    \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    ## fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    ## pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "   \n",
    "\n",
    "\n",
    "    ## add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "\n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsElectrabase_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)   \n",
    "print(\"SG_MA_SS_bsElectrabase_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsElectrabase_bertlarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e6039-6f67-4d07-a0ad-cd9e287468a3",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsElectrabase_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsElectrabase_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fba1ba79-1615-4f23-9ebf-439f57390876",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsElectrabase_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsElectrabase_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5268\n",
    "\n",
    "MAP@3 = 0.3687\n",
    "MAP@5 = 0.2776\n",
    "MAP@10 = 0.1746\n",
    "\n",
    "Potential@3 = 0.7956\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9435\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2204\n",
    "Accuracy@2@top_gold_1 = 0.3279\n",
    "Accuracy@3@top_gold_1 = 0.4354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc79664-0e78-4e81-b70b-00dbc36913a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec57fe3e-ffe7-4b29-80ea-15bebc7b0115",
   "metadata": {},
   "source": [
    "#### bertscore based on electralarge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac6d84-61f2-445c-b37b-f75eba2dbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat= f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word : {substitutes_no_dupl_complex_word\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bertscores: \n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='google/electra-large-generator', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "    \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    ## fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    ## pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "   \n",
    "\n",
    "\n",
    "    ## add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "\n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsElectralarge_bertlarge.tsv\", sep=\"\\t\", index=False, header=False)   \n",
    "print(\"SG_MA_SS_bsElectralarge_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsElectralarge_bertlarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3d02d-7977-46ee-a8fb-bad280b6c3fb",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsElectralarge_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsElectralarge_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f1aa1e5-cabf-4cc7-bdf7-d649f06e5fe1",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsElectralarge_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsElectralarge_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5591\n",
    "\n",
    "MAP@3 = 0.379\n",
    "MAP@5 = 0.2848\n",
    "MAP@10 = 0.1818\n",
    "\n",
    "Potential@3 = 0.8279\n",
    "Potential@5 = 0.8951\n",
    "Potential@10 = 0.9489\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2258\n",
    "Accuracy@2@top_gold_1 = 0.3387\n",
    "Accuracy@3@top_gold_1 = 0.4489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4147c68-95cf-4ed6-adc6-dafebc41560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0420d8ff-5cf9-4719-b1ad-5e5ff758bc29",
   "metadata": {},
   "source": [
    "##### bertscore based on Robertabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cb111-c194-4d47-b187-807b66f85bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='roberta-base', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "   \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsRobertabase_bertlarge.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "print(\"SG_MA_SS_bsRobertabase_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsRobertabase_bertlarge.tsv'}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc5000-78b9-4132-bc5d-feb838031b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsRobertabase_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsRobertabase_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e81b1441-9aa1-4168-8a18-0d5eedda0db9",
   "metadata": {},
   "source": [
    "\n",
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsRobertabase_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsRobertabase_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.586\n",
    "\n",
    "MAP@3 = 0.4139\n",
    "MAP@5 = 0.3115\n",
    "MAP@10 = 0.1923\n",
    "\n",
    "Potential@3 = 0.8306\n",
    "Potential@5 = 0.9059\n",
    "Potential@10 = 0.9408\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2688\n",
    "Accuracy@2@top_gold_1 = 0.4059\n",
    "Accuracy@3@top_gold_1 = 0.4731"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3f813-884b-4b9e-b694-0401fb6d9d2e",
   "metadata": {},
   "source": [
    "##### bertscore based on Robertalarge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dbae0-a6b9-448f-b678-b5bf1b2e406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower().strip() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    #print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms.append(substitute)\n",
    "        # else:\n",
    "        #     print(f\"Removed antonym: {substitute}\")\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_antonyms}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='roberta-large', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "   \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "print(\"SG_MA_SS_bsRobertalarge_bertlarge exported to csv in path './predictions/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv'}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb32f7-5706-4acd-bfd2-1573a5130b80",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv --output_file ./output/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14ce1d9b-67f8-458c-a880-c488f082dd98",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv\n",
    "OUTPUT file = ./output/test/SG_MA_SS_bsRobertalarge_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5994\n",
    "\n",
    "MAP@3 = 0.4266\n",
    "MAP@5 = 0.3227\n",
    "MAP@10 = 0.2001\n",
    "\n",
    "Potential@3 = 0.8629\n",
    "Potential@5 = 0.922\n",
    "Potential@10 = 0.9569\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2688\n",
    "Accuracy@2@top_gold_1 = 0.4166\n",
    "Accuracy@3@top_gold_1 = 0.5053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75169a-f501-4271-8dbb-c4d5b7014372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458abb5a-40ca-45fc-bda2-bb71e5c8faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967f0b63-a1f4-4d37-9d3e-6070ccb440c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5be671-1e2b-481c-8b16-10bb0ab69e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae88b78-3bb4-484f-aef4-7894e871aef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e455db2-1598-40c6-8ff5-fcda675a17e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
