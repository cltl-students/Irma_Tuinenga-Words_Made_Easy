{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## TEST set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96519786-e1d4-4855-ab6a-bdc3048c9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1719f821-ce93-4c5a-8e3c-5d93d3d47728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441171e7-74fe-4499-87fa-53a4df7fe5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map spaCy PoS tags to WordNet PoS tags\n",
    "def map_pos_spacy_wordnet(pos_spacy):\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    return pos_map.get(pos_spacy, wn.NOUN) # default to NOUN if pos_spacy does not exist in the dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e47a95-bf30-4b15-9a9c-f5b075bb6dcc",
   "metadata": {},
   "source": [
    "## for CEFR-J dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ed8a8-6947-400c-8070-148c5c7c36b7",
   "metadata": {},
   "source": [
    "### for model  SS_bsRobertalarge_robertabase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da70885-23ac-4446-aa01-8806876900a3",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05a2b2-47bb-411e-bc41-f75f4f6ab3cb",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3231f4cf-f8c8-4005-8a91-8837e7f01c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_j exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    #print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_j exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f8d48-a9c7-45e6-b9c7-a65749aae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv --output_file ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f2fb408-daf3-477b-b70f-f8e7d7e03fe5",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_j.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4838\n",
    "\n",
    "MAP@3 = 0.3437\n",
    "MAP@5 = 0.2704\n",
    "MAP@10 = 0.1824\n",
    "\n",
    "Potential@3 = 0.7822\n",
    "Potential@5 = 0.9032\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2069\n",
    "Accuracy@2@top_gold_1 = 0.3279\n",
    "Accuracy@3@top_gold_1 = 0.4274\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea6076-4989-4bb3-a38a-fbb308e9c972",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2692d94-e07b-4785-bc29-6c8f792c7f48",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3aba6-6484-44fd-adb0-169868a39e70",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebaee8a3-fed3-457e-a2e7-e941e3f46c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_j exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_j exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233af449-7f82-4f0c-aa1b-f760b90ffa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv --output_file ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c838580d-bd29-44c7-a00e-10535fcac759",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_j.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4327\n",
    "\n",
    "MAP@3 = 0.3151\n",
    "MAP@5 = 0.2548\n",
    "MAP@10 = 0.1743\n",
    "\n",
    "Potential@3 = 0.801\n",
    "Potential@5 = 0.8844\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.2983\n",
    "Accuracy@3@top_gold_1 = 0.3978"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656259c9-8c8b-4c2c-88f7-6b4812ce9386",
   "metadata": {},
   "source": [
    "## for Uchida et al dataset (CEFR-LS):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e5b77-4bba-4dad-b5b5-0d25308b1eea",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_robertabase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411bd4-dc10-49df-869b-7e8dccfe6838",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d77c6a-844c-470c-a80d-0dda1aaf2c6d",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f822a1c-269e-4508-af6a-f9bdd3f71571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_ls exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr/uchida_pos.tsv AND the POS tag of that word ('./cefr/uchida_pos.tsv is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_ls exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955588b1-4d8a-43cf-973d-e6901edf9ed7",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv --output_file ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "565a5d28-0f34-40ce-8dbe-182ef27c1715",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_ls.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4919\n",
    "\n",
    "MAP@3 = 0.372\n",
    "MAP@5 = 0.2979\n",
    "MAP@10 = 0.1908\n",
    "\n",
    "Potential@3 = 0.8252\n",
    "Potential@5 = 0.9193\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1935\n",
    "Accuracy@2@top_gold_1 = 0.3387\n",
    "Accuracy@3@top_gold_1 = 0.4327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226194b3-7739-4ec1-9607-57fdc65ceab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f85e233f-8a9d-4991-8472-8e9ee6e518e2",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53435a02-f09b-48e0-8f89-37c2474cb777",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445392b-1f09-452a-b32b-7097a6b8a224",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e262cb-6e71-45a4-9e7b-b85894ed19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_ls exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr/uchida_pos.tsv AND the POS tag of that word ('./cefr/uchida_pos.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    " \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_ls exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa548726-e84a-4395-88a6-2f27429af1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv --output_file ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18b87a70-d6cc-4429-8f9c-338cd0bdd34e",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_ls.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.465\n",
    "\n",
    "MAP@3 = 0.3457\n",
    "MAP@5 = 0.2843\n",
    "MAP@10 = 0.1838\n",
    "\n",
    "Potential@3 = 0.8064\n",
    "Potential@5 = 0.8897\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.3225\n",
    "Accuracy@3@top_gold_1 = 0.4005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216606bf-07ba-490d-bee1-cf692564c177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c86be84-3a42-4c9a-9ec3-91ee39d1b888",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97872bc-a34e-442b-a765-fd2effde1c0e",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_robertabase::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eae2f7-08bc-451c-88e2-a899f274a441",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa20c8-03cc-4b31-a0d0-184b3efa7be1",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e27321-a677-4184-a3d3-6fe6d9b7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in'./cefr_efllex/EFLLex_mostfreq AND the POS tag of that word ('./cefr_efllex/EFLLex_mostfreq) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde1cd5-ed4c-4246-a04c-2c1a9c717041",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv --output_file ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c2a61ed-3f0f-4ea1-ad61-50f0a45d8b75",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_mostfreq.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4543\n",
    "\n",
    "MAP@3 = 0.3322\n",
    "MAP@5 = 0.2678\n",
    "MAP@10 = 0.1804\n",
    "\n",
    "Potential@3 = 0.7661\n",
    "Potential@5 = 0.8951\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1666\n",
    "Accuracy@2@top_gold_1 = 0.301\n",
    "Accuracy@3@top_gold_1 = 0.4139\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d3ef88-3c0a-48e8-ba70-3e636b845458",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7d83d-cd2c-4973-838d-fa97cb23ad2f",
   "metadata": {},
   "source": [
    "#### with pos tag of substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae81a0-a56c-421c-a2ed-1a4f1f39bac2",
   "metadata": {},
   "source": [
    "\n",
    "If the lemmatized version of the substitute is found in the './cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in ./cefr_efllex/EFLLex_mostfreq.tsv') matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f38f1d8-7450-4c58-a14f-7eb63bcfe502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_efllex/EFLLex_mostfreq AND the POS tag of that word (in './cefr_efllex/EFLLex_mostfreq) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7fad3-801b-4d7a-821b-80b946d68b05",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv --output_file ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a53d373e-d7ca-4491-a805-88f6e693c135",
   "metadata": {},
   "source": [
    "\n",
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_mostfreq.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3951\n",
    "\n",
    "MAP@3 = 0.3112\n",
    "MAP@5 = 0.2571\n",
    "MAP@10 = 0.1742\n",
    "\n",
    "Potential@3 = 0.7876\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1505\n",
    "Accuracy@2@top_gold_1 = 0.3037\n",
    "Accuracy@3@top_gold_1 = 0.4032\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56de14-b948-41e4-9724-055372c88fdc",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f419dc7-44e9-47fe-a430-69ff733a19e0",
   "metadata": {},
   "source": [
    "### with pos tag of the substitutes taken into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9922a-d499-4e87-b6bc-edcca3a39c68",
   "metadata": {},
   "source": [
    "If the lemmatized version of the substitute is found in the './cefr_efllex/EFLLex_weighted.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex.tsv') matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a6fc1-e3cb-4f27-87b3-6dd0e08c0278",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_robertabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "002bb78b-fc32-461e-b082-eb36d97e982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_efllex/EFLLex_weighted.tsv AND the POS tag of that word ('./cefr_efllex/EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8152afe-4740-4d42-8c86-d70aae3e415c",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted.tsv --output_file ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a2109b-4842-4bdc-a1f7-01d6a8740052",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_efl_weighted.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4596\n",
    "\n",
    "MAP@3 = 0.324\n",
    "MAP@5 = 0.262\n",
    "MAP@10 = 0.1786\n",
    "\n",
    "Potential@3 = 0.7526\n",
    "Potential@5 = 0.887\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1666\n",
    "Accuracy@2@top_gold_1 = 0.2822\n",
    "Accuracy@3@top_gold_1 = 0.387"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd54228-5653-40fa-9e78-68f2ea53fa37",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be52582-bb1b-49e9-a41a-937a418f5036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_efllex/EFLLex_weighted.tsv AND the POS tag of that word (in './cefr_efllex/EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207f125-ddba-4b44-ab63-e9dd986fe09f",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv --output_file ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20161bb0-1ce7-49c0-8d83-62a89bc4bd5f",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_efl_weighted.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3709\n",
    "\n",
    "MAP@3 = 0.2919\n",
    "MAP@5 = 0.2442\n",
    "MAP@10 = 0.1695\n",
    "\n",
    "Potential@3 = 0.7715\n",
    "Potential@5 = 0.8817\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1317\n",
    "Accuracy@2@top_gold_1 = 0.2715\n",
    "Accuracy@3@top_gold_1 = 0.3655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85640600-e82c-473d-bc43-4286f2a44271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d44bfb1-a6b2-4fa0-8a2a-daa6a56bfd33",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: (unique word-postag groups with duplicate CEFR scores have been averaged):  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ac328-4db5-4568-a6a8-404f18ce40cb",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_robertabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5073b0d-6acd-42d5-9989-7bc679c73ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "   \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d53bc1-eead-499e-8ce3-6f677c5be75b",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv --output_file ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16a72a41-7e21-40a0-98ee-3d707b166b4c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4327\n",
    "\n",
    "MAP@3 = 0.3101\n",
    "MAP@5 = 0.2553\n",
    "MAP@10 = 0.175\n",
    "\n",
    "Potential@3 = 0.7768\n",
    "Potential@5 = 0.9059\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1801\n",
    "Accuracy@2@top_gold_1 = 0.3064\n",
    "Accuracy@3@top_gold_1 = 0.3978\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285ab7f-4533-477f-b123-000b002929a5",
   "metadata": {},
   "source": [
    "### code updated to present averages of substitutes not found in the combined CEFR database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e27111-92f2-463e-bc38-3a708bc132b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_robertabase_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all'\n",
      "\n",
      "On average, there were 6.088709677419355 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    #print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_robertabase_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_robertabase_SR_cefr_all'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebf9f1-0aa5-421d-b2ed-67052939f8bb",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d2be08-9fbf-4d82-9813-0234de63b617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "   \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be6741-2b43-49b5-b54d-c3d8c5cc18f3",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv --output_file ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6ba5fa4-f80e-4972-9a68-acedb2adfa5b",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv\n",
    "OUTPUT file = ./output/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.387\n",
    "\n",
    "MAP@3 = 0.2821\n",
    "MAP@5 = 0.2379\n",
    "MAP@10 = 0.1674\n",
    "\n",
    "Potential@3 = 0.7715\n",
    "Potential@5 = 0.8763\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1478\n",
    "Accuracy@2@top_gold_1 = 0.2634\n",
    "Accuracy@3@top_gold_1 = 0.379\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88ea1f-64b0-4881-94bd-757fdddc805a",
   "metadata": {},
   "source": [
    "### code updated to present averages of substitutes not found in the combined CEFR database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd170c8b-fc66-4e33-be13-2b6c678df1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_bsRobertalarge_electralarge_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all'\n",
      "\n",
      "On average, there were 5.830645161290323 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/test/SG_MA_SS_bsRobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "   # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_bsRobertalarge_electralarge_SR_cefr_all exported to csv in path './predictions/test/SS_bsRobertalarge_electralarge_SR_cefr_all'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a710f9-4e70-4654-a333-3259286b7ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
