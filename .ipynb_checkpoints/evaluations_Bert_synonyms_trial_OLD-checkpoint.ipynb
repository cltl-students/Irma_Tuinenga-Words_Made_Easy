{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0085fb59-3709-4063-b22e-73a011dbb414",
   "metadata": {},
   "source": [
    "#### Evaluations BERT for the trial set (10 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5683d06d-2380-4e93-bd91-fa8787d8f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_none_no_noise.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "# create an empty dataframe to store the substitutes for evaluation\n",
    "substitutes_df = pd.DataFrame(columns=[\"sentence\", \"complex_word\"] + [f\"substitute_{i+1}\" for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0009449d-71e8-4be9-989d-722bbbd02202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc147290-d88e-4c2b-94bc-9e21f266759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used for morphological adjustments in step MA\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46dbbc20-f729-4e8e-82bf-67373cba2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used when word embeddings are used in step SS\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c75960-7777-4ac2-a607-15725beaedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used when Bertscore is used in step SS\n",
    "import bert_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7beb261e-7d45-4254-aec6-7d6be569902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display.max_rows option to None to display all rows instead of limiting it to 50\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa3af3-857e-4db9-99b4-38c34eff6a23",
   "metadata": {},
   "source": [
    "#### Bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f19cfd-81d2-4e7a-b664-c070d9df5d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the tokenizer and the model\n",
    "\n",
    "# for bert-base:\n",
    "lm_tokenizer_bertbase = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "lm_model_bertbase = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# Instantiate the fill-mask pipeline with the model\n",
    "fill_mask_bertbase = pipeline(\"fill-mask\", lm_model_bertbase, tokenizer = lm_tokenizer_bertbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0727fad-27a5-4bd5-bbbe-247db133f613",
   "metadata": {},
   "source": [
    "#### Substitute Generation and Morphological Adaptation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a039b4f1-3fff-4647-9327-188e9d861160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in each row, for each complex word: \n",
    "# for index, row in data.iterrows():\n",
    "    \n",
    "#     # print the sentence and the complex word\n",
    "#     sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "#     #print(f\"Sentence: {sentence}\")\n",
    "#     #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "#     # for bertbase model:\n",
    "       \n",
    "#     # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "#     ## in the sentence, replace the complex word with a masked word\n",
    "#     sentence_masked_word_bertbase = sentence.replace(complex_word, lm_tokenizer_bertbase.mask_token)\n",
    "\n",
    "#     ## concatenate the original sentence and the masked sentence\n",
    "#     sentences_concat_bertbase = f\"{sentence} {lm_tokenizer_bertbase.sep_token} {sentence_masked_word_bertbase}\"\n",
    "\n",
    "#     ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "#     top_k = 30\n",
    "#     result_bertbase = fill_mask_bertbase(sentences_concat_bertbase, top_k=top_k)\n",
    "#     substitutes_bertbase = [substitute[\"token_str\"] for substitute in result_bertbase if \"token_str\" in substitute]\n",
    "#     #print(f\"Substitute Generation step: initial substitute list: {substitutes_bertbase}\\n\")\n",
    "\n",
    "\n",
    "#     #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "#     ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "#     ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "#     punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "#     punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "#     try:\n",
    "#         substitutes_bertbase = [substitute[\"token_str\"].lower().strip() for substitute in result_bertbase if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "#                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "#         # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters for bertbase model: {substitutes_bertbase}\\n\")\n",
    "#     except TypeError as error:\n",
    "#         continue\n",
    "\n",
    "\n",
    "\n",
    "#     ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "#     ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "#     substitutes_no_dupl_bertbase = []\n",
    "#     for sub in substitutes_bertbase:\n",
    "#         if sub not in substitutes_no_dupl_bertbase:\n",
    "#             substitutes_no_dupl_bertbase.append(sub)\n",
    "#     #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes for bertbase model: {substitutes_no_dupl_bertbase}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#     ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "#     ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "#     doc_complex_word = nlp(complex_word)\n",
    "#     complex_word_lemma = doc_complex_word[0].lemma_\n",
    "#     #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "#     ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "#     substitutes_no_dupl_complex_word_bertbase = []\n",
    "#     for substitute in substitutes_no_dupl_bertbase:\n",
    "#         doc_substitute = nlp(substitute)\n",
    "#         substitute_lemma = doc_substitute[0].lemma_\n",
    "#         if substitute_lemma != complex_word_lemma:\n",
    "#             substitutes_no_dupl_complex_word_bertbase.append(substitute)\n",
    "#     #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word for bertbase model: {substitutes_no_dupl_complex_word_bertbase}\\n\")\n",
    "\n",
    "\n",
    "#      ## d) remove antonyms of the complex word from the substitute list\n",
    "#     ## step 1: get the antonyms of the complex word\n",
    "#     antonyms_complex_word = []\n",
    "#     for syn in wn.synsets(complex_word_lemma):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             for antonym in lemma.antonyms():\n",
    "#                     antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "#     print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "#     ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "#     substitutes_no_antonyms_bertbase = []\n",
    "#     for substitute in substitutes_no_dupl_complex_word_bertbase:\n",
    "#         doc_substitute = nlp(substitute)\n",
    "#         substitute_lemma = doc_substitute[0].lemma_\n",
    "#         if substitute_lemma not in antonyms_complex_word:\n",
    "#             substitutes_no_antonyms_bertbase.append(substitute)\n",
    "#         else:\n",
    "#             print(f\"Removed antonym: {substitute}\")\n",
    "#     print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: {substitutes_no_antonyms_bertbase}\\n\") \n",
    "  \n",
    "    \n",
    "        \n",
    "#     # limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "#     top_10_substitutes = substitutes_no_antonyms_bertbase[:10]\n",
    "#     #print(f\"top-10 substitutes SG and MA: {top_10_substitutes}\\n\")\n",
    "    \n",
    "#     # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "#     required_for_dataframe = 10\n",
    "#     # pad the list with None until it has 10 elements\n",
    "#     top_10_substitutes += [None] * (required_for_dataframe - len(top_10_substitutes))\n",
    "\n",
    "#     # # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "#     substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "#     #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "# # export the dataframe to a tsv file for evaluation\n",
    "# substitutes_df.to_csv(\"./predictions/trial/BertBase_SG_MA.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "# print(\"BertBase_SG_MA exported to csv in path './predictions/trial/BertBase_SG_MA.tsv'}\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36cf61-3f53-49e5-95b1-2bf60a36ae41",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/BertBase_SG_MA.tsv --output_file ./output/trial/BertBase_SG_MA.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4788fbe1-e53a-4cba-b0da-3b8594fbcc2d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/BertBase_SG_MA.tsv\n",
    "OUTPUT file = ./output/trial/BertBase_SG_MA.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2253\n",
    "MAP@10 = 0.1298\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7be10-e432-484a-9f6c-43bddb491678",
   "metadata": {},
   "source": [
    "#### Substitute Generation and Morphological Adaptation, with additions:\n",
    "- moving already generated hypernyms (1 or 2 layers above) from the generated top-30 to the top-10 (if not already there), as they may be perceived as simpler.  \n",
    "- adding hypernyms to the generated list, to the top-10 so that the top-10 can be ranked on SR in the SR step. \n",
    "\n",
    "Question to consider: should adding or moving hypernyms be executed before the SS step (Bertscores) or after? Even if Bertscores reveal that they are not very similar, did annotators still use hypernyms  for simplification? If so, then the hypernyms should be added after the SS step (Bertscores). \n",
    "Answer: perform both before and after Bertscores, and see from the trial set results what will be the best approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521ab03-d88c-4212-8a4a-46bc92a3699c",
   "metadata": {},
   "source": [
    "### Before the SS step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33b57ed-5f62-4a0e-a422-0dc97100247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "complex_word_lemma for complex word 'compulsory': compulsory\n",
      "\n",
      "Antonyms for complex word 'compulsory': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['mandatory', 'obligatory', 'optional', 'required', 'necessary', 'standard', 'voluntary', 'customary', 'impossible', 'easier', 'only', 'illegal', 'sufficient', 'unnecessary', 'easy', 'normal', 'permitted', 'mandated', 'difficult', 'simple', 'appropriate', 'expensive', 'possible', 'commonplace', 'essential', 'proper', 'available', 'enough', 'affordable']\n",
      "\n",
      "complex_word_hypernyms_lemmas: []\n",
      "-------------------------------------\n",
      "Sentence: Rajoy's conservative government had instilled markets with a brief dose of confidence by stepping into Bankia, performing a U-turn on its refusal to spend public money to rescue banks.\n",
      "Complex word: instilled\n",
      "complex_word_lemma for complex word 'instilled': instill\n",
      "\n",
      "Antonyms for complex word 'instilled': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['strengthened', 'targeted', 'tested', 'provided', 'created', 'affected', 'addressed', 'attacked', 'punished', 'bred', 'rewarded', 'empowered', 'shocked', 'introduced', 'silenced', 'improved', 'set', 'defeated', 'treated', 'reinforced', 'prepared', 'hit', 'surprised', 'hardened', 'infused', 'challenged', 'released', 'secured', 'delivered', 'established']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['lend', 'impart', 'bestow', 'contribute', 'add', 'bring', 'insert', 'infix', 'enter', 'introduce', 'affect', 'impress', 'move', 'strike', 'drill', 'fill', 'fill_up', 'make_full']\n",
      "Substitute created has same one-level hypernyms with the complex word: {'move'}\n",
      "Substitute addressed has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute attacked has same one-level hypernyms with the complex word: {'move', 'affect'}\n",
      "Substitute rewarded has same one-level hypernyms with the complex word: {'move'}\n",
      "Substitute shocked has same two-level hypernyms with the complex word: {'move', 'impress', 'affect', 'strike'}\n",
      "Substitute introduced has same one-level hypernyms with the complex word: {'introduce'}\n",
      "Substitute introduced has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute set has same one-level hypernyms with the complex word: {'move', 'impress'}\n",
      "Substitute set has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute treated has same one-level hypernyms with the complex word: {'affect'}\n",
      "Substitute treated has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute hit has same one-level hypernyms with the complex word: {'move', 'affect'}\n",
      "Substitute hit has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute surprised has same one-level hypernyms with the complex word: {'move', 'impress', 'affect', 'strike'}\n",
      "Substitute infused has same one-level hypernyms with the complex word: {'fill', 'drill', 'fill_up', 'make_full'}\n",
      "Substitute released has same two-level hypernyms with the complex word: {'move'}\n",
      "Substitute secured has same one-level hypernyms with the complex word: {'fill_up'}\n",
      "Substitute secured has same two-level hypernyms with the complex word: {'fill'}\n",
      "Substitute delivered has same one-level hypernyms with the complex word: {'bring'}\n",
      "Substitute delivered has same two-level hypernyms with the complex word: {'move'}\n",
      "-------------------------------------\n",
      "Sentence: War maniacs of the South Korean puppet military made another grave provocation to the DPRK in the central western sector of the front on Thursday afternoon.\n",
      "Complex word: maniacs\n",
      "complex_word_lemma for complex word 'maniacs': maniac\n",
      "\n",
      "Antonyms for complex word 'maniacs': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['criminals', 'machines', 'rats', 'crimes', 'dogs', 'robots', 'ants', 'pigs', 'elephants', 'doctors', 'bandits', 'priests', 'demons', 'hawks', 'lords', 'machine', 'soldiers', 'leaders', 'monsters', 'heroes', 'drones', 'fighters', 'veterans', 'puppets', 'ministers', 'workers']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['sick_person', 'diseased_person', 'sufferer', 'fancier', 'enthusiast']\n",
      "-------------------------------------\n",
      "Sentence: The daily death toll in Syria has declined as the number of observers has risen, but few experts expect the U.N. plan to succeed in its entirety.\n",
      "Complex word: observers\n",
      "complex_word_lemma for complex word 'observers': observer\n",
      "\n",
      "Antonyms for complex word 'observers': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['witnesses', 'participants', 'casualties', 'refugees', 'visitors', 'monitors', 'victims', 'delegates', 'travelers', 'experts', 'troops', 'inspectors', 'allies', 'diplomats', 'inspections', 'soldiers', 'fatalities', 'workers', 'pilgrims', 'people', 'officials', 'deaths', 'tourists', 'survivors', 'volunteers', 'citizens', 'civilians', 'combatants']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['person', 'individual', 'someone', 'somebody', 'mortal', 'soul', 'expert']\n",
      "Substitute witnesses has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute witnesses has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute participants has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute visitors has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute victims has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute victims has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute travelers has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute experts has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute inspectors has same two-level hypernyms with the complex word: {'expert'}\n",
      "Substitute workers has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute workers has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute officials has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute tourists has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute survivors has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute survivors has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute volunteers has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute citizens has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute combatants has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "-------------------------------------\n",
      "Sentence: An amateur video showed a young girl who apparently suffered shrapnel wounds in her thigh undergoing treatment in a makeshift Rastan hospital while screaming in pain.\n",
      "Complex word: shrapnel\n",
      "complex_word_lemma for complex word 'shrapnel': shrapnel\n",
      "\n",
      "Antonyms for complex word 'shrapnel': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['stab', 'gunshot', 'bullet', 'knife', 'multiple', 'severe', 'arrow', 'several', 'minor', 'stabbing', 'laser', 'open', 'two', 'rifle', 'serious', 'blunt', 'similar', 'blast', 'numerous', 'penetrating', 'three', 'sword', 'some', 'slash', 'shooting', 'the', 'painful', 'fatal', 'deep', 'horrible']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['shell']\n",
      "-------------------------------------\n",
      "Sentence: A local witness said a separate group of attackers disguised in burqas — the head-to-toe robes worn by conservative Afghan women — then tried to storm the compound.\n",
      "Complex word: disguised\n",
      "complex_word_lemma for complex word 'disguised': disguised\n",
      "\n",
      "Antonyms for complex word 'disguised': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['dressed', 'clad', 'masked', 'clothed', 'dressing', 'disguise', 'concealed', 'cloak', 'posing', 'appeared', 'dress', 'posed', 'guise', 'costume', 'attire', 'appearing', 'dresses', 'hiding', 'hooded', 'covered', 'draped', 'costumes', 'hidden', 'wrapped', 'undercover', 'lured', 'present', 'adorned']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['hide', 'conceal']\n",
      "Substitute masked has same one-level hypernyms with the complex word: {'hide', 'conceal'}\n",
      "Substitute masked has same two-level hypernyms with the complex word: {'hide', 'conceal'}\n",
      "Substitute disguise has same one-level hypernyms with the complex word: {'hide', 'conceal'}\n",
      "Substitute cloak has same two-level hypernyms with the complex word: {'hide', 'conceal'}\n",
      "Substitute covered has same one-level hypernyms with the complex word: {'hide', 'conceal'}\n",
      "-------------------------------------\n",
      "Sentence: Syria's Sunni majority is at the forefront of the uprising against Assad, whose minority Alawite sect is an offshoot of Shi'ite Islam.\n",
      "Complex word: offshoot\n",
      "complex_word_lemma for complex word 'offshoot': offshoot\n",
      "\n",
      "Antonyms for complex word 'offshoot': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['affiliate', 'extension', 'arm', 'adaptation', 'incarnation', 'ally', 'opponent', 'attack', 'expansion', 'outpost', 'associate', 'advocate', 'enemy', 'aspect', 'origin', 'branch', 'adversary', 'independent', 'alliance', 'ancestor', 'iteration', 'evolution', 'independently', 'antagonist', 'offspring', 'overthrow', 'imitation', 'implementation', 'organization', 'example']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['consequence', 'effect', 'outcome', 'result', 'event', 'issue', 'upshot']\n",
      "Substitute branch has same one-level hypernyms with the complex word: {'result', 'upshot', 'effect', 'outcome', 'consequence', 'issue', 'event'}\n",
      "Substitute alliance has same two-level hypernyms with the complex word: {'event'}\n",
      "Substitute offspring has same one-level hypernyms with the complex word: {'result', 'upshot', 'effect', 'outcome', 'consequence', 'issue', 'event'}\n",
      "Substitute implementation has same two-level hypernyms with the complex word: {'event'}\n",
      "Substitute example has same two-level hypernyms with the complex word: {'event'}\n",
      "-------------------------------------\n",
      "Sentence: Although not as rare in the symphonic literature as sharper keys , examples of symphonies in A major are not as numerous as for D major or G major .\n",
      "Complex word: symphonic\n",
      "complex_word_lemma for complex word 'symphonic': symphonic\n",
      "\n",
      "Antonyms for complex word 'symphonic': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['orchestral', 'symphony', 'musical', 'classical', 'operatic', 'philharmonic', 'melodic', 'choral', 'music', 'orchestra', 'instrumental', 'concert', 'historical', 'popular', 'concerto', 'harmonic', 'symphonies', 'technical', 'trombone', 'romantic', 'lyric', 'liturgical', 'dramatic', 'thematic', 'traditional', 'composers', 'vocal', 'dynamic', 'general']\n",
      "\n",
      "complex_word_hypernyms_lemmas: []\n",
      "-------------------------------------\n",
      "Sentence: That prompted the military to deploy its largest warship, the BRP Gregorio del Pilar, which was recently acquired from the United States.\n",
      "Complex word: deploy\n",
      "complex_word_lemma for complex word 'deploy': deploy\n",
      "\n",
      "Antonyms for complex word 'deploy': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['withdraw', 'activate', 'deployment', 'send', 'maintain', 'dispatch', 'use', 'acquire', 'reserve', 'retain', 'move', 'utilize', 'request', 'return', 'include', 'add', 'launch', 'designate', 'fire', 'operate', 'provide', 'establish', 'employ', 'outfit', 'purchase', 'expand', 'retire', 'build']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['position', 'spread', 'distribute']\n",
      "Substitute reserve has same two-level hypernyms with the complex word: {'distribute'}\n",
      "-------------------------------------\n",
      "Sentence:  UK police were expressly forbidden, at a ministerial level, to provide any assistance to Thai authorities as the case involves the death penalty.\n",
      "Complex word: authorities\n",
      "complex_word_lemma for complex word 'authorities': authority\n",
      "\n",
      "Antonyms for complex word 'authorities': []\n",
      "\n",
      "Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: ['officials', 'police', 'citizens', 'forces', 'officers', 'people', 'government', 'residents', 'governments', 'troops', 'nationals', 'persons', 'civilians', 'courts', 'policemen', 'individuals', 'protesters', 'subjects', 'personnel', 'organisations', 'agencies', 'suspects', 'interests', 'leaders', 'politicians', 'activists', 'institutions']\n",
      "\n",
      "complex_word_hypernyms_lemmas: ['control', 'person', 'individual', 'someone', 'somebody', 'mortal', 'soul', 'expert', 'certainty', 'administrative_unit', 'administrative_body', 'permission', 'book']\n",
      "Substitute officials has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute citizens has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute residents has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute nationals has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute protesters has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute subjects has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute subjects has same two-level hypernyms with the complex word: {'control'}\n",
      "Substitute agencies has same one-level hypernyms with the complex word: {'administrative_body', 'administrative_unit'}\n",
      "Substitute suspects has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute leaders has same one-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "Substitute politicians has same two-level hypernyms with the complex word: {'individual', 'mortal', 'somebody', 'someone', 'soul', 'person'}\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # for bertbase model:\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word_bertbase = sentence.replace(complex_word, lm_tokenizer_bertbase.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat_bertbase = f\"{sentence} {lm_tokenizer_bertbase.sep_token} {sentence_masked_word_bertbase}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result_bertbase = fill_mask_bertbase(sentences_concat_bertbase, top_k=top_k)\n",
    "    substitutes_bertbase = [substitute[\"token_str\"] for substitute in result_bertbase if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes_bertbase}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes_bertbase = [substitute[\"token_str\"].lower().strip() for substitute in result_bertbase if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters for bertbase model: {substitutes_bertbase}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl_bertbase = []\n",
    "    for sub in substitutes_bertbase:\n",
    "        if sub not in substitutes_no_dupl_bertbase:\n",
    "            substitutes_no_dupl_bertbase.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes for bertbase model: {substitutes_no_dupl_bertbase}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word_bertbase.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word for bertbase model: {substitutes_no_dupl_complex_word_bertbase}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_complex_word_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms_bertbase.append(substitute)\n",
    "        else:\n",
    "            print(f\"Removed antonym: {substitute}\")\n",
    "    print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: {substitutes_no_antonyms_bertbase}\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## checking for hypernyms in the substitute list\n",
    "    # step 1: getting immediate hypernyms of complex word\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms = [syn.hypernyms() for syn in complex_word_synsets]\n",
    "    complex_word_hypernyms_lemmas = [lemma for sublist in complex_word_hypernyms for h in sublist for lemma in h.lemma_names()] # list of hypernyms' lemmas\n",
    "    print(f\"complex_word_hypernyms_lemmas: {complex_word_hypernyms_lemmas}\")\n",
    "\n",
    "    # getting one and two level hypernyms of the substitutes\n",
    "    for substitute in substitutes_no_antonyms_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "\n",
    "        # Checking one level hypernyms\n",
    "        substitute_hypernyms_1 = [syn.hypernyms() for syn in substitute_synsets]\n",
    "        substitute_hypernyms_1_lemmas = [lemma for sublist in substitute_hypernyms_1 for h in sublist for lemma in h.lemma_names()] # list of hypernyms' lemmas\n",
    "        intersection_1 = set(complex_word_hypernyms_lemmas).intersection(set(substitute_hypernyms_1_lemmas))\n",
    "        if intersection_1:\n",
    "            print(f\"Substitute {substitute} has same one-level hypernyms with the complex word: {intersection_1}\")\n",
    "\n",
    "        # Checking two level hypernyms\n",
    "        substitute_hypernyms_2 = [h.hypernyms() for sublist in substitute_hypernyms_1 for h in sublist] # Getting the hypernyms of the first-level hypernyms\n",
    "        substitute_hypernyms_2_lemmas = [lemma for sublist in substitute_hypernyms_2 for h in sublist for lemma in h.lemma_names()] # list of hypernyms' lemmas\n",
    "        intersection_2 = set(complex_word_hypernyms_lemmas).intersection(set(substitute_hypernyms_2_lemmas))\n",
    "        if intersection_2:\n",
    "            print(f\"Substitute {substitute} has same two-level hypernyms with the complex word: {intersection_2}\")\n",
    "            \n",
    "    print('-------------------------------------')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da859561-de7c-4f1b-b0f6-e05b6acdcd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36698c4-2fb4-4d9a-a32b-2c83e280792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d65157-4885-4300-b2b2-f06deeea1873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1ccd4-801a-4aac-b9e0-6c8857432ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bc036c5-6a1e-4e71-a184-b2201ac7baf7",
   "metadata": {},
   "source": [
    "#### Substitute Generation, Morphological Adaptation, and BertScore:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34e0321f-543d-4a12-9607-da7b26365895",
   "metadata": {},
   "source": [
    "##### BErtscore based on BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504ebf4-576b-4370-8345-2492ac65fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # for bertbase model:\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word_bertbase = sentence.replace(complex_word, lm_tokenizer_bertbase.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat_bertbase = f\"{sentence} {lm_tokenizer_bertbase.sep_token} {sentence_masked_word_bertbase}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result_bertbase = fill_mask_bertbase(sentences_concat_bertbase, top_k=top_k)\n",
    "    substitutes_bertbase = [substitute[\"token_str\"] for substitute in result_bertbase if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes_bertbase}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes_bertbase = [substitute[\"token_str\"].lower().strip() for substitute in result_bertbase if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters for bertbase model: {substitutes_bertbase}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl_bertbase = []\n",
    "    for sub in substitutes_bertbase:\n",
    "        if sub not in substitutes_no_dupl_bertbase:\n",
    "            substitutes_no_dupl_bertbase.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes for bertbase model: {substitutes_no_dupl_bertbase}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word_bertbase.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word for bertbase model: {substitutes_no_dupl_complex_word_bertbase}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_complex_word_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms_bertbase.append(substitute)\n",
    "        else:\n",
    "            print(f\"Removed antonym: {substitute}\")\n",
    "    print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: {substitutes_no_antonyms_bertbase}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms_bertbase]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='bert-base-uncased', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms_bertbase, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/trial/BertBase_SG_MA_SS_bsBert.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"BertBase_SG_MA_SS_bsBert exported to csv in path './predictions/trial/BertBase_SG_MA_SS_bsBert.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dbfa50-213d-4d26-afce-1df5ce5b539a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/BertBase_SG_MA_SS_bsBert.tsv --output_file ./output/trial/BertBase_SG_MA_SS_bsBert.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200eac4b-d783-4c11-a56e-61cbf22cc7c6",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/BertBase_SG_MA_SS_bsBert.tsv\n",
    "OUTPUT file = ./output/trial/BertBase_SG_MA_SS_bsBert.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.7\n",
    "\n",
    "MAP@3 = 0.3222\n",
    "MAP@5 = 0.2203\n",
    "MAP@10 = 0.1386\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.5\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335de33-4c05-4040-8e4d-265d75a0587d",
   "metadata": {},
   "source": [
    "Result: best performance so far, on all metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57fe3e-ffe7-4b29-80ea-15bebc7b0115",
   "metadata": {},
   "source": [
    "#### bertscore based on electra since it performed so well on Electramodel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac6d84-61f2-445c-b37b-f75eba2dbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # for bertbase model:\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word_bertbase = sentence.replace(complex_word, lm_tokenizer_bertbase.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat_bertbase = f\"{sentence} {lm_tokenizer_bertbase.sep_token} {sentence_masked_word_bertbase}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result_bertbase = fill_mask_bertbase(sentences_concat_bertbase, top_k=top_k)\n",
    "    substitutes_bertbase = [substitute[\"token_str\"] for substitute in result_bertbase if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes_bertbase}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes_bertbase = [substitute[\"token_str\"].lower().strip() for substitute in result_bertbase if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters for bertbase model: {substitutes_bertbase}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl_bertbase = []\n",
    "    for sub in substitutes_bertbase:\n",
    "        if sub not in substitutes_no_dupl_bertbase:\n",
    "            substitutes_no_dupl_bertbase.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes for bertbase model: {substitutes_no_dupl_bertbase}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word_bertbase.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word for bertbase model: {substitutes_no_dupl_complex_word_bertbase}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_complex_word_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms_bertbase.append(substitute)\n",
    "        else:\n",
    "            print(f\"Removed antonym: {substitute}\")\n",
    "    print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: {substitutes_no_antonyms_bertbase}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bertscores: \n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms_bertbase]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='google/electra-base-generator', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "    \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms_bertbase, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    ## fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    ## pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "   \n",
    "\n",
    "\n",
    "    ## add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "\n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/trial/BertBase_SG_MA_SS_bsElectra.tsv\", sep=\"\\t\", index=False, header=False)   \n",
    "print(\"BertBase_SG_MA_SS_bsElectra exported to csv in path './predictions/trial/BertBase_SG_MA_SS_bsElectra.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3d02d-7977-46ee-a8fb-bad280b6c3fb",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/BertBase_SG_MA_SS_bsElectra.tsv --output_file ./output/trial/BertBase_SG_MA_SS_bsElectra.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f1aa1e5-cabf-4cc7-bdf7-d649f06e5fe1",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/BertBase_SG_MA_SS_bsElectra.tsv\n",
    "OUTPUT file = ./output/trial/BertBase_SG_MA_SS_bsElectra.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3555\n",
    "MAP@5 = 0.2663\n",
    "MAP@10 = 0.1612\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3f813-884b-4b9e-b694-0401fb6d9d2e",
   "metadata": {},
   "source": [
    "#### bertscore based on Roberta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dbae0-a6b9-448f-b678-b5bf1b2e406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "    \n",
    "    \n",
    "    # for bertbase model:\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word_bertbase = sentence.replace(complex_word, lm_tokenizer_bertbase.mask_token)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat_bertbase = f\"{sentence} {lm_tokenizer_bertbase.sep_token} {sentence_masked_word_bertbase}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result_bertbase = fill_mask_bertbase(sentences_concat_bertbase, top_k=top_k)\n",
    "    substitutes_bertbase = [substitute[\"token_str\"] for substitute in result_bertbase if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes_bertbase}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes_bertbase = [substitute[\"token_str\"].lower().strip() for substitute in result_bertbase if not any(char in punctuation_set for char in substitute[\"token_str\"]) # added .strip as roberta uses a leading space before each substitute\n",
    "                      and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters for bertbase model: {substitutes_bertbase}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl_bertbase = []\n",
    "    for sub in substitutes_bertbase:\n",
    "        if sub not in substitutes_no_dupl_bertbase:\n",
    "            substitutes_no_dupl_bertbase.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes for bertbase model: {substitutes_no_dupl_bertbase}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word_bertbase.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word for bertbase model: {substitutes_no_dupl_complex_word_bertbase}\\n\")\n",
    "\n",
    "\n",
    "     ## d) remove antonyms of the complex word from the substitute list\n",
    "    ## step 1: get the antonyms of the complex word\n",
    "    antonyms_complex_word = []\n",
    "    for syn in wn.synsets(complex_word_lemma):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                    antonyms_complex_word.append(antonym.name())\n",
    "\n",
    "    print(f\"Antonyms for complex word '{complex_word}': {antonyms_complex_word}\\n\")\n",
    "\n",
    "    ## step 2: remove antonyms of the complex word from the list with substitutes\n",
    "    substitutes_no_antonyms_bertbase = []\n",
    "    for substitute in substitutes_no_dupl_complex_word_bertbase:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma not in antonyms_complex_word:\n",
    "            substitutes_no_antonyms_bertbase.append(substitute)\n",
    "        else:\n",
    "            print(f\"Removed antonym: {substitute}\")\n",
    "    print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word for bertbase model: {substitutes_no_antonyms_bertbase}\\n\") \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_antonyms_bertbase]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='roberta-base', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_antonyms_bertbase, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "   \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/trial/BertBase_SG_MA_SS_bsRoberta.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "print(\"BertBase_SG_MA_SS_bsRoberta exported to csv in path './predictions/trial/BertBase_SG_MA_SS_bsRoberta.tsv'}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb32f7-5706-4acd-bfd2-1573a5130b80",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/BertBase_SG_MA_SS_bsRoberta.tsv --output_file ./output/trial/BertBase_SG_MA_SS_bsRoberta.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14ce1d9b-67f8-458c-a880-c488f082dd98",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/BertBase_SG_MA_SS_bsRoberta.tsv\n",
    "OUTPUT file = ./output/trial/BertBase_SG_MA_SS_bsRoberta.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.2722\n",
    "MAP@5 = 0.2083\n",
    "MAP@10 = 0.1508\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5443b5-13d4-498c-bce6-d8a599a36429",
   "metadata": {},
   "source": [
    "#### Conclusion (based on on MAP@1):\n",
    "1. BertBase_SG_MA_SS_bs (0.7)\n",
    "2. BertBase_SG_MA_SS_bsElectra (0.6) (based on other scores)\n",
    "3. BertBase_SG_MA_SS_ce (0.6) (based on other scores)\n",
    "4. BertBase_SG_MA_SS_bsRoberta (0.6)\n",
    "5. BertBase_SG_MA_SS   (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d522985-2e8d-4cf4-8159-c94fafdaf0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75169a-f501-4271-8dbb-c4d5b7014372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458abb5a-40ca-45fc-bda2-bb71e5c8faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967f0b63-a1f4-4d37-9d3e-6070ccb440c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5be671-1e2b-481c-8b16-10bb0ab69e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae88b78-3bb4-484f-aef4-7894e871aef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e455db2-1598-40c6-8ff5-fcda675a17e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
