{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc4a85b-c819-4ed1-a597-e004761790a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# load RoBERTa model and tokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/trial/tsar2022_en_trial_none.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6db1ba-b272-4ff9-96ef-90c5731bd64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "Top 30 substitutes: ['compulsory', 'mandatory', 'obligatory', 'voluntary', 'required', 'optional', 'obliged', 'uniform', 'necessary', 'available', 'mandated', 'sufficient', 'routine', 'forced', 'customary', 'prerequisite', 'feasible', 'indispensable', 'forthcoming', 'universal', 'requirement', 'involuntary', 'obligated', 'compelled', 'conditional', 'enforced', 'contingent', 'possible', 'compulsion', 'Mandatory']\n",
      "\n",
      "Sentence: Rajoy's conservative government had instilled markets with a brief dose of confidence by stepping into Bankia, performing a U-turn on its refusal to spend public money to rescue banks.\n",
      "Complex word: instilled\n",
      "Top 30 substitutes: ['infused', 'injected', 'endowed', 'illed', 'inst', 'furnished', 'supplied', 'bolstered', 'implanted', 'impressed', 'reinforced', 'invested', 'provided', 'filled', 'reassured', 'undermined', 'filled', 'pumped', 'struck', 'augmented', 'enriched', 'stirred', 'vested', 'imb', 'intoxicated', 'seeded', 'misled', 'stunned', 'inspired', 'inflated']\n",
      "\n",
      "Sentence: #34-3 \"War maniacs of the South Korean puppet military made another grave provocation to the DPRK in the central western sector of the front on Thursday afternoon.\n",
      "Complex word: maniacs\n",
      "Top 30 substitutes: ['maniac', 'criminals', 'riors', 'hawks', 'heads', 'killers', 'gangs', 'lords', 'thugs', 'devils', 'drums', 'murderers', 'fighters', 'monsters', 'fools', 'villains', 'idiots', 'machines', 'demons', 'doctors', 'planes', 'igans', 'bosses', 'Nazis', 'beasts', 'fascists', 'children', 'nerds', 'extremists', 'parasites']\n",
      "\n",
      "Sentence: The daily death toll in Syria has declined as the number of observers has risen, but few experts expect the U.N. plan to succeed in its entirety.\n",
      "Complex word: observers\n",
      "Top 30 substitutes: ['observers', 'monitors', 'spectators', 'witnesses', 'observer', 'observes', 'visitors', 'viewers', 'reporters', 'inspectors', 'commentators', 'investigators', 'participants', 'analysts', 'cameras', 'outsiders', 'demonstrators', 'activists', 'journalists', 'experts', 'authorities', 'observations', 'eyes', 'supporters', 'observing', 'residents', 'protesters', 'responders', 'parties', 'followers']\n",
      "\n",
      "Sentence: An amateur video showed a young girl who apparently suffered shrapnel wounds in her thigh undergoing treatment in a makeshift Rastan hospital while screaming in pain.\n",
      "Complex word: shrapnel\n",
      "Top 30 substitutes: ['rapnel', 'bullet', 'gunshot', 'stab', 'radiation', 'projectile', 'crippling', 'shotgun', 'mortar', 'stun', 'explosive', 'stray', 'shell', 'bomb', 'microscopic', 'fragmentation', 'cartridge', 'grenade', 'rocket', 'torpedo', 'piercing', 'unidentified', 'ammunition', 'insurgent', 'similar', 'blast', 'bullets', 'slug', 'terrorist', 'pillow']\n",
      "\n",
      "Sentence: A local witness said a separate group of attackers disguised in burqas — the head-to-toe robes worn by conservative Afghan women — then tried to storm the compound.\n",
      "Complex word: disguised\n",
      "Top 30 substitutes: ['disguised', 'dressed', 'veiled', 'masked', 'concealed', 'hidden', 'disguise', 'clothed', 'clad', 'posed', 'covered', 'cloaked', 'shrouded', 'wrapped', 'posing', 'hiding', 'trained', 'camoufl', 'presented', 'known', 'smuggled', 'staged', 'draped', 'described', 'decorated', 'armed', 'buried', 'camouflage', 'marked', 'hid']\n",
      "\n",
      "Sentence: Syria's Sunni majority is at the forefront of the uprising against Assad, whose minority Alawite sect is an offshoot of Shi'ite Islam.\n",
      "Complex word: offshoot\n",
      "Top 30 substitutes: ['off', 'extension', 'opposite', 'outpost', 'off', 'out', 'affiliate', 'offspring', 'overthrow', 'element', 'outside', 'echo', 'expansion', 'imprint', 'arm', 'ideology', 'overhaul', 'branch', 'opposition', 'evolution', 'indication', 'upset', 'archetype', 'extend', 'independent', 'alternative', 'example', 'instance', 'embrace', 'adjunct']\n",
      "\n",
      "Sentence: Although not as rare in the symphonic literature as sharper keys , examples of symphonies in A major are not as numerous as for D major or G major .\n",
      "Complex word: symphonic\n",
      "Top 30 substitutes: ['classical', 'harmonic', 'musical', 'sym', 'music', 'instrumental', 'piano', 'lyric', 'jazz', 'modern', 'dramatic', 'major', 'vocal', 'composing', 'concert', 'opera', 'sonic', 'formal', 'popular', 'early', 'composition', 'English', 'singing', 'standard', 'trumpet', 'keyboard', 'contemporary', 'orchestra', 'technical', 'onic']\n",
      "\n",
      "Sentence: That prompted the military to deploy its largest warship, the BRP Gregorio del Pilar, which was recently acquired from the United States.\n",
      "Complex word: deploy\n",
      "Top 30 substitutes: ['deploy', 'deployed', 'deployment', 'utilize', 'mobilize', 'employ', 'deploying', 'dispatch', 'equip', 'Deploy', 'ploy', 'use', 'activate', 'recruit', 'transport', 'operate', 'withdraw', 'install', 'locate', 'commit', 'construct', 'summon', 'expend', 'send', 'reserve', 'possess', 'cultivate', 'adopt', 'engage', 'unleash']\n",
      "\n",
      "Sentence: #35-14 UK police were expressly forbidden, at a ministerial level, to provide any assistance to Thai authorities as the case involves the death penalty.\n",
      "Complex word: authorities\n",
      "Top 30 substitutes: ['authorities', 'officials', 'Authorities', 'authority', 'investigators', 'counterparts', 'police', 'agencies', 'administrators', 'forces', 'prosecutors', 'officers', 'Authorities', 'entities', 'intelligence', 'jurisdictions', 'individuals', 'detainees', 'institutions', 'affairs', 'demonstrators', 'superiors', 'victims', 'regulators', 'concerns', 'ministers', 'figures', 'suspects', 'sources', 'assistance']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# substitute generation\n",
    "\n",
    "\n",
    "# in each row, mask the complex word and generate substitutes\n",
    "for index, row in data.iterrows():\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    \n",
    "    # in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, \"<mask>\")\n",
    "    \n",
    "    # concatenate the sentence with the complex word and the sentence with the masked word \n",
    "    sentences_concat = f\"{sentence} {tokenizer.sep_token} {sentence_masked_word}\"\n",
    "    \n",
    "    # tokenize the concatenated sentence\n",
    "    sentences_concat_tokenized = tokenizer.encode(sentences_concat, return_tensors='pt')\n",
    "    \n",
    "    # find the masked word in the tokenized sentence\n",
    "    mask_location = torch.where( sentences_concat_tokenized == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "    # generate predictions for the masked word\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentences_concat_tokenized)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # get the top-k predictions\n",
    "    top_k = 30\n",
    "    top_tokens = torch.topk(predictions[0, mask_location], top_k).indices\n",
    "\n",
    "    # decode the top-k tokens\n",
    "    substitutes = [tokenizer.decode(token.item()).strip() for token in top_tokens]\n",
    "    \n",
    "    # print sentence, complex word, and the top_k substitutes for the complex word\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "    print(f\"Top {top_k} substitutes: {substitutes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc8dfb-bac1-4a17-9eb7-cc8423a1ce27",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0598242f-3ed6-4669-8b94-05d8cd3b37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953af63c-03c0-49e2-9b41-7b3db672c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "SG step: generated substitutes: ['compulsory', 'mandatory', 'obligatory', 'obliged', 'voluntary', 'uniform', 'required', 'mandated', 'optional', 'dogma', 'forced', 'intrinsic', 'advisable', 'routine', 'forthcoming', 'prerequisite', 'ubiquitous', 'involuntary', 'forcibly', 'feasible', 'compelled', 'indispensable', 'necessary', 'obligated', 'liable', 'habitual', 'enforced', 'universal', 'contingent', 'commonplace']\n",
      "\n",
      "SS step: substitute: compulsory, cosine similarity: 0.9015046486291226\n",
      "SS step: substitute: mandatory, cosine similarity: 0.9021189359409612\n",
      "SS step: substitute: obligatory, cosine similarity: 0.9014813422164168\n",
      "SS step: substitute: obliged, cosine similarity: 0.9015624717592371\n",
      "SS step: substitute: voluntary, cosine similarity: 0.9021885012375569\n",
      "SS step: substitute: uniform, cosine similarity: 0.9025437448116703\n",
      "SS step: substitute: required, cosine similarity: 0.9026452935703692\n",
      "SS step: substitute: mandated, cosine similarity: 0.9016573971370299\n",
      "SS step: substitute: optional, cosine similarity: 0.9021221644114442\n",
      "SS step: substitute: dogma, cosine similarity: 0.901350211475577\n",
      "SS step: substitute: forced, cosine similarity: 0.9023645096978682\n",
      "SS step: substitute: intrinsic, cosine similarity: 0.9012908124131281\n",
      "SS step: substitute: advisable, cosine similarity: 0.9005569672984866\n",
      "SS step: substitute: routine, cosine similarity: 0.9028284700646062\n",
      "SS step: substitute: forthcoming, cosine similarity: 0.902364325419225\n",
      "SS step: substitute: prerequisite, cosine similarity: 0.9011738632767057\n",
      "SS step: substitute: ubiquitous, cosine similarity: 0.9015520208694314\n",
      "SS step: substitute: involuntary, cosine similarity: 0.9009861145880484\n",
      "SS step: substitute: forcibly, cosine similarity: 0.9016239806819277\n",
      "SS step: substitute: feasible, cosine similarity: 0.9013101793633413\n",
      "SS step: substitute: compelled, cosine similarity: 0.9012277426651089\n",
      "SS step: substitute: indispensable, cosine similarity: 0.9017168390038571\n",
      "SS step: substitute: necessary, cosine similarity: 0.9025668849474453\n",
      "SS step: substitute: obligated, cosine similarity: 0.9012847044358692\n",
      "SS step: substitute: liable, cosine similarity: 0.90174792322184\n",
      "SS step: substitute: habitual, cosine similarity: 0.9013323224941745\n",
      "SS step: substitute: enforced, cosine similarity: 0.9019468303704508\n",
      "SS step: substitute: universal, cosine similarity: 0.9022740850789331\n",
      "SS step: substitute: contingent, cosine similarity: 0.9021562234337552\n",
      "SS step: substitute: commonplace, cosine similarity: 0.9014147944611076\n",
      "\n",
      "Sentence: Rajoy's conservative government had instilled markets with a brief dose of confidence by stepping into Bankia, performing a U-turn on its refusal to spend public money to rescue banks.\n",
      "Complex word: instilled\n",
      "SG step: generated substitutes: ['illed', 'infused', 'injected', 'filled', 'impressed', 'endowed', 'pumped', 'bolstered', 'furnished', 'struck', 'inflated', 'invaded', 'reassured', 'augmented', 'populated', 'stunned', 'filled', 'supplied', 'poisoned', 'fortified', 'reinforced', 'provided', 'enhanced', 'flooded', 'chilled', 'shaken', 'revived', 'intoxicated', 'implanted', 'drilled']\n",
      "\n",
      "SS step: substitute: illed, cosine similarity: 0.8699870415483291\n",
      "SS step: substitute: infused, cosine similarity: 0.8646815352524341\n",
      "SS step: substitute: injected, cosine similarity: 0.8646787688692084\n",
      "SS step: substitute: filled, cosine similarity: 0.865710853747208\n",
      "SS step: substitute: impressed, cosine similarity: 0.8649943449952617\n",
      "SS step: substitute: endowed, cosine similarity: 0.8644141856782374\n",
      "SS step: substitute: pumped, cosine similarity: 0.8649636155931576\n",
      "SS step: substitute: bolstered, cosine similarity: 0.8641681044783808\n",
      "SS step: substitute: furnished, cosine similarity: 0.8646894063299287\n",
      "SS step: substitute: struck, cosine similarity: 0.8654988309926535\n",
      "SS step: substitute: inflated, cosine similarity: 0.8644588061496588\n",
      "SS step: substitute: invaded, cosine similarity: 0.8641992440330479\n",
      "SS step: substitute: reassured, cosine similarity: 0.8639151657847376\n",
      "SS step: substitute: augmented, cosine similarity: 0.8643669405918023\n",
      "SS step: substitute: populated, cosine similarity: 0.8647979529479907\n",
      "SS step: substitute: stunned, cosine similarity: 0.8645793408306383\n",
      "SS step: substitute: filled, cosine similarity: 0.865710853747208\n",
      "SS step: substitute: supplied, cosine similarity: 0.8649488587050633\n",
      "SS step: substitute: poisoned, cosine similarity: 0.8645867653799378\n",
      "SS step: substitute: fortified, cosine similarity: 0.86447688098369\n",
      "SS step: substitute: reinforced, cosine similarity: 0.8645018758800483\n",
      "SS step: substitute: provided, cosine similarity: 0.8658080736604206\n",
      "SS step: substitute: enhanced, cosine similarity: 0.8647369530129447\n",
      "SS step: substitute: flooded, cosine similarity: 0.8649805593117659\n",
      "SS step: substitute: chilled, cosine similarity: 0.8645397884784023\n",
      "SS step: substitute: shaken, cosine similarity: 0.8648799057835227\n",
      "SS step: substitute: revived, cosine similarity: 0.864771484692215\n",
      "SS step: substitute: intoxicated, cosine similarity: 0.8642083341732889\n",
      "SS step: substitute: implanted, cosine similarity: 0.863928619929659\n",
      "SS step: substitute: drilled, cosine similarity: 0.8648102566390281\n",
      "\n",
      "Sentence: #34-3 \"War maniacs of the South Korean puppet military made another grave provocation to the DPRK in the central western sector of the front on Thursday afternoon.\n",
      "Complex word: maniacs\n",
      "SG step: generated substitutes: ['maniac', 'heads', 'criminals', 'hawks', 'lords', 'gangs', 'fighters', 'riors', 'thugs', 'bosses', 'killers', 'militias', 'drums', 'forces', 'bugs', 'planes', 'foes', 'igans', 'monsters', 'nerds', 'opponents', 'devils', 'buddies', 'storms', 'creatures', 'rs', 'defenders', 'planners', 'flies', 'makers']\n",
      "\n",
      "SS step: substitute: maniac, cosine similarity: 0.8215144198187556\n",
      "SS step: substitute: heads, cosine similarity: 0.8231937055252363\n",
      "SS step: substitute: criminals, cosine similarity: 0.821293577365428\n",
      "SS step: substitute: hawks, cosine similarity: 0.8284007069581503\n",
      "SS step: substitute: lords, cosine similarity: 0.8218986707627651\n",
      "SS step: substitute: gangs, cosine similarity: 0.8218573087746599\n",
      "SS step: substitute: fighters, cosine similarity: 0.8217800556439383\n",
      "SS step: substitute: riors, cosine similarity: 0.8283137540594925\n",
      "SS step: substitute: thugs, cosine similarity: 0.8214568897886807\n",
      "SS step: substitute: bosses, cosine similarity: 0.8221540201959779\n",
      "SS step: substitute: killers, cosine similarity: 0.8220663168955675\n",
      "SS step: substitute: militias, cosine similarity: 0.8212528989815078\n",
      "SS step: substitute: drums, cosine similarity: 0.8223977830032376\n",
      "SS step: substitute: forces, cosine similarity: 0.8227342199480714\n",
      "SS step: substitute: bugs, cosine similarity: 0.8220670864633064\n",
      "SS step: substitute: planes, cosine similarity: 0.8225698092817375\n",
      "SS step: substitute: foes, cosine similarity: 0.8220927357466895\n",
      "SS step: substitute: igans, cosine similarity: 0.8277825000280712\n",
      "SS step: substitute: monsters, cosine similarity: 0.8215679655624204\n",
      "SS step: substitute: nerds, cosine similarity: 0.8213515202017431\n",
      "SS step: substitute: opponents, cosine similarity: 0.8224823368585849\n",
      "SS step: substitute: devils, cosine similarity: 0.8217042751611962\n",
      "SS step: substitute: buddies, cosine similarity: 0.8216283553041476\n",
      "SS step: substitute: storms, cosine similarity: 0.8218511863344028\n",
      "SS step: substitute: creatures, cosine similarity: 0.8216749015225222\n",
      "SS step: substitute: rs, cosine similarity: 0.8215098757885694\n",
      "SS step: substitute: defenders, cosine similarity: 0.8221497814049359\n",
      "SS step: substitute: planners, cosine similarity: 0.8217339586910326\n",
      "SS step: substitute: flies, cosine similarity: 0.8230707452420486\n",
      "SS step: substitute: makers, cosine similarity: 0.8226683180741227\n",
      "\n",
      "Sentence: The daily death toll in Syria has declined as the number of observers has risen, but few experts expect the U.N. plan to succeed in its entirety.\n",
      "Complex word: observers\n",
      "SG step: generated substitutes: ['observers', 'spectators', 'observes', 'viewers', 'witnesses', 'observer', 'participants', 'visitors', 'commentators', 'residents', 'monitors', 'users', 'observations', 'protestors', 'supporters', 'inhabitants', 'analysts', 'eyes', 'protesters', 'respondents', 'observing', 'demonstrators', 'opponents', 'bystanders', 'civilians', 'outsiders', 'astronomers', 'followers', 'neighbors', 'researchers']\n",
      "\n",
      "SS step: substitute: observers, cosine similarity: 0.8578603588623062\n",
      "SS step: substitute: spectators, cosine similarity: 0.8581077009190341\n",
      "SS step: substitute: observes, cosine similarity: 0.8580284940685339\n",
      "SS step: substitute: viewers, cosine similarity: 0.8583675644213214\n",
      "SS step: substitute: witnesses, cosine similarity: 0.8586937849679543\n",
      "SS step: substitute: observer, cosine similarity: 0.8584808669610353\n",
      "SS step: substitute: participants, cosine similarity: 0.8585403475176319\n",
      "SS step: substitute: visitors, cosine similarity: 0.858958236741655\n",
      "SS step: substitute: commentators, cosine similarity: 0.8578205422445984\n",
      "SS step: substitute: residents, cosine similarity: 0.858588285931301\n",
      "SS step: substitute: monitors, cosine similarity: 0.8584169987504411\n",
      "SS step: substitute: users, cosine similarity: 0.8586632317770437\n",
      "SS step: substitute: observations, cosine similarity: 0.858356425802461\n",
      "SS step: substitute: protestors, cosine similarity: 0.8577815289808701\n",
      "SS step: substitute: supporters, cosine similarity: 0.858430190564797\n",
      "SS step: substitute: inhabitants, cosine similarity: 0.8578874576339832\n",
      "SS step: substitute: analysts, cosine similarity: 0.8585709093692345\n",
      "SS step: substitute: eyes, cosine similarity: 0.8593486578904488\n",
      "SS step: substitute: protesters, cosine similarity: 0.8579174078604372\n",
      "SS step: substitute: respondents, cosine similarity: 0.8577966962206425\n",
      "SS step: substitute: observing, cosine similarity: 0.8582146863718677\n",
      "SS step: substitute: demonstrators, cosine similarity: 0.8574568722823727\n",
      "SS step: substitute: opponents, cosine similarity: 0.8588483050129525\n",
      "SS step: substitute: bystanders, cosine similarity: 0.8578987301201093\n",
      "SS step: substitute: civilians, cosine similarity: 0.8577796780204848\n",
      "SS step: substitute: outsiders, cosine similarity: 0.8583051113554524\n",
      "SS step: substitute: astronomers, cosine similarity: 0.8574642297020096\n",
      "SS step: substitute: followers, cosine similarity: 0.8586861823338071\n",
      "SS step: substitute: neighbors, cosine similarity: 0.8588185311827898\n",
      "SS step: substitute: researchers, cosine similarity: 0.8582900348301937\n",
      "\n",
      "Sentence: An amateur video showed a young girl who apparently suffered shrapnel wounds in her thigh undergoing treatment in a makeshift Rastan hospital while screaming in pain.\n",
      "Complex word: shrapnel\n",
      "SG step: generated substitutes: ['bullet', 'stab', 'rapnel', 'gunshot', 'torn', 'trauma', 'blast', 'deep', 'raw', 'small', 'severe', 'knife', 'radiation', 'injury', 'the', 'tore', 'internal', 'a', 'similar', 'superficial', 'compression', 'traumatic', 'such', 'thigh', 'inflicted', 'gaping', 'scratch', 'tiny', 'battered', 'some']\n",
      "\n",
      "SS step: substitute: bullet, cosine similarity: 0.8205240079803301\n",
      "SS step: substitute: stab, cosine similarity: 0.8201174725676521\n",
      "SS step: substitute: rapnel, cosine similarity: 0.8267909522569052\n",
      "SS step: substitute: gunshot, cosine similarity: 0.8183915861690676\n",
      "SS step: substitute: torn, cosine similarity: 0.8204568008744006\n",
      "SS step: substitute: trauma, cosine similarity: 0.8189807322749183\n",
      "SS step: substitute: blast, cosine similarity: 0.8205151894448856\n",
      "SS step: substitute: deep, cosine similarity: 0.8212742530841728\n",
      "SS step: substitute: raw, cosine similarity: 0.8205649591259121\n",
      "SS step: substitute: small, cosine similarity: 0.8199834287026369\n",
      "SS step: substitute: severe, cosine similarity: 0.8190659096048062\n",
      "SS step: substitute: knife, cosine similarity: 0.819522270160523\n",
      "SS step: substitute: radiation, cosine similarity: 0.8193519489980753\n",
      "SS step: substitute: injury, cosine similarity: 0.8196605804551395\n",
      "SS step: substitute: the, cosine similarity: 0.8196218813330847\n",
      "SS step: substitute: tore, cosine similarity: 0.8193204781365961\n",
      "SS step: substitute: internal, cosine similarity: 0.8202822198817833\n",
      "SS step: substitute: a, cosine similarity: 0.819708533341867\n",
      "SS step: substitute: similar, cosine similarity: 0.8200109338725601\n",
      "SS step: substitute: superficial, cosine similarity: 0.818631529025008\n",
      "SS step: substitute: compression, cosine similarity: 0.8191770570490375\n",
      "SS step: substitute: traumatic, cosine similarity: 0.8187274245293918\n",
      "SS step: substitute: such, cosine similarity: 0.8203750111345846\n",
      "SS step: substitute: thigh, cosine similarity: 0.8188880375643353\n",
      "SS step: substitute: inflicted, cosine similarity: 0.8187693221370198\n",
      "SS step: substitute: gaping, cosine similarity: 0.8190335650055698\n",
      "SS step: substitute: scratch, cosine similarity: 0.8208923380023965\n",
      "SS step: substitute: tiny, cosine similarity: 0.8192158697410162\n",
      "SS step: substitute: battered, cosine similarity: 0.819175536321346\n",
      "SS step: substitute: some, cosine similarity: 0.819581584406101\n",
      "\n",
      "Sentence: A local witness said a separate group of attackers disguised in burqas — the head-to-toe robes worn by conservative Afghan women — then tried to storm the compound.\n",
      "Complex word: disguised\n",
      "SG step: generated substitutes: ['disguised', 'veiled', 'masked', 'concealed', 'dressed', 'hidden', 'disguise', 'clothed', 'cloaked', 'shrouded', 'posed', 'clad', 'hiding', 'covered', 'wrapped', 'camoufl', 'disgu', 'posing', 'camouflage', 'presented', 'naked', 'trained', 'draped', 'buried', 'framed', 'hid', 'manifested', 'obscured', 'armed', 'smuggled']\n",
      "\n",
      "SS step: substitute: disguised, cosine similarity: 0.8556850126788013\n",
      "SS step: substitute: veiled, cosine similarity: 0.8555604652951889\n",
      "SS step: substitute: masked, cosine similarity: 0.8554422900701426\n",
      "SS step: substitute: concealed, cosine similarity: 0.8554857940166373\n",
      "SS step: substitute: dressed, cosine similarity: 0.8558756245887176\n",
      "SS step: substitute: hidden, cosine similarity: 0.8561717356462785\n",
      "SS step: substitute: disguise, cosine similarity: 0.8556791535446077\n",
      "SS step: substitute: clothed, cosine similarity: 0.8553670606762817\n",
      "SS step: substitute: cloaked, cosine similarity: 0.8548429142103003\n",
      "SS step: substitute: shrouded, cosine similarity: 0.8551913550730985\n",
      "SS step: substitute: posed, cosine similarity: 0.8560944936993296\n",
      "SS step: substitute: clad, cosine similarity: 0.8561164082730721\n",
      "SS step: substitute: hiding, cosine similarity: 0.8556442294382043\n",
      "SS step: substitute: covered, cosine similarity: 0.856728349359864\n",
      "SS step: substitute: wrapped, cosine similarity: 0.8558041613294574\n",
      "SS step: substitute: camoufl, cosine similarity: 0.8546807655441782\n",
      "SS step: substitute: disgu, cosine similarity: 0.8551746199551977\n",
      "SS step: substitute: posing, cosine similarity: 0.8559236595667277\n",
      "SS step: substitute: camouflage, cosine similarity: 0.8551068465121852\n",
      "SS step: substitute: presented, cosine similarity: 0.8561732001725004\n",
      "SS step: substitute: naked, cosine similarity: 0.856127790785737\n",
      "SS step: substitute: trained, cosine similarity: 0.8563770107668324\n",
      "SS step: substitute: draped, cosine similarity: 0.8551449759634718\n",
      "SS step: substitute: buried, cosine similarity: 0.8560883954267435\n",
      "SS step: substitute: framed, cosine similarity: 0.855990722804941\n",
      "SS step: substitute: hid, cosine similarity: 0.8557446167754529\n",
      "SS step: substitute: manifested, cosine similarity: 0.8548818105942927\n",
      "SS step: substitute: obscured, cosine similarity: 0.8549241300922756\n",
      "SS step: substitute: armed, cosine similarity: 0.8566317966536531\n",
      "SS step: substitute: smuggled, cosine similarity: 0.8552226839756988\n",
      "\n",
      "Sentence: Syria's Sunni majority is at the forefront of the uprising against Assad, whose minority Alawite sect is an offshoot of Shi'ite Islam.\n",
      "Complex word: offshoot\n",
      "SG step: generated substitutes: ['off', 'extension', 'echo', 'offspring', 'out', 'element', 'outpost', 'imprint', 'opposite', 'example', 'arm', 'affiliate', 'expansion', 'analogue', 'archetype', 'idea', 'off', 'overthrow', 'enemy', 'indication', 'adjunct', 'alternative', 'opposition', 'opponent', 'ideology', 'expression', 'embryo', 'outside', 'imitation', 'offshore']\n",
      "\n",
      "SS step: substitute: off, cosine similarity: 0.834242337517315\n",
      "SS step: substitute: extension, cosine similarity: 0.8343847909553712\n",
      "SS step: substitute: echo, cosine similarity: 0.8338956541635669\n",
      "SS step: substitute: offspring, cosine similarity: 0.8327578937214754\n",
      "SS step: substitute: out, cosine similarity: 0.8340282066002035\n",
      "SS step: substitute: element, cosine similarity: 0.834343926696643\n",
      "SS step: substitute: outpost, cosine similarity: 0.832983684078322\n",
      "SS step: substitute: imprint, cosine similarity: 0.8334657792827976\n",
      "SS step: substitute: opposite, cosine similarity: 0.8346510584792054\n",
      "SS step: substitute: example, cosine similarity: 0.8342065943824979\n",
      "SS step: substitute: arm, cosine similarity: 0.8347214361799127\n",
      "SS step: substitute: affiliate, cosine similarity: 0.833981063966243\n",
      "SS step: substitute: expansion, cosine similarity: 0.833976126719889\n",
      "SS step: substitute: analogue, cosine similarity: 0.8335579821299605\n",
      "SS step: substitute: archetype, cosine similarity: 0.8322235399205038\n",
      "SS step: substitute: idea, cosine similarity: 0.8339680198704356\n",
      "SS step: substitute: off, cosine similarity: 0.834242337517315\n",
      "SS step: substitute: overthrow, cosine similarity: 0.832289395438048\n",
      "SS step: substitute: enemy, cosine similarity: 0.8337975022422563\n",
      "SS step: substitute: indication, cosine similarity: 0.8331061735825507\n",
      "SS step: substitute: adjunct, cosine similarity: 0.832650890407574\n",
      "SS step: substitute: alternative, cosine similarity: 0.8339431905445613\n",
      "SS step: substitute: opposition, cosine similarity: 0.834531799437014\n",
      "SS step: substitute: opponent, cosine similarity: 0.8336742386927901\n",
      "SS step: substitute: ideology, cosine similarity: 0.832783136682843\n",
      "SS step: substitute: expression, cosine similarity: 0.8341192171111851\n",
      "SS step: substitute: embryo, cosine similarity: 0.8325103968549525\n",
      "SS step: substitute: outside, cosine similarity: 0.8342069494971452\n",
      "SS step: substitute: imitation, cosine similarity: 0.8337503843968271\n",
      "SS step: substitute: offshore, cosine similarity: 0.8336438201857692\n",
      "\n",
      "Sentence: Although not as rare in the symphonic literature as sharper keys , examples of symphonies in A major are not as numerous as for D major or G major .\n",
      "Complex word: symphonic\n",
      "SG step: generated substitutes: ['musical', 'music', 'classical', 'literary', 'sonic', 'Greek', 'educational', 'harmonic', 'same', 'the', 'instrumental', 'English', 'academic', 'lyric', 'epic', 'esoteric', 'Classical', 'composition', 'jazz', 'major', 'pop', 'modern', 'Italian', 'formal', 'Japanese', 'onic', 'conceptual', 'verbal', 'Renaissance', 'common']\n",
      "\n",
      "SS step: substitute: musical, cosine similarity: 0.8680572374140515\n",
      "SS step: substitute: music, cosine similarity: 0.8682736102817463\n",
      "SS step: substitute: classical, cosine similarity: 0.8672929488348311\n",
      "SS step: substitute: literary, cosine similarity: 0.867109282053881\n",
      "SS step: substitute: sonic, cosine similarity: 0.8676532713331236\n",
      "SS step: substitute: Greek, cosine similarity: 0.8681232529124735\n",
      "SS step: substitute: educational, cosine similarity: 0.8676046243811759\n",
      "SS step: substitute: harmonic, cosine similarity: 0.8667565745912897\n",
      "SS step: substitute: same, cosine similarity: 0.8685965864585194\n",
      "SS step: substitute: the, cosine similarity: 0.8679608401332909\n",
      "SS step: substitute: instrumental, cosine similarity: 0.8675025933739093\n",
      "SS step: substitute: English, cosine similarity: 0.8685652499527887\n",
      "SS step: substitute: academic, cosine similarity: 0.8679429794439829\n",
      "SS step: substitute: lyric, cosine similarity: 0.8673066731962101\n",
      "SS step: substitute: epic, cosine similarity: 0.8679957245739922\n",
      "SS step: substitute: esoteric, cosine similarity: 0.8663633135015879\n",
      "SS step: substitute: Classical, cosine similarity: 0.8672256100327118\n",
      "SS step: substitute: composition, cosine similarity: 0.8676105383672045\n",
      "SS step: substitute: jazz, cosine similarity: 0.8675916564834744\n",
      "SS step: substitute: major, cosine similarity: 0.868411901778605\n",
      "SS step: substitute: pop, cosine similarity: 0.8688151844162263\n",
      "SS step: substitute: modern, cosine similarity: 0.8683572562310751\n",
      "SS step: substitute: Italian, cosine similarity: 0.8679394662429843\n",
      "SS step: substitute: formal, cosine similarity: 0.8679672264745326\n",
      "SS step: substitute: Japanese, cosine similarity: 0.8678195273786791\n",
      "SS step: substitute: onic, cosine similarity: 0.8725805089690079\n",
      "SS step: substitute: conceptual, cosine similarity: 0.8671446654865615\n",
      "SS step: substitute: verbal, cosine similarity: 0.867779417192456\n",
      "SS step: substitute: Renaissance, cosine similarity: 0.8679082314579666\n",
      "SS step: substitute: common, cosine similarity: 0.8687435498219094\n",
      "\n",
      "Sentence: That prompted the military to deploy its largest warship, the BRP Gregorio del Pilar, which was recently acquired from the United States.\n",
      "Complex word: deploy\n",
      "SG step: generated substitutes: ['deploy', 'employ', 'mobilize', 'deployed', 'utilize', 'deployment', 'deploying', 'ploy', 'equip', 'dispatch', 'activate', 'use', 'Deploy', 'possess', 'cultivate', 'unveil', 'unleash', 'wield', 'manufacture', 'summon', 'populate', 'expend', 'install', 'devote', 'propel', 'Deploy', 'spawn', 'acquire', 'recruit', 'operate']\n",
      "\n",
      "SS step: substitute: deploy, cosine similarity: 0.8130372079769735\n",
      "SS step: substitute: employ, cosine similarity: 0.8128934456593858\n",
      "SS step: substitute: mobilize, cosine similarity: 0.8117894757900683\n",
      "SS step: substitute: deployed, cosine similarity: 0.8126207432040058\n",
      "SS step: substitute: utilize, cosine similarity: 0.8124444041441365\n",
      "SS step: substitute: deployment, cosine similarity: 0.8128060972816695\n",
      "SS step: substitute: deploying, cosine similarity: 0.8124738149297668\n",
      "SS step: substitute: ploy, cosine similarity: 0.8119947810518301\n",
      "SS step: substitute: equip, cosine similarity: 0.813375043426326\n",
      "SS step: substitute: dispatch, cosine similarity: 0.8133631847944013\n",
      "SS step: substitute: activate, cosine similarity: 0.8124022405817448\n",
      "SS step: substitute: use, cosine similarity: 0.8139038145638307\n",
      "SS step: substitute: Deploy, cosine similarity: 0.8120662116212501\n",
      "SS step: substitute: possess, cosine similarity: 0.8126260501071432\n",
      "SS step: substitute: cultivate, cosine similarity: 0.8119525642895283\n",
      "SS step: substitute: unveil, cosine similarity: 0.8120509977069813\n",
      "SS step: substitute: unleash, cosine similarity: 0.8125334500026258\n",
      "SS step: substitute: wield, cosine similarity: 0.8124595853887819\n",
      "SS step: substitute: manufacture, cosine similarity: 0.8123582798502323\n",
      "SS step: substitute: summon, cosine similarity: 0.8129535018103223\n",
      "SS step: substitute: populate, cosine similarity: 0.8125908298908205\n",
      "SS step: substitute: expend, cosine similarity: 0.8123458913410271\n",
      "SS step: substitute: install, cosine similarity: 0.8128599692403706\n",
      "SS step: substitute: devote, cosine similarity: 0.8129666822305178\n",
      "SS step: substitute: propel, cosine similarity: 0.8118298290120263\n",
      "SS step: substitute: Deploy, cosine similarity: 0.8120662116212501\n",
      "SS step: substitute: spawn, cosine similarity: 0.8134596285075476\n",
      "SS step: substitute: acquire, cosine similarity: 0.8126497672607246\n",
      "SS step: substitute: recruit, cosine similarity: 0.8128995581271443\n",
      "SS step: substitute: operate, cosine similarity: 0.8128782518039117\n",
      "\n",
      "Sentence: #35-14 UK police were expressly forbidden, at a ministerial level, to provide any assistance to Thai authorities as the case involves the death penalty.\n",
      "Complex word: authorities\n",
      "SG step: generated substitutes: ['authorities', 'Authorities', 'officials', 'authority', 'police', 'ís', 'Authorities', 'superiors', 'administrators', 'entities', 'officers', 'residents', 'counterparts', 'individuals', 'prosecutors', ',', 'jurisdictions', 'forces', 'elders', 'cops', 'regimes', 'masters', 'bureaucrats', 'elites', 'Iranians', 'respondents', 'ians', 'investigators', 'ese', 'constituents']\n",
      "\n",
      "SS step: substitute: authorities, cosine similarity: 0.8015049829409079\n",
      "SS step: substitute: Authorities, cosine similarity: 0.8006117754059943\n",
      "SS step: substitute: officials, cosine similarity: 0.8013155580585168\n",
      "SS step: substitute: authority, cosine similarity: 0.8023804893822206\n",
      "SS step: substitute: police, cosine similarity: 0.8019819877419578\n",
      "SS step: substitute: ís, cosine similarity: 0.8123868414871864\n",
      "SS step: substitute: Authorities, cosine similarity: 0.8006117754059943\n",
      "SS step: substitute: superiors, cosine similarity: 0.8006157859453618\n",
      "SS step: substitute: administrators, cosine similarity: 0.8007220226364649\n",
      "SS step: substitute: entities, cosine similarity: 0.8006307767957879\n",
      "SS step: substitute: officers, cosine similarity: 0.8010896009620601\n",
      "SS step: substitute: residents, cosine similarity: 0.8013718713236062\n",
      "SS step: substitute: counterparts, cosine similarity: 0.801771811617729\n",
      "SS step: substitute: individuals, cosine similarity: 0.8011064221192491\n",
      "SS step: substitute: prosecutors, cosine similarity: 0.8011054447318848\n",
      "SS step: substitute: ,, cosine similarity: 0.801802259570429\n",
      "SS step: substitute: jurisdictions, cosine similarity: 0.8007417820558147\n",
      "SS step: substitute: forces, cosine similarity: 0.8020647436806917\n",
      "SS step: substitute: elders, cosine similarity: 0.8012180925065324\n",
      "SS step: substitute: cops, cosine similarity: 0.8007551115551594\n",
      "SS step: substitute: regimes, cosine similarity: 0.8011743519419612\n",
      "SS step: substitute: masters, cosine similarity: 0.80206976618649\n",
      "SS step: substitute: bureaucrats, cosine similarity: 0.8000553068741629\n",
      "SS step: substitute: elites, cosine similarity: 0.8003296126823448\n",
      "SS step: substitute: Iranians, cosine similarity: 0.8008280146378592\n",
      "SS step: substitute: respondents, cosine similarity: 0.8006388662993789\n",
      "SS step: substitute: ians, cosine similarity: 0.808407962177579\n",
      "SS step: substitute: investigators, cosine similarity: 0.8011181612755701\n",
      "SS step: substitute: ese, cosine similarity: 0.8084378283703977\n",
      "SS step: substitute: constituents, cosine similarity: 0.8010181164765978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sklearn cosine similarity with padded sentence lengths to account for different lengths due to substitutes\n",
    "# problem: values are very close to each other, probably the cosine similarity of both sentences is calculated. And a complex word that is the same as its substitute did not get a value near 1.\n",
    "\n",
    "\n",
    "\n",
    "# set the maximum length for both sentences\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, \"<mask>\")\n",
    "\n",
    "    ## concatenate the sentence with the complex word and the sentence with the masked word, by using RoBERTa's separator token to create one string of both sentences\n",
    "    sentences_concat = f\"{sentence} {tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## tokenize the concatenated sentence\n",
    "    sentences_concat_tokenized = tokenizer.encode(sentences_concat, add_special_tokens=True)\n",
    "\n",
    "    ## make sure the input length is not longer than max_length\n",
    "    if len(sentences_concat_tokenized) > max_length:\n",
    "        sentences_concat_tokenized = sentences_concat_tokenized[:max_length]\n",
    "\n",
    "    ## pad the tokenized sentence to max_length\n",
    "    padding_length = max_length - len(sentences_concat_tokenized)\n",
    "    sentences_concat_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    ## convert the tokenized sentence to a tensor\n",
    "    sentences_concat_tokenized = torch.tensor(sentences_concat_tokenized)\n",
    "\n",
    "    ## find the masked word in the tokenized sentence\n",
    "    mask_location = torch.where(sentences_concat_tokenized == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "    ## generate predictions for the masked word (forward pass not needed for predictions, only for training)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentences_concat_tokenized.unsqueeze(0))\n",
    "        predictions = outputs.logits[0]\n",
    "\n",
    "    ## get the top-k substitutes based on the predicted logits\n",
    "    top_k = 30\n",
    "    top_tokens = torch.topk(predictions[mask_location], top_k).indices\n",
    "\n",
    "    ## decode the top-k substitutes and print them\n",
    "    substitutes = [tokenizer.decode(token.item()).strip() for token in top_tokens]\n",
    "    print(f\"SG step: generated substitutes: {substitutes}\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # 2. Substitute Selection (SS): apply cosine similarity, of:\n",
    "    # - The contextualized embedding of the complex word in the context of the original sentence, \n",
    "    # - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\n",
    "\n",
    "    ## get the contextualized embedding of the complex word in the original sentence\n",
    "    with torch.no_grad():\n",
    "        complex_word_tokenized = tokenizer.encode(complex_word, add_special_tokens=True)\n",
    "\n",
    "        ## make sure the input length is not longer than max_length\n",
    "        if len(complex_word_tokenized) > max_length:\n",
    "            complex_word_tokenized = complex_word_tokenized[:max_length]\n",
    "\n",
    "        ## pad the tokenized sentence to max_length\n",
    "        padding_length = max_length - len(complex_word_tokenized)\n",
    "        complex_word_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "        ## convert the tokenized sentence to a tensor\n",
    "        complex_word_tokenized = torch.tensor(complex_word_tokenized)\n",
    "\n",
    "        complex_word_embedding = model.get_input_embeddings()(complex_word_tokenized.unsqueeze(0)).squeeze(0).flatten()\n",
    "        complex_word_embedding_norm = normalize(complex_word_embedding.reshape(1, -1))\n",
    "\n",
    "\n",
    "    ## calculate the cosine similarity between the complex word embedding and each substitute embedding\n",
    "    for substitute in substitutes:\n",
    "        ## replace the complex word in the original sentence with the substitute\n",
    "        sentence_substitute = sentence.replace(complex_word, substitute)\n",
    "\n",
    "        ## tokenize the sentence with the substitute\n",
    "        sentence_substitute_tokenized = tokenizer.encode(sentence_substitute, add_special_tokens=True)\n",
    "\n",
    "        ## make sure the input length is not longer than max_length\n",
    "        if len(sentence_substitute_tokenized) > max_length:\n",
    "            sentence_substitute_tokenized = sentence_substitute_tokenized[:max_length]\n",
    "\n",
    "        ## pad the tokenized sentence to max_length\n",
    "        padding_length = max_length - len(sentence_substitute_tokenized)\n",
    "        sentence_substitute_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "        ## convert the tokenized sentence to a tensor\n",
    "        sentence_substitute_tokenized = torch.tensor(sentence_substitute_tokenized)\n",
    "\n",
    "        ## get the contextualized embedding of the substitute word in the modified sentence\n",
    "        with torch.no_grad():\n",
    "            substitute_embedding = model.get_input_embeddings()(sentence_substitute_tokenized.unsqueeze(0)).squeeze(0).flatten()\n",
    "            substitute_embedding_norm = normalize(substitute_embedding.reshape(1, -1))\n",
    "\n",
    "\n",
    "        ## calculate cosine similarity\n",
    "        similarity = cosine_similarity(complex_word_embedding_norm, substitute_embedding_norm)\n",
    "        print(f\"SS step: substitute: {substitute}, cosine similarity: {1-similarity[0][0]}\")\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6c032b-08c4-4bd1-9097-df25e8b42d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "SG step: generated substitutes: ['compulsory', 'mandatory', 'obligatory', 'obliged', 'voluntary', 'uniform', 'required', 'mandated', 'optional', 'dogma', 'forced', 'intrinsic', 'advisable', 'routine', 'forthcoming', 'prerequisite', 'ubiquitous', 'involuntary', 'forcibly', 'feasible', 'compelled', 'indispensable', 'necessary', 'obligated', 'liable', 'habitual', 'enforced', 'universal', 'contingent', 'commonplace']\n",
      "\n",
      "SS step: substitute: advisable, cosine similarity: 0.0994430327015135\n",
      "SS step: substitute: involuntary, cosine similarity: 0.0990138854119515\n",
      "SS step: substitute: prerequisite, cosine similarity: 0.09882613672329427\n",
      "SS step: substitute: compelled, cosine similarity: 0.09877225733489117\n",
      "SS step: substitute: obligated, cosine similarity: 0.09871529556413078\n",
      "SS step: substitute: intrinsic, cosine similarity: 0.09870918758687189\n",
      "SS step: substitute: feasible, cosine similarity: 0.09868982063665861\n",
      "SS step: substitute: habitual, cosine similarity: 0.09866767750582553\n",
      "SS step: substitute: dogma, cosine similarity: 0.09864978852442297\n",
      "SS step: substitute: commonplace, cosine similarity: 0.09858520553889237\n",
      "SS step: substitute: obligatory, cosine similarity: 0.09851865778358312\n",
      "SS step: substitute: compulsory, cosine similarity: 0.09849535137087737\n",
      "SS step: substitute: ubiquitous, cosine similarity: 0.09844797913056857\n",
      "SS step: substitute: obliged, cosine similarity: 0.09843752824076288\n",
      "SS step: substitute: forcibly, cosine similarity: 0.09837601931807223\n",
      "SS step: substitute: mandated, cosine similarity: 0.09834260286297013\n",
      "SS step: substitute: indispensable, cosine similarity: 0.09828316099614283\n",
      "SS step: substitute: liable, cosine similarity: 0.09825207677815997\n",
      "SS step: substitute: enforced, cosine similarity: 0.0980531696295493\n",
      "SS step: substitute: mandatory, cosine similarity: 0.09788106405903885\n",
      "SS step: substitute: optional, cosine similarity: 0.09787783558855577\n",
      "SS step: substitute: contingent, cosine similarity: 0.09784377656624482\n",
      "SS step: substitute: voluntary, cosine similarity: 0.09781149876244309\n",
      "SS step: substitute: universal, cosine similarity: 0.09772591492106684\n",
      "SS step: substitute: forthcoming, cosine similarity: 0.097635674580775\n",
      "SS step: substitute: forced, cosine similarity: 0.09763549030213177\n",
      "SS step: substitute: uniform, cosine similarity: 0.09745625518832966\n",
      "SS step: substitute: necessary, cosine similarity: 0.09743311505255467\n",
      "SS step: substitute: required, cosine similarity: 0.09735470642963084\n",
      "SS step: substitute: routine, cosine similarity: 0.0971715299353938\n",
      "\n",
      "Sentence: Rajoy's conservative government had instilled markets with a brief dose of confidence by stepping into Bankia, performing a U-turn on its refusal to spend public money to rescue banks.\n",
      "Complex word: instilled\n",
      "SG step: generated substitutes: ['illed', 'infused', 'injected', 'filled', 'impressed', 'endowed', 'pumped', 'bolstered', 'furnished', 'struck', 'inflated', 'invaded', 'reassured', 'augmented', 'populated', 'stunned', 'filled', 'supplied', 'poisoned', 'fortified', 'reinforced', 'provided', 'enhanced', 'flooded', 'chilled', 'shaken', 'revived', 'intoxicated', 'implanted', 'drilled']\n",
      "\n",
      "SS step: substitute: reassured, cosine similarity: 0.13608483421526243\n",
      "SS step: substitute: implanted, cosine similarity: 0.13607138007034103\n",
      "SS step: substitute: bolstered, cosine similarity: 0.13583189552161926\n",
      "SS step: substitute: invaded, cosine similarity: 0.13580075596695207\n",
      "SS step: substitute: intoxicated, cosine similarity: 0.13579166582671112\n",
      "SS step: substitute: augmented, cosine similarity: 0.13563305940819767\n",
      "SS step: substitute: endowed, cosine similarity: 0.13558581432176267\n",
      "SS step: substitute: inflated, cosine similarity: 0.13554119385034127\n",
      "SS step: substitute: fortified, cosine similarity: 0.13552311901630995\n",
      "SS step: substitute: reinforced, cosine similarity: 0.13549812411995163\n",
      "SS step: substitute: chilled, cosine similarity: 0.13546021152159776\n",
      "SS step: substitute: stunned, cosine similarity: 0.13542065916936175\n",
      "SS step: substitute: poisoned, cosine similarity: 0.1354132346200622\n",
      "SS step: substitute: injected, cosine similarity: 0.1353212311307917\n",
      "SS step: substitute: infused, cosine similarity: 0.13531846474756593\n",
      "SS step: substitute: furnished, cosine similarity: 0.13531059367007123\n",
      "SS step: substitute: enhanced, cosine similarity: 0.13526304698705532\n",
      "SS step: substitute: revived, cosine similarity: 0.135228515307785\n",
      "SS step: substitute: populated, cosine similarity: 0.13520204705200933\n",
      "SS step: substitute: drilled, cosine similarity: 0.13518974336097195\n",
      "SS step: substitute: shaken, cosine similarity: 0.13512009421647733\n",
      "SS step: substitute: supplied, cosine similarity: 0.13505114129493664\n",
      "SS step: substitute: pumped, cosine similarity: 0.13503638440684243\n",
      "SS step: substitute: flooded, cosine similarity: 0.13501944068823407\n",
      "SS step: substitute: impressed, cosine similarity: 0.13500565500473827\n",
      "SS step: substitute: struck, cosine similarity: 0.13450116900734652\n",
      "SS step: substitute: filled, cosine similarity: 0.134289146252792\n",
      "SS step: substitute: filled, cosine similarity: 0.134289146252792\n",
      "SS step: substitute: provided, cosine similarity: 0.13419192633957938\n",
      "SS step: substitute: illed, cosine similarity: 0.13001295845167096\n",
      "\n",
      "Sentence: #34-3 \"War maniacs of the South Korean puppet military made another grave provocation to the DPRK in the central western sector of the front on Thursday afternoon.\n",
      "Complex word: maniacs\n",
      "SG step: generated substitutes: ['maniac', 'heads', 'criminals', 'hawks', 'lords', 'gangs', 'fighters', 'riors', 'thugs', 'bosses', 'killers', 'militias', 'drums', 'forces', 'bugs', 'planes', 'foes', 'igans', 'monsters', 'nerds', 'opponents', 'devils', 'buddies', 'storms', 'creatures', 'rs', 'defenders', 'planners', 'flies', 'makers']\n",
      "\n",
      "SS step: substitute: militias, cosine similarity: 0.17874710101849228\n",
      "SS step: substitute: criminals, cosine similarity: 0.178706422634572\n",
      "SS step: substitute: nerds, cosine similarity: 0.1786484797982569\n",
      "SS step: substitute: thugs, cosine similarity: 0.17854311021131933\n",
      "SS step: substitute: rs, cosine similarity: 0.17849012421143065\n",
      "SS step: substitute: maniac, cosine similarity: 0.17848558018124439\n",
      "SS step: substitute: monsters, cosine similarity: 0.17843203443757966\n",
      "SS step: substitute: buddies, cosine similarity: 0.1783716446958524\n",
      "SS step: substitute: creatures, cosine similarity: 0.1783250984774778\n",
      "SS step: substitute: devils, cosine similarity: 0.17829572483880385\n",
      "SS step: substitute: planners, cosine similarity: 0.17826604130896742\n",
      "SS step: substitute: fighters, cosine similarity: 0.17821994435606164\n",
      "SS step: substitute: storms, cosine similarity: 0.17814881366559718\n",
      "SS step: substitute: gangs, cosine similarity: 0.17814269122534013\n",
      "SS step: substitute: lords, cosine similarity: 0.17810132923723485\n",
      "SS step: substitute: killers, cosine similarity: 0.17793368310443244\n",
      "SS step: substitute: bugs, cosine similarity: 0.1779329135366936\n",
      "SS step: substitute: foes, cosine similarity: 0.1779072642533105\n",
      "SS step: substitute: defenders, cosine similarity: 0.1778502185950641\n",
      "SS step: substitute: bosses, cosine similarity: 0.17784597980402203\n",
      "SS step: substitute: drums, cosine similarity: 0.17760221699676237\n",
      "SS step: substitute: opponents, cosine similarity: 0.17751766314141512\n",
      "SS step: substitute: planes, cosine similarity: 0.17743019071826255\n",
      "SS step: substitute: makers, cosine similarity: 0.17733168192587737\n",
      "SS step: substitute: forces, cosine similarity: 0.17726578005192867\n",
      "SS step: substitute: flies, cosine similarity: 0.17692925475795138\n",
      "SS step: substitute: heads, cosine similarity: 0.17680629447476373\n",
      "SS step: substitute: igans, cosine similarity: 0.17221749997192876\n",
      "SS step: substitute: riors, cosine similarity: 0.17168624594050755\n",
      "SS step: substitute: hawks, cosine similarity: 0.1715992930418498\n",
      "\n",
      "Sentence: The daily death toll in Syria has declined as the number of observers has risen, but few experts expect the U.N. plan to succeed in its entirety.\n",
      "Complex word: observers\n",
      "SG step: generated substitutes: ['observers', 'spectators', 'observes', 'viewers', 'witnesses', 'observer', 'participants', 'visitors', 'commentators', 'residents', 'monitors', 'users', 'observations', 'protestors', 'supporters', 'inhabitants', 'analysts', 'eyes', 'protesters', 'respondents', 'observing', 'demonstrators', 'opponents', 'bystanders', 'civilians', 'outsiders', 'astronomers', 'followers', 'neighbors', 'researchers']\n",
      "\n",
      "SS step: substitute: demonstrators, cosine similarity: 0.1425431277176273\n",
      "SS step: substitute: astronomers, cosine similarity: 0.14253577029799036\n",
      "SS step: substitute: civilians, cosine similarity: 0.1422203219795152\n",
      "SS step: substitute: protestors, cosine similarity: 0.14221847101912993\n",
      "SS step: substitute: respondents, cosine similarity: 0.14220330377935747\n",
      "SS step: substitute: commentators, cosine similarity: 0.14217945775540158\n",
      "SS step: substitute: observers, cosine similarity: 0.1421396411376939\n",
      "SS step: substitute: inhabitants, cosine similarity: 0.1421125423660168\n",
      "SS step: substitute: bystanders, cosine similarity: 0.14210126987989063\n",
      "SS step: substitute: protesters, cosine similarity: 0.14208259213956273\n",
      "SS step: substitute: observes, cosine similarity: 0.1419715059314661\n",
      "SS step: substitute: spectators, cosine similarity: 0.14189229908096596\n",
      "SS step: substitute: observing, cosine similarity: 0.14178531362813226\n",
      "SS step: substitute: researchers, cosine similarity: 0.1417099651698063\n",
      "SS step: substitute: outsiders, cosine similarity: 0.1416948886445476\n",
      "SS step: substitute: observations, cosine similarity: 0.1416435741975391\n",
      "SS step: substitute: viewers, cosine similarity: 0.14163243557867852\n",
      "SS step: substitute: monitors, cosine similarity: 0.14158300124955894\n",
      "SS step: substitute: supporters, cosine similarity: 0.14156980943520303\n",
      "SS step: substitute: observer, cosine similarity: 0.14151913303896474\n",
      "SS step: substitute: participants, cosine similarity: 0.14145965248236816\n",
      "SS step: substitute: analysts, cosine similarity: 0.14142909063076545\n",
      "SS step: substitute: residents, cosine similarity: 0.14141171406869907\n",
      "SS step: substitute: users, cosine similarity: 0.1413367682229562\n",
      "SS step: substitute: followers, cosine similarity: 0.14131381766619291\n",
      "SS step: substitute: witnesses, cosine similarity: 0.1413062150320457\n",
      "SS step: substitute: neighbors, cosine similarity: 0.1411814688172101\n",
      "SS step: substitute: opponents, cosine similarity: 0.14115169498704755\n",
      "SS step: substitute: visitors, cosine similarity: 0.14104176325834503\n",
      "SS step: substitute: eyes, cosine similarity: 0.14065134210955121\n",
      "\n",
      "Sentence: An amateur video showed a young girl who apparently suffered shrapnel wounds in her thigh undergoing treatment in a makeshift Rastan hospital while screaming in pain.\n",
      "Complex word: shrapnel\n",
      "SG step: generated substitutes: ['bullet', 'stab', 'rapnel', 'gunshot', 'torn', 'trauma', 'blast', 'deep', 'raw', 'small', 'severe', 'knife', 'radiation', 'injury', 'the', 'tore', 'internal', 'a', 'similar', 'superficial', 'compression', 'traumatic', 'such', 'thigh', 'inflicted', 'gaping', 'scratch', 'tiny', 'battered', 'some']\n",
      "\n",
      "SS step: substitute: gunshot, cosine similarity: 0.18160841383093235\n",
      "SS step: substitute: superficial, cosine similarity: 0.181368470974992\n",
      "SS step: substitute: traumatic, cosine similarity: 0.18127257547060818\n",
      "SS step: substitute: inflicted, cosine similarity: 0.18123067786298022\n",
      "SS step: substitute: thigh, cosine similarity: 0.1811119624356648\n",
      "SS step: substitute: trauma, cosine similarity: 0.18101926772508165\n",
      "SS step: substitute: gaping, cosine similarity: 0.18096643499443024\n",
      "SS step: substitute: severe, cosine similarity: 0.1809340903951938\n",
      "SS step: substitute: battered, cosine similarity: 0.18082446367865407\n",
      "SS step: substitute: compression, cosine similarity: 0.1808229429509625\n",
      "SS step: substitute: tiny, cosine similarity: 0.18078413025898377\n",
      "SS step: substitute: tore, cosine similarity: 0.1806795218634039\n",
      "SS step: substitute: radiation, cosine similarity: 0.18064805100192474\n",
      "SS step: substitute: knife, cosine similarity: 0.180477729839477\n",
      "SS step: substitute: some, cosine similarity: 0.1804184155938991\n",
      "SS step: substitute: the, cosine similarity: 0.1803781186669153\n",
      "SS step: substitute: injury, cosine similarity: 0.18033941954486055\n",
      "SS step: substitute: a, cosine similarity: 0.18029146665813303\n",
      "SS step: substitute: small, cosine similarity: 0.18001657129736312\n",
      "SS step: substitute: similar, cosine similarity: 0.17998906612743987\n",
      "SS step: substitute: stab, cosine similarity: 0.1798825274323479\n",
      "SS step: substitute: internal, cosine similarity: 0.1797177801182167\n",
      "SS step: substitute: such, cosine similarity: 0.17962498886541542\n",
      "SS step: substitute: torn, cosine similarity: 0.1795431991255993\n",
      "SS step: substitute: blast, cosine similarity: 0.17948481055511445\n",
      "SS step: substitute: bullet, cosine similarity: 0.1794759920196699\n",
      "SS step: substitute: raw, cosine similarity: 0.1794350408740879\n",
      "SS step: substitute: scratch, cosine similarity: 0.17910766199760353\n",
      "SS step: substitute: deep, cosine similarity: 0.17872574691582718\n",
      "SS step: substitute: rapnel, cosine similarity: 0.1732090477430947\n",
      "\n",
      "Sentence: A local witness said a separate group of attackers disguised in burqas — the head-to-toe robes worn by conservative Afghan women — then tried to storm the compound.\n",
      "Complex word: disguised\n",
      "SG step: generated substitutes: ['disguised', 'veiled', 'masked', 'concealed', 'dressed', 'hidden', 'disguise', 'clothed', 'cloaked', 'shrouded', 'posed', 'clad', 'hiding', 'covered', 'wrapped', 'camoufl', 'disgu', 'posing', 'camouflage', 'presented', 'naked', 'trained', 'draped', 'buried', 'framed', 'hid', 'manifested', 'obscured', 'armed', 'smuggled']\n",
      "\n",
      "SS step: substitute: camoufl, cosine similarity: 0.14531923445582184\n",
      "SS step: substitute: cloaked, cosine similarity: 0.14515708578969963\n",
      "SS step: substitute: manifested, cosine similarity: 0.1451181894057073\n",
      "SS step: substitute: obscured, cosine similarity: 0.14507586990772436\n",
      "SS step: substitute: camouflage, cosine similarity: 0.14489315348781484\n",
      "SS step: substitute: draped, cosine similarity: 0.1448550240365282\n",
      "SS step: substitute: disgu, cosine similarity: 0.14482538004480222\n",
      "SS step: substitute: shrouded, cosine similarity: 0.1448086449269015\n",
      "SS step: substitute: smuggled, cosine similarity: 0.14477731602430122\n",
      "SS step: substitute: clothed, cosine similarity: 0.14463293932371835\n",
      "SS step: substitute: masked, cosine similarity: 0.14455770992985734\n",
      "SS step: substitute: concealed, cosine similarity: 0.1445142059833627\n",
      "SS step: substitute: veiled, cosine similarity: 0.14443953470481113\n",
      "SS step: substitute: hiding, cosine similarity: 0.14435577056179572\n",
      "SS step: substitute: disguise, cosine similarity: 0.14432084645539237\n",
      "SS step: substitute: disguised, cosine similarity: 0.14431498732119877\n",
      "SS step: substitute: hid, cosine similarity: 0.14425538322454712\n",
      "SS step: substitute: wrapped, cosine similarity: 0.1441958386705427\n",
      "SS step: substitute: dressed, cosine similarity: 0.14412437541128245\n",
      "SS step: substitute: posing, cosine similarity: 0.1440763404332724\n",
      "SS step: substitute: framed, cosine similarity: 0.1440092771950589\n",
      "SS step: substitute: buried, cosine similarity: 0.14391160457325652\n",
      "SS step: substitute: posed, cosine similarity: 0.1439055063006704\n",
      "SS step: substitute: clad, cosine similarity: 0.14388359172692794\n",
      "SS step: substitute: naked, cosine similarity: 0.14387220921426302\n",
      "SS step: substitute: hidden, cosine similarity: 0.14382826435372156\n",
      "SS step: substitute: presented, cosine similarity: 0.14382679982749963\n",
      "SS step: substitute: trained, cosine similarity: 0.14362298923316766\n",
      "SS step: substitute: armed, cosine similarity: 0.1433682033463469\n",
      "SS step: substitute: covered, cosine similarity: 0.143271650640136\n",
      "\n",
      "Sentence: Syria's Sunni majority is at the forefront of the uprising against Assad, whose minority Alawite sect is an offshoot of Shi'ite Islam.\n",
      "Complex word: offshoot\n",
      "SG step: generated substitutes: ['off', 'extension', 'echo', 'offspring', 'out', 'element', 'outpost', 'imprint', 'opposite', 'example', 'arm', 'affiliate', 'expansion', 'analogue', 'archetype', 'idea', 'off', 'overthrow', 'enemy', 'indication', 'adjunct', 'alternative', 'opposition', 'opponent', 'ideology', 'expression', 'embryo', 'outside', 'imitation', 'offshore']\n",
      "\n",
      "SS step: substitute: archetype, cosine similarity: 0.16777646007949626\n",
      "SS step: substitute: overthrow, cosine similarity: 0.16771060456195197\n",
      "SS step: substitute: embryo, cosine similarity: 0.1674896031450474\n",
      "SS step: substitute: adjunct, cosine similarity: 0.16734910959242605\n",
      "SS step: substitute: offspring, cosine similarity: 0.16724210627852465\n",
      "SS step: substitute: ideology, cosine similarity: 0.16721686331715702\n",
      "SS step: substitute: outpost, cosine similarity: 0.167016315921678\n",
      "SS step: substitute: indication, cosine similarity: 0.16689382641744926\n",
      "SS step: substitute: imprint, cosine similarity: 0.16653422071720234\n",
      "SS step: substitute: analogue, cosine similarity: 0.1664420178700395\n",
      "SS step: substitute: offshore, cosine similarity: 0.1663561798142308\n",
      "SS step: substitute: opponent, cosine similarity: 0.1663257613072099\n",
      "SS step: substitute: imitation, cosine similarity: 0.1662496156031729\n",
      "SS step: substitute: enemy, cosine similarity: 0.1662024977577437\n",
      "SS step: substitute: echo, cosine similarity: 0.16610434583643313\n",
      "SS step: substitute: alternative, cosine similarity: 0.16605680945543866\n",
      "SS step: substitute: idea, cosine similarity: 0.1660319801295645\n",
      "SS step: substitute: expansion, cosine similarity: 0.16602387328011103\n",
      "SS step: substitute: affiliate, cosine similarity: 0.16601893603375698\n",
      "SS step: substitute: out, cosine similarity: 0.1659717933997965\n",
      "SS step: substitute: expression, cosine similarity: 0.16588078288881497\n",
      "SS step: substitute: example, cosine similarity: 0.16579340561750217\n",
      "SS step: substitute: outside, cosine similarity: 0.16579305050285476\n",
      "SS step: substitute: off, cosine similarity: 0.16575766248268498\n",
      "SS step: substitute: off, cosine similarity: 0.16575766248268498\n",
      "SS step: substitute: element, cosine similarity: 0.16565607330335694\n",
      "SS step: substitute: extension, cosine similarity: 0.16561520904462879\n",
      "SS step: substitute: opposition, cosine similarity: 0.165468200562986\n",
      "SS step: substitute: opposite, cosine similarity: 0.16534894152079457\n",
      "SS step: substitute: arm, cosine similarity: 0.16527856382008727\n",
      "\n",
      "Sentence: Although not as rare in the symphonic literature as sharper keys , examples of symphonies in A major are not as numerous as for D major or G major .\n",
      "Complex word: symphonic\n",
      "SG step: generated substitutes: ['musical', 'music', 'classical', 'literary', 'sonic', 'Greek', 'educational', 'harmonic', 'same', 'the', 'instrumental', 'English', 'academic', 'lyric', 'epic', 'esoteric', 'Classical', 'composition', 'jazz', 'major', 'pop', 'modern', 'Italian', 'formal', 'Japanese', 'onic', 'conceptual', 'verbal', 'Renaissance', 'common']\n",
      "\n",
      "SS step: substitute: esoteric, cosine similarity: 0.13363668649841212\n",
      "SS step: substitute: harmonic, cosine similarity: 0.13324342540871026\n",
      "SS step: substitute: literary, cosine similarity: 0.13289071794611892\n",
      "SS step: substitute: conceptual, cosine similarity: 0.1328553345134385\n",
      "SS step: substitute: Classical, cosine similarity: 0.1327743899672882\n",
      "SS step: substitute: classical, cosine similarity: 0.13270705116516884\n",
      "SS step: substitute: lyric, cosine similarity: 0.13269332680378987\n",
      "SS step: substitute: instrumental, cosine similarity: 0.13249740662609072\n",
      "SS step: substitute: jazz, cosine similarity: 0.13240834351652564\n",
      "SS step: substitute: educational, cosine similarity: 0.13239537561882403\n",
      "SS step: substitute: composition, cosine similarity: 0.1323894616327955\n",
      "SS step: substitute: sonic, cosine similarity: 0.13234672866687633\n",
      "SS step: substitute: verbal, cosine similarity: 0.13222058280754395\n",
      "SS step: substitute: Japanese, cosine similarity: 0.13218047262132093\n",
      "SS step: substitute: Renaissance, cosine similarity: 0.13209176854203342\n",
      "SS step: substitute: Italian, cosine similarity: 0.13206053375701562\n",
      "SS step: substitute: academic, cosine similarity: 0.1320570205560171\n",
      "SS step: substitute: the, cosine similarity: 0.13203915986670914\n",
      "SS step: substitute: formal, cosine similarity: 0.1320327735254674\n",
      "SS step: substitute: epic, cosine similarity: 0.13200427542600773\n",
      "SS step: substitute: musical, cosine similarity: 0.1319427625859485\n",
      "SS step: substitute: Greek, cosine similarity: 0.13187674708752653\n",
      "SS step: substitute: music, cosine similarity: 0.13172638971825373\n",
      "SS step: substitute: modern, cosine similarity: 0.13164274376892487\n",
      "SS step: substitute: major, cosine similarity: 0.13158809822139494\n",
      "SS step: substitute: English, cosine similarity: 0.13143475004721128\n",
      "SS step: substitute: same, cosine similarity: 0.13140341354148052\n",
      "SS step: substitute: common, cosine similarity: 0.1312564501780906\n",
      "SS step: substitute: pop, cosine similarity: 0.1311848155837737\n",
      "SS step: substitute: onic, cosine similarity: 0.12741949103099215\n",
      "\n",
      "Sentence: That prompted the military to deploy its largest warship, the BRP Gregorio del Pilar, which was recently acquired from the United States.\n",
      "Complex word: deploy\n",
      "SG step: generated substitutes: ['deploy', 'employ', 'mobilize', 'deployed', 'utilize', 'deployment', 'deploying', 'ploy', 'equip', 'dispatch', 'activate', 'use', 'Deploy', 'possess', 'cultivate', 'unveil', 'unleash', 'wield', 'manufacture', 'summon', 'populate', 'expend', 'install', 'devote', 'propel', 'Deploy', 'spawn', 'acquire', 'recruit', 'operate']\n",
      "\n",
      "SS step: substitute: mobilize, cosine similarity: 0.1882105242099317\n",
      "SS step: substitute: propel, cosine similarity: 0.18817017098797367\n",
      "SS step: substitute: cultivate, cosine similarity: 0.18804743571047167\n",
      "SS step: substitute: ploy, cosine similarity: 0.18800521894816993\n",
      "SS step: substitute: unveil, cosine similarity: 0.1879490022930187\n",
      "SS step: substitute: Deploy, cosine similarity: 0.18793378837874986\n",
      "SS step: substitute: Deploy, cosine similarity: 0.18793378837874986\n",
      "SS step: substitute: expend, cosine similarity: 0.18765410865897292\n",
      "SS step: substitute: manufacture, cosine similarity: 0.18764172014976768\n",
      "SS step: substitute: activate, cosine similarity: 0.1875977594182552\n",
      "SS step: substitute: utilize, cosine similarity: 0.18755559585586346\n",
      "SS step: substitute: wield, cosine similarity: 0.187540414611218\n",
      "SS step: substitute: deploying, cosine similarity: 0.1875261850702332\n",
      "SS step: substitute: unleash, cosine similarity: 0.18746654999737428\n",
      "SS step: substitute: populate, cosine similarity: 0.18740917010917946\n",
      "SS step: substitute: deployed, cosine similarity: 0.18737925679599424\n",
      "SS step: substitute: possess, cosine similarity: 0.18737394989285683\n",
      "SS step: substitute: acquire, cosine similarity: 0.18735023273927542\n",
      "SS step: substitute: deployment, cosine similarity: 0.1871939027183305\n",
      "SS step: substitute: install, cosine similarity: 0.18714003075962937\n",
      "SS step: substitute: operate, cosine similarity: 0.1871217481960883\n",
      "SS step: substitute: employ, cosine similarity: 0.18710655434061416\n",
      "SS step: substitute: recruit, cosine similarity: 0.18710044187285566\n",
      "SS step: substitute: summon, cosine similarity: 0.1870464981896777\n",
      "SS step: substitute: devote, cosine similarity: 0.18703331776948215\n",
      "SS step: substitute: deploy, cosine similarity: 0.18696279202302643\n",
      "SS step: substitute: dispatch, cosine similarity: 0.18663681520559874\n",
      "SS step: substitute: equip, cosine similarity: 0.186624956573674\n",
      "SS step: substitute: spawn, cosine similarity: 0.18654037149245242\n",
      "SS step: substitute: use, cosine similarity: 0.1860961854361694\n",
      "\n",
      "Sentence: #35-14 UK police were expressly forbidden, at a ministerial level, to provide any assistance to Thai authorities as the case involves the death penalty.\n",
      "Complex word: authorities\n",
      "SG step: generated substitutes: ['authorities', 'Authorities', 'officials', 'authority', 'police', 'ís', 'Authorities', 'superiors', 'administrators', 'entities', 'officers', 'residents', 'counterparts', 'individuals', 'prosecutors', ',', 'jurisdictions', 'forces', 'elders', 'cops', 'regimes', 'masters', 'bureaucrats', 'elites', 'Iranians', 'respondents', 'ians', 'investigators', 'ese', 'constituents']\n",
      "\n",
      "SS step: substitute: bureaucrats, cosine similarity: 0.19994469312583713\n",
      "SS step: substitute: elites, cosine similarity: 0.1996703873176552\n",
      "SS step: substitute: Authorities, cosine similarity: 0.19938822459400574\n",
      "SS step: substitute: Authorities, cosine similarity: 0.19938822459400574\n",
      "SS step: substitute: superiors, cosine similarity: 0.19938421405463821\n",
      "SS step: substitute: entities, cosine similarity: 0.1993692232042122\n",
      "SS step: substitute: respondents, cosine similarity: 0.1993611337006211\n",
      "SS step: substitute: administrators, cosine similarity: 0.19927797736353517\n",
      "SS step: substitute: jurisdictions, cosine similarity: 0.19925821794418536\n",
      "SS step: substitute: cops, cosine similarity: 0.19924488844484062\n",
      "SS step: substitute: Iranians, cosine similarity: 0.19917198536214073\n",
      "SS step: substitute: constituents, cosine similarity: 0.19898188352340215\n",
      "SS step: substitute: officers, cosine similarity: 0.1989103990379399\n",
      "SS step: substitute: prosecutors, cosine similarity: 0.19889455526811514\n",
      "SS step: substitute: individuals, cosine similarity: 0.19889357788075088\n",
      "SS step: substitute: investigators, cosine similarity: 0.1988818387244299\n",
      "SS step: substitute: regimes, cosine similarity: 0.19882564805803885\n",
      "SS step: substitute: elders, cosine similarity: 0.19878190749346764\n",
      "SS step: substitute: officials, cosine similarity: 0.19868444194148313\n",
      "SS step: substitute: residents, cosine similarity: 0.19862812867639373\n",
      "SS step: substitute: authorities, cosine similarity: 0.1984950170590921\n",
      "SS step: substitute: counterparts, cosine similarity: 0.19822818838227102\n",
      "SS step: substitute: ,, cosine similarity: 0.1981977404295709\n",
      "SS step: substitute: police, cosine similarity: 0.19801801225804222\n",
      "SS step: substitute: forces, cosine similarity: 0.1979352563193083\n",
      "SS step: substitute: masters, cosine similarity: 0.1979302338135101\n",
      "SS step: substitute: authority, cosine similarity: 0.19761951061777933\n",
      "SS step: substitute: ians, cosine similarity: 0.19159203782242096\n",
      "SS step: substitute: ese, cosine similarity: 0.1915621716296023\n",
      "SS step: substitute: ís, cosine similarity: 0.18761315851281354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with ranking on cos sim scores\n",
    "\n",
    "# using sklearn cosine similarity with padded sentence lengths to account for different lengths due to substitutes\n",
    "# problem: values are very close to each other, probably the cosine similarity of both sentences is calculated. And a complex word that is the same as its substitute did not get a value near 1.\n",
    "\n",
    "\n",
    "# set the maximum length for both sentences\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, \"<mask>\")\n",
    "\n",
    "    ## concatenate the sentence with the complex word and the sentence with the masked word, by using RoBERTa's separator token to create one string of both sentences\n",
    "    sentences_concat = f\"{sentence} {tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## tokenize the concatenated sentence\n",
    "    sentences_concat_tokenized = tokenizer.encode(sentences_concat, add_special_tokens=True)\n",
    "\n",
    "    ## make sure the input length is not longer than max_length\n",
    "    if len(sentences_concat_tokenized) > max_length:\n",
    "        sentences_concat_tokenized = sentences_concat_tokenized[:max_length]\n",
    "\n",
    "    ## pad the tokenized sentence to max_length\n",
    "    padding_length = max_length - len(sentences_concat_tokenized)\n",
    "    sentences_concat_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    ## convert the tokenized sentence to a tensor\n",
    "    sentences_concat_tokenized = torch.tensor(sentences_concat_tokenized)\n",
    "\n",
    "    ## find the masked word in the tokenized sentence\n",
    "    mask_location = torch.where(sentences_concat_tokenized == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "    ## generate predictions for the masked word (forward pass not needed for predictions, only for training)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentences_concat_tokenized.unsqueeze(0))\n",
    "        predictions = outputs.logits[0]\n",
    "\n",
    "    ## get the top-k substitutes based on the predicted logits\n",
    "    top_k = 30\n",
    "    top_tokens = torch.topk(predictions[mask_location], top_k).indices\n",
    "\n",
    "    ## decode the top-k substitutes and print them\n",
    "    substitutes = [tokenizer.decode(token.item()).strip() for token in top_tokens]\n",
    "    print(f\"SG step: generated substitutes: {substitutes}\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # 2. Substitute Selection (SS): apply cosine similarity, of:\n",
    "    # - The contextualized embedding of the complex word in the context of the original sentence, \n",
    "    # - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\n",
    "\n",
    "    ## get the contextualized embedding of the complex word in the original sentence\n",
    "    with torch.no_grad():\n",
    "        complex_word_tokenized = tokenizer.encode(complex_word, add_special_tokens=True)\n",
    "\n",
    "        ## make sure the input length is not longer than max_length\n",
    "        if len(complex_word_tokenized) > max_length:\n",
    "            complex_word_tokenized = complex_word_tokenized[:max_length]\n",
    "\n",
    "        ## pad the tokenized sentence to max_length\n",
    "        padding_length = max_length - len(complex_word_tokenized)\n",
    "        complex_word_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "        ## convert the tokenized sentence to a tensor\n",
    "        complex_word_tokenized = torch.tensor(complex_word_tokenized)\n",
    "\n",
    "        complex_word_embedding = model.get_input_embeddings()(complex_word_tokenized.unsqueeze(0)).squeeze(0).flatten()\n",
    "        complex_word_embedding_norm = normalize(complex_word_embedding.reshape(1, -1))\n",
    "\n",
    "\n",
    "    ## calculate the cosine similarity between the complex word embedding and each substitute embedding\n",
    "    # Create a list to store the cosine similarity scores of the substitutes\n",
    "    substitute_similarities = []\n",
    "    for substitute in substitutes:\n",
    "        ## replace the complex word in the original sentence with the substitute\n",
    "        sentence_substitute = sentence.replace(complex_word, substitute)\n",
    "\n",
    "        ## tokenize the sentence with the substitute\n",
    "        sentence_substitute_tokenized = tokenizer.encode(sentence_substitute, add_special_tokens=True)\n",
    "\n",
    "        ## make sure the input length is not longer than max_length\n",
    "        if len(sentence_substitute_tokenized) > max_length:\n",
    "            sentence_substitute_tokenized = sentence_substitute_tokenized[:max_length]\n",
    "\n",
    "        ## pad the tokenized sentence to max_length\n",
    "        padding_length = max_length - len(sentence_substitute_tokenized)\n",
    "        sentence_substitute_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "        ## convert the tokenized sentence to a tensor\n",
    "        sentence_substitute_tokenized = torch.tensor(sentence_substitute_tokenized)\n",
    "\n",
    "        ## get the contextualized embedding of the substitute word in the modified sentence\n",
    "        with torch.no_grad():\n",
    "            substitute_embedding = model.get_input_embeddings()(sentence_substitute_tokenized.unsqueeze(0)).squeeze(0).flatten()\n",
    "            substitute_embedding_norm = normalize(substitute_embedding.reshape(1, -1))\n",
    "\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(complex_word_embedding_norm, substitute_embedding_norm)\n",
    "        substitute_similarities.append((substitute, similarity[0][0]))\n",
    "\n",
    "    # Sort the substitutes based on their cosine similarity scores\n",
    "    substitute_similarities_sorted = sorted(substitute_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted substitutes and their cosine similarity scores\n",
    "    for substitute, similarity in substitute_similarities_sorted:\n",
    "        print(f\"SS step: substitute: {substitute}, cosine similarity: {similarity}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c66602-c004-4b02-8e17-e8c2d3c9c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc09961-89be-419b-be69-b0c6fbbe141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "SG step: generated substitutes: ['compulsory', 'mandatory', 'obligatory', 'obliged', 'voluntary', 'uniform', 'required', 'mandated', 'optional', 'dogma', 'forced', 'intrinsic', 'advisable', 'routine', 'forthcoming', 'prerequisite', 'ubiquitous', 'involuntary', 'forcibly', 'feasible', 'compelled', 'indispensable', 'necessary', 'obligated', 'liable', 'habitual', 'enforced', 'universal', 'contingent', 'commonplace']\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 2. Substitute Selection (SS): Apply cosine similarity for:\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# - The contextualized embedding of the complex word in the context of the original sentence,\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m## Step 2.1: Get the contextualized embedding of the complex word in the context of the original sentence\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m     complex_word_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_concat_tokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m## Normalize the complex word embedding\u001b[39;00m\n\u001b[0;32m     70\u001b[0m complex_word_embedding_norm \u001b[38;5;241m=\u001b[39m normalize(complex_word_embedding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# using regex\n",
    "# using sklearn cosine similarity with padded sentence lengths to account for different lengths due to substitutes\n",
    "# trying cosine similarity of the embeddings of complex word vs. embeddings of substitute word.\n",
    "## values are very much the same, a lot lower than above, and still a complex word that is the same as its substitute did not get a value near 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set the maximum length for both sentences\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, \"<mask>\")\n",
    "\n",
    "    ## concatenate the sentence with the complex word and the sentence with the masked word, by using RoBERTa's separator token to create one string of both sentences\n",
    "    sentences_concat = f\"{sentence} {tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## tokenize the concatenated sentence\n",
    "    sentences_concat_tokenized = tokenizer.encode(sentences_concat, add_special_tokens=True)\n",
    "\n",
    "    ## make sure the input length is not longer than max_length\n",
    "    if len(sentences_concat_tokenized) > max_length:\n",
    "        sentences_concat_tokenized = sentences_concat_tokenized[:max_length]\n",
    "\n",
    "    ## pad the tokenized sentence to max_length\n",
    "    padding_length = max_length - len(sentences_concat_tokenized)\n",
    "    sentences_concat_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    ## convert the tokenized sentence to a tensor\n",
    "    sentences_concat_tokenized = torch.tensor(sentences_concat_tokenized)\n",
    "\n",
    "    ## find the masked word in the tokenized sentence\n",
    "    mask_location = torch.where(sentences_concat_tokenized == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "    ## generate predictions for the masked word (no gradients, as forward pass not needed for predictions, only for training)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentences_concat_tokenized.unsqueeze(0))\n",
    "        predictions = outputs.logits[0]\n",
    "\n",
    "    ## get the top-k substitutes based on the predicted logits\n",
    "    top_k = 30\n",
    "    top_tokens = torch.topk(predictions[mask_location], top_k).indices\n",
    "\n",
    "    ## decode the top-k substitutes and print them\n",
    "    substitutes = [tokenizer.decode(token.item()).strip() for token in top_tokens]\n",
    "    print(f\"SG step: generated substitutes: {substitutes}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    # 2. Substitute Selection (SS): Apply cosine similarity for:\n",
    "    # - The contextualized embedding of the complex word in the context of the original sentence,\n",
    "    # - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\n",
    "\n",
    "    ## Step 2.1: Get the contextualized embedding of the complex word in the context of the original sentence\n",
    "    with torch.no_grad():\n",
    "        complex_word_embedding = model(sentences_concat_tokenized.unsqueeze(0)).hidden_states[-1].squeeze(0)\n",
    "\n",
    "    ## Normalize the complex word embedding\n",
    "    complex_word_embedding_norm = normalize(complex_word_embedding.reshape(1, -1))\n",
    "\n",
    "    # Iterate through the substitutes\n",
    "    substitute_similarities = []\n",
    "    for substitute in substitutes:\n",
    "        ## Step 2.2: Replace the complex word with a substitute in the sentence\n",
    "        sentence_substitute = re.sub(r'\\b' + re.escape(complex_word) + r'\\b', substitute, sentence)\n",
    "\n",
    "        ## Step 2.3: Tokenize the sentence with the substitute\n",
    "        sentence_substitute_tokenized = tokenizer.encode(sentence_substitute, add_special_tokens=True)\n",
    "\n",
    "        ## Pad the tokenized sentence to max_length\n",
    "        padding_length_sub = max_length - len(sentence_substitute_tokenized)\n",
    "        sentence_substitute_tokenized += [tokenizer.pad_token_id] * padding_length_sub\n",
    "\n",
    "        ## Convert the tokenized sentence to a tensor\n",
    "        input_ids_sub = torch.tensor(sentence_substitute_tokenized).unsqueeze(0)\n",
    "\n",
    "        ## Step 2.4: Get the embedding of the substitute in the context of the sentence with the substitute\n",
    "        with torch.no_grad():\n",
    "            substitute_embedding = model(input_ids_sub).hidden_states[-1].squeeze(0)\n",
    "\n",
    "        ## Normalize the substitute embedding\n",
    "        substitute_embedding_norm = normalize(substitute_embedding.reshape(1, -1))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(complex_word_embedding_norm, substitute_embedding_norm)\n",
    "        substitute_similarities.append((substitute, similarity[0][0]))\n",
    "\n",
    "    # Sort the substitutes based on their cosine similarity\n",
    "    substitute_similarities = sorted(substitute_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the substitutes ranked on cosine similarity value\n",
    "    for substitute, similarity in substitute_similarities:\n",
    "        print(f\"SS step: substitute: {substitute}, ranked on cosine similarity: {similarity}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41c9eb-e491-4bc0-9909-f1b0d139f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A Spanish government source, however, later said that banks able to cover by themselves losses on their toxic property assets will not be forced to remove them from their books while it will be compulsory for those receiving public help.\n",
      "Complex word: compulsory\n",
      "SG step: generated substitutes: ['compulsory', 'mandatory', 'obligatory', 'obliged', 'voluntary', 'uniform', 'required', 'mandated', 'optional', 'dogma', 'forced', 'intrinsic', 'advisable', 'routine', 'forthcoming', 'prerequisite', 'ubiquitous', 'involuntary', 'forcibly', 'feasible', 'compelled', 'indispensable', 'necessary', 'obligated', 'liable', 'habitual', 'enforced', 'universal', 'contingent', 'commonplace']\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 68\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# 2. Substitute Selection (SS): Apply cosine similarity for:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# - The contextualized embedding of the complex word in the context of the original sentence,\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m## Step 2.1: Get the contextualized embedding of the complex word in the context of the original sentence\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 68\u001b[0m         complex_word_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_concat_tokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m## Normalize the complex word embedding\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     complex_word_embedding_norm \u001b[38;5;241m=\u001b[39m normalize(complex_word_embedding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# ranking on cosine similarity \n",
    "\n",
    "# using regex\n",
    "# using sklearn cosine similarity with padded sentence lengths to account for different lengths due to substitutes\n",
    "# trying cosine similarity of the embeddings of complex word vs. embeddings of substitute word.\n",
    "## values are very much the same, but a lot lower than above, and still a complex word that is the same as its substitute did not get a value near 1.\n",
    "\n",
    "\n",
    "\n",
    "# set the maximum length for both sentences\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, \"<mask>\")\n",
    "\n",
    "    ## concatenate the sentence with the complex word and the sentence with the masked word, by using RoBERTa's separator token to create one string of both sentences\n",
    "    sentences_concat = f\"{sentence} {tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## tokenize the concatenated sentence\n",
    "    sentences_concat_tokenized = tokenizer.encode(sentences_concat, add_special_tokens=True)\n",
    "\n",
    "    ## make sure the input length is not longer than max_length\n",
    "    if len(sentences_concat_tokenized) > max_length:\n",
    "        sentences_concat_tokenized = sentences_concat_tokenized[:max_length]\n",
    "\n",
    "    ## pad the tokenized sentence to max_length\n",
    "    padding_length = max_length - len(sentences_concat_tokenized)\n",
    "    sentences_concat_tokenized += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    ## convert the tokenized sentence to a tensor\n",
    "    sentences_concat_tokenized = torch.tensor(sentences_concat_tokenized)\n",
    "\n",
    "    ## find the masked word in the tokenized sentence\n",
    "    mask_location = torch.where(sentences_concat_tokenized == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "    ## generate predictions for the masked word (no gradients, as forward pass not needed for predictions, only for training)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sentences_concat_tokenized.unsqueeze(0))\n",
    "        predictions = outputs.logits[0]\n",
    "\n",
    "    ## get the top-k substitutes based on the predicted logits\n",
    "    top_k = 30\n",
    "    top_tokens = torch.topk(predictions[mask_location], top_k).indices\n",
    "\n",
    "    ## decode the top-k substitutes and print them\n",
    "    substitutes = [tokenizer.decode(token.item()).strip() for token in top_tokens]\n",
    "    print(f\"SG step: generated substitutes: {substitutes}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    # 2. Substitute Selection (SS): Apply cosine similarity for:\n",
    "# - The contextualized embedding of the complex word in the context of the original sentence,\n",
    "# - and the contextualized embedding of each substitute after replacing the complex word in the original sentence with the substitute word.\n",
    "\n",
    "## Step 2.1: Get the contextualized embedding of the complex word in the context of the original sentence\n",
    "    with torch.no_grad():\n",
    "        complex_word_embedding = model(sentences_concat_tokenized.unsqueeze(0)).hidden_states[-1].squeeze(0)\n",
    "\n",
    "    ## Normalize the complex word embedding\n",
    "    complex_word_embedding_norm = normalize(complex_word_embedding.reshape(1, -1))\n",
    "\n",
    "    # Iterate through the substitutes\n",
    "    substitute_similarities = []\n",
    "    for substitute in substitutes:\n",
    "        ## Step 2.2: Replace the complex word with a substitute in the sentence\n",
    "        sentence_substitute = re.sub(r'\\b' + re.escape(complex_word) + r'\\b', substitute, sentence)\n",
    "\n",
    "        ## Step 2.3: Tokenize the sentence with the substitute\n",
    "        sentence_substitute_tokenized = tokenizer.encode(sentence_substitute, add_special_tokens=True)\n",
    "\n",
    "        ## Pad the tokenized sentence to max_length\n",
    "        padding_length_sub = max_length - len(sentence_substitute_tokenized)\n",
    "        sentence_substitute_tokenized += [tokenizer.pad_token_id] * padding_length_sub\n",
    "\n",
    "        ## Convert the tokenized sentence to a tensor\n",
    "        input_ids_sub = torch.tensor(sentence_substitute_tokenized).unsqueeze(0)\n",
    "\n",
    "        ## Step 2.4: Get the embedding of the substitute in the context of the sentence with the substitute\n",
    "        with torch.no_grad():\n",
    "            substitute_embedding = model(input_ids_sub).hidden_states[-1].squeeze(0)\n",
    "\n",
    "        ## Normalize the substitute embedding\n",
    "        substitute_embedding_norm = normalize(substitute_embedding.reshape(1, -1))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(complex_word_embedding_norm, substitute_embedding_norm)\n",
    "        substitute_similarities.append((substitute, similarity[0][0]))\n",
    "\n",
    "    # Sort the substitutes based on their cosine similarity\n",
    "    substitute_similarities = sorted(substitute_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the substitutes ranked on cosine similarity value\n",
    "    print(\"SS step: Ranked substitutes based on cosine similarity:\")\n",
    "    ranked_substitutes = [sub[0] for sub in substitute_similarities]\n",
    "    print(ranked_substitutes)\n",
    "    print()\n",
    "    \n",
    "    for substitute, similarity in substitute_similarities:\n",
    "        print(f\"SS step: substitute: {substitute}, cosine similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d77ddf-f873-4175-9437-c9e4f1dd7d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
