{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## TEST set: Substitute Ranking (SR) step with hypernym relations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65663d1d-adb2-4a02-a542-2e1d9d99853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dbc521-0099-456e-8f52-437923e652da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ed8a8-6947-400c-8070-148c5c7c36b7",
   "metadata": {},
   "source": [
    "### for model  SS_bsRobertalarge_robertabase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2ae89-efce-4c9c-b9bc-b274c26a7777",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98b0bdd-cd9d-433e-be88-14049e435e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1aShared1first_robertabase exported to csv in path './predictions/test/SR_option1aShared1first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1aShared1first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1aShared1first_robertabase exported to csv in path './predictions/test/SR_option1aShared1first_robertabase.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eb14d-97e3-4304-9dee-c2dfba9ef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1aShared1first_robertabase.tsv --output_file ./output/test/SR_option1aShared1first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49ac3c1b-acf6-4187-9c8c-e725b6fff271",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1aShared1first_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SR_option1aShared1first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.672\n",
    "\n",
    "MAP@3 = 0.4551\n",
    "MAP@5 = 0.3453\n",
    "MAP@10 = 0.2112\n",
    "\n",
    "Potential@3 = 0.8629\n",
    "Potential@5 = 0.9274\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2741\n",
    "Accuracy@2@top_gold_1 = 0.4166\n",
    "Accuracy@3@top_gold_1 = 0.4892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afdab-6fc9-4374-963a-ae454047f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663737d1-38cd-4e67-baef-c7d110d2dbdb",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16999ad8-89ca-4bd2-8e30-0afb31254cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1bShared2first_robertabase exported to csv in path './predictions/test/SR_option1bShared2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1bShared2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1bShared2first_robertabase exported to csv in path './predictions/test/SR_option1bShared2first_robertabase.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a7854-d8ff-42a7-8bc0-f5b12445998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1bShared2first_robertabase.tsv --output_file ./output/test/SR_option1bShared2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a8855f9-256f-4b81-9922-1e674ef40e5e",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1bShared2first_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SR_option1bShared2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6075\n",
    "\n",
    "MAP@3 = 0.4205\n",
    "MAP@5 = 0.3244\n",
    "MAP@10 = 0.2023\n",
    "\n",
    "Potential@3 = 0.8521\n",
    "Potential@5 = 0.9274\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.25\n",
    "Accuracy@2@top_gold_1 = 0.3736\n",
    "Accuracy@3@top_gold_1 = 0.4596\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b993e57-4b9f-4050-9a3c-3a3bd3d07a42",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04db1e47-f676-4c97-b298-f7e2bab06e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1cShared1+2first_robertabase exported to csv in path './predictions/test/SR_option1cShared1+2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1cShared1+2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1cShared1+2first_robertabase exported to csv in path './predictions/test/SR_option1cShared1+2first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379ed3f-34d7-4c30-8b34-a4cffdb5a2a3",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1cShared1+2first_robertabase.tsv --output_file ./output/test/SR_option1cShared1+2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ee41713-aa30-4236-87e5-1a8046fa4ecf",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1cShared1+2first_robertabase.tsv\n",
    "OUTPUT file = ./output/test/SR_option1cShared1+2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6666\n",
    "\n",
    "MAP@3 = 0.4514\n",
    "MAP@5 = 0.3463\n",
    "MAP@10 = 0.211\n",
    "\n",
    "Potential@3 = 0.8575\n",
    "Potential@5 = 0.9354\n",
    "Potential@10 = 0.9677\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2661\n",
    "Accuracy@2@top_gold_1 = 0.4139\n",
    "Accuracy@3@top_gold_1 = 0.4865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38902f5f-12e2-4aec-a07b-c35f56f9f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89950e-6a25-4ae3-b455-3d9c502f60a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5f76b-236e-42f5-b6b8-fb269099bea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ea6076-4989-4bb3-a38a-fbb308e9c972",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_electralarge:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089b4ef-c718-4d23-a47b-12127e159b35",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ea0756-ae9c-4214-803d-b46e820566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1aShared1first_electralarge exported to csv in path './predictions/test/SR_option1aShared1first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1aShared1first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1aShared1first_electralarge exported to csv in path './predictions/test/SR_option1aShared1first_electralarge.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be281437-449d-4e47-8396-2aafbf3e1e02",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1aShared1first_electralarge.tsv --output_file ./output/test/SR_option1aShared1first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a01c2d8b-71f5-4403-88f4-ca1b0e906001",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1aShared1first_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SR_option1aShared1first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6424\n",
    "\n",
    "MAP@3 = 0.4514\n",
    "MAP@5 = 0.3431\n",
    "MAP@10 = 0.2079\n",
    "\n",
    "Potential@3 = 0.8629\n",
    "Potential@5 = 0.9086\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2768\n",
    "Accuracy@2@top_gold_1 = 0.4462\n",
    "Accuracy@3@top_gold_1 = 0.5188\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f8421-f481-4de9-b83e-f48db055e2e8",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b8d4c3-937d-46f5-a920-780b9da7a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1bShared2first_electralarge exported to csv in path './predictions/test/SR_option1bShared2first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1bShared2first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1bShared2first_electralarge exported to csv in path './predictions/test/SR_option1bShared2first_electralarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa8e14-e2c0-47e9-a67f-47400b23c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1bShared2first_electralarge.tsv --output_file ./output/test/SR_option1bShared2first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba5b8187-01ae-47cc-947f-f193625ac607",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1bShared2first_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SR_option1bShared2first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5564\n",
    "\n",
    "MAP@3 = 0.4201\n",
    "MAP@5 = 0.3204\n",
    "MAP@10 = 0.199\n",
    "\n",
    "Potential@3 = 0.8521\n",
    "Potential@5 = 0.8978\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2258\n",
    "Accuracy@2@top_gold_1 = 0.3844\n",
    "Accuracy@3@top_gold_1 = 0.4758\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd0d7-5941-484f-abb9-d99a9c21526e",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5773232-38b7-4c07-9c3e-179a1ac431be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_option1cShared1+2first_electralarge exported to csv in path './predictions/test/SR_option1cShared1+2first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/test/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "#     print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/test/SR_option1cShared1+2first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SR_option1cShared1+2first_electralarge exported to csv in path './predictions/test/SR_option1cShared1+2first_electralarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce885e2d-1b46-4e50-b8ce-1995abc82b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/SR_option1cShared1+2first_electralarge.tsv --output_file ./output/test/SR_option1cShared1+2first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "733e8cbe-2aae-45e7-9bc4-5c97189740e4",
   "metadata": {},
   "source": [
    "========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/SR_option1cShared1+2first_electralarge.tsv\n",
    "OUTPUT file = ./output/test/SR_option1cShared1+2first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6236\n",
    "\n",
    "MAP@3 = 0.4438\n",
    "MAP@5 = 0.3408\n",
    "MAP@10 = 0.2066\n",
    "\n",
    "Potential@3 = 0.8682\n",
    "Potential@5 = 0.9112\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2526\n",
    "Accuracy@2@top_gold_1 = 0.4301\n",
    "Accuracy@3@top_gold_1 = 0.5161\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
