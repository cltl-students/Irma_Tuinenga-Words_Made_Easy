{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations for SG and SS step phase 1, all 6 models, trial dataset.\n",
    " This file corresponds to the results presented in sections 4.1 and 4.2 of my thesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'bert-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60dfb929d6a42c2919aae4e42fec3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IrmaT\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IrmaT\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'bert-base-uncased'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertbase_maskedsentenceonly exported to csv in path './predictions/trial/SG_bertbase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertbase_maskedsentenceonly.tsv --output_file ./output/trial/SG_bertbase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertbase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertbase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.0777\n",
    "MAP@5 = 0.0546\n",
    "MAP@10 = 0.0327\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.3\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertbase exported to csv in path './predictions/trial/SG_bertbase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertbase.tsv --output_file ./output/trial/SG_bertbase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertbase.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertbase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2243\n",
    "MAP@10 = 0.1289\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_bertbase exported to csv in path './predictions/trial/SS_phase1_bertbase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_bertbase.tsv --output_file ./output/trial/SS_phase1_bertbase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_bertbase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_bertbase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2253\n",
    "MAP@10 = 0.1298\n",
    "\n",
    "Potential@3 = 0.6\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'bert-large':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'bert-large-uncased'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertlarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_bertlarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertlarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_bertlarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertlarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertlarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0666\n",
    "MAP@5 = 0.044\n",
    "MAP@10 = 0.0259\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_bertlarge exported to csv in path './predictions/trial/SG_bertlarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_bertlarge.tsv --output_file ./output/trial/SG_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_bertlarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3055\n",
    "MAP@5 = 0.1873\n",
    "MAP@10 = 0.1278\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_bertlarge exported to csv in path './predictions/trial/SS_phase1_bertlarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_bertlarge.tsv --output_file ./output/trial/SS_phase1_bertlarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_bertlarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_bertlarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2026\n",
    "MAP@10 = 0.1355\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'roberta-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'roberta-base'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertabase_maskedsentenceonly exported to csv in path './predictions/trial/SG_robertabase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertabase_maskedsentenceonly.tsv --output_file ./output/trial/SG_robertabase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertabase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertabase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0666\n",
    "MAP@5 = 0.0709\n",
    "MAP@10 = 0.0434\n",
    "\n",
    "Potential@3 = 0.3\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.7\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertabase exported to csv in path './predictions/trial/SG_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertabase.tsv --output_file ./output/trial/SG_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.24\n",
    "MAP@10 = 0.1411\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_robertabase exported to csv in path './predictions/trial/SS_phase1_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_robertabase.tsv --output_file ./output/trial/SS_phase1_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.2449\n",
    "MAP@10 = 0.1546\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'roberta-large':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'roberta-large'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertalarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_robertalarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertalarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_robertalarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertalarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertalarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2999\n",
    "MAP@5 = 0.218\n",
    "MAP@10 = 0.1177\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_robertalarge exported to csv in path './predictions/trial/SG_robertalarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_robertalarge.tsv --output_file ./output/trial/SG_robertalarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_robertalarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_robertalarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2944\n",
    "MAP@5 = 0.2016\n",
    "MAP@10 = 0.1067\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.7\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_robertalarge exported to csv in path './predictions/trial/SS_phase1_robertalarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_robertalarge.tsv --output_file ./output/trial/SS_phase1_robertalarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_robertalarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_robertalarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2944\n",
    "MAP@5 = 0.2056\n",
    "MAP@10 = 0.1159\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.8\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'electra-base':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'google/electra-base-generator'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electrabase_maskedsentenceonly exported to csv in path './predictions/trial/SG_electrabase_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electrabase_maskedsentenceonly.tsv --output_file ./output/trial/SG_electrabase_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electrabase_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_electrabase_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0888\n",
    "MAP@5 = 0.0533\n",
    "MAP@10 = 0.0398\n",
    "\n",
    "Potential@3 = 0.4\n",
    "Potential@5 = 0.4\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.1\n",
    "Accuracy@3@top_gold_1 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electrabase exported to csv in path './predictions/trial/SG_electrabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electrabase.tsv --output_file ./output/trial/SG_electrabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electrabase.tsv\n",
    "OUTPUT file = ./output/trial/SG_electrabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3\n",
    "MAP@5 = 0.233\n",
    "MAP@10 = 0.148\n",
    "\n",
    "Potential@3 = 0.5\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_electrabase exported to csv in path './predictions/trial/SS_phase1_electrabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_electrabase.tsv --output_file ./output/trial/SS_phase1_electrabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_electrabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_electrabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3\n",
    "MAP@5 = 0.233\n",
    "MAP@10 = 0.148\n",
    "\n",
    "Potential@3 = 0.5\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 'electralarge':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, substitutes_df = get_data_and_create_empty_df()\n",
    "\n",
    "model = 'google/electra-large-generator'\n",
    "model_name_str = get_str_for_file_name(model)\n",
    "\n",
    "nlp, lm_tokenizer, lm_model, fill_mask = instantiate_spacy_tokenizer_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electralarge_maskedsentenceonly exported to csv in path './predictions/trial/SG_electralarge_maskedsentenceonly.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, excluding original unmasked sentence (to show the big difference in semantic similarity; see bad eval. scores as well)\n",
    "substitutes_df = substitute_generation_including_noise_removal_excluding_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electralarge_maskedsentenceonly.tsv --output_file ./output/trial/SG_electralarge_maskedsentenceonly.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electralarge_maskedsentenceonly.tsv\n",
    "OUTPUT file = ./output/trial/SG_electralarge_maskedsentenceonly.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.1\n",
    "\n",
    "MAP@3 = 0.0444\n",
    "MAP@5 = 0.0306\n",
    "MAP@10 = 0.0254\n",
    "\n",
    "Potential@3 = 0.2\n",
    "Potential@5 = 0.3\n",
    "Potential@10 = 0.5\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.1\n",
    "Accuracy@3@top_gold_1 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG_electralarge exported to csv in path './predictions/trial/SG_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Generation including noise removal, including original, unmasked sentence (concatenated with masked sentence to get more semantic similar results)\n",
    "# This feature will be used in all subsequent steps/models.\n",
    "substitutes_df = substitute_generation_including_noise_removal_including_original_unmasked(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SG_electralarge.tsv --output_file ./output/trial/SG_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SG_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SG_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2576\n",
    "MAP@10 = 0.1546\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_phase1_electralarge exported to csv in path './predictions/trial/SS_phase1_electralarge.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute Selection phase 1 (removal of: dupl.of complex word + infl.forms of complex word + antonyms of complex word)\n",
    "substitutes_df = substitute_selection_phase_1(data, substitutes_df, lm_tokenizer, fill_mask, model_name_str, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_phase1_electralarge.tsv --output_file ./output/trial/SS_phase1_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_phase1_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_phase1_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2686\n",
    "MAP@10 = 0.1616\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accumulated scores, the best 2 models proceed to SS phase 2 (refer to table 4.4 in section 4.2 in the thesis):\n",
    "1. SS_phase1_robertabase (accum. score 5.1161).\n",
    "2. SS_phase1_electralarge (accum. score 4.9413)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
