{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## trial set: Substitute Ranking (SR) step with CEFR levels:\n",
    "#### Performed on best 3 models after SS step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b67d20-7b08-4c5b-8967-82e833aee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f341d72-bd0a-431f-82d9-8a073b23c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde89e63-5e5e-46f4-b5cc-abe282ed9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map spaCy PoS tags to WordNet PoS tags\n",
    "def map_pos_spacy_wordnet(pos_spacy):\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    return pos_map.get(pos_spacy, wn.NOUN) # default to NOUN if pos_spacy does not exist in the dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7a2d-bd38-4e24-82a2-f56942a528b4",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option2bHyps2first_robertabase (No. 1 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e892ae-2302-44cf-b3e9-8a056d30cd82",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9584832e-ea68-4e27-acc6-d1f6da9ca3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b7e9b-8fa6-4662-aa1a-03216fd6bc5d",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "976c7f4a-2781-4cbd-be67-06f8003be519",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2666\n",
    "MAP@5 = 0.253\n",
    "MAP@10 = 0.1532\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af888b06-2f22-4677-ae56-f35736772843",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cc1099-766b-4e58-a3e9-5792dac5ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08f7fe-2b00-4fa7-942d-77ffe7e649f6",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99144a35-c82c-4534-bcff-a008c7d0740e",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2999\n",
    "MAP@5 = 0.304\n",
    "MAP@10 = 0.1639\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2708c-33c5-46df-9143-86aa638756f8",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f21881-970b-4e66-991d-451700f9edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed32c6-dacf-4417-9098-dca1a237a71a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e59c3d4-287e-4b7f-8980-28a34973e34c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2506\n",
    "MAP@10 = 0.1516\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a62fd6-9ed7-4b5f-941c-2807126b7737",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b17c3-3b0a-4e9f-925e-f2512d0dcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fd602-9386-49b4-944f-fbbed6b37dab",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80911004-e7f8-4464-bae8-cb24ef6e2772",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2388\n",
    "MAP@5 = 0.2503\n",
    "MAP@10 = 0.1498\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42eb75-8f45-4866-9ba7-66d726522757",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7905f849-1c41-429f-89d5-d8fe6239890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no1_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf96e5-e9a2-4da4-b7c1-b9beb97412cf",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95c804ae-278b-41f5-b63a-5f219a841e9d",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2222\n",
    "MAP@5 = 0.2323\n",
    "MAP@10 = 0.1424\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f616167-c318-4cf9-a5ce-d9d6b3fcd128",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option1Synsfirst_robertabase (No. 2 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531ac67-bf07-4c89-8c88-6ea6e6304d2e",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74bcbc3b-a13e-4db5-a7cc-c97fb4f76417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d75955-87e2-4694-bf1e-4cc13548c1c2",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebbf7dd3-5fbc-4190-a22d-046c8b645d92",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2166\n",
    "MAP@5 = 0.1689\n",
    "MAP@10 = 0.1281\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef6c70-b21a-49e3-9194-f1b4498f4e46",
   "metadata": {},
   "source": [
    "## for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3182cbb2-8877-4a24-a035-68b7be162cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cb519-02c0-41a5-9f40-1ef9d9b3ce1a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac470774-b059-41f5-a655-96ca4a873ad9",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2256\n",
    "MAP@10 = 0.1416\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfcbde-63a7-466c-9c50-dbb29900a3fc",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b12723c-3177-42b7-8871-b89a7683136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2cCEFR_efl_weighted_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2cCEFR_efl_weighted_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194f890-6138-4ff4-9caf-883e824e6f71",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a399c20-b120-4d78-abe8-f1716a9f220b",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.2111\n",
    "MAP@5 = 0.1666\n",
    "MAP@10 = 0.1265\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec5807-b19a-4222-9037-92cdcd98c7b2",
   "metadata": {},
   "source": [
    "### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37274071-fb1b-41ea-87aa-9522de899809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "   \n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183f2f9-ab62-4de8-a7d1-3e8c9caa3b3e",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5739ebc6-54c9-4754-8254-20f4fd645aab",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.1888\n",
    "MAP@5 = 0.1663\n",
    "MAP@10 = 0.1247\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.3\n",
    "Accuracy@3@top_gold_1 = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83457260-d8e5-4720-902c-4e1a9d48dd05",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "code includes averages of substitutes not found in the combined CEFR database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837d1adb-bd55-4869-bc8f-5fa4c5926447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    \n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no2_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "# print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f45d-0dda-4f25-8ea4-ce74c595e3d9",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bb9515c-b55e-45ea-8fb5-fe8a8a9f42c8",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.1722\n",
    "MAP@5 = 0.1483\n",
    "MAP@10 = 0.1173\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.2\n",
    "Accuracy@3@top_gold_1 = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c0466-09ab-4b24-a751-8f091e626249",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_robertabase (No. 3 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e47a95-bf30-4b15-9a9c-f5b075bb6dcc",
   "metadata": {},
   "source": [
    "#### for CEFR-J dataset (SR_option2a):\n",
    "If the lemmatized version of the substitute is found in the 'cefrj_all_treebank.tsv' file, and\n",
    "If the POS tag of that word (as listed in 'cefrj_all_treebank.tsv') matches the POS tag of the substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fca0d04-155b-490c-984c-9f1c35f03896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefrj/cefrj_all_treebank.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "    #print(f\"substitutes: {substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "    #print(f\"substitutes_cefr: {substitutes_cefr}\\n\")\n",
    "         \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"Substitute Ranking (SR), option 2: substitutes with cefr level ranked first: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2aCEFR_J_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d20def-64b2-416a-99da-505465c506c9",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2541df8d-0b0d-4b74-a794-c12f6d55b831",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2aCEFR_J_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.22\n",
    "MAP@10 = 0.1466\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.7\n",
    "Accuracy@3@top_gold_1 = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656259c9-8c8b-4c2c-88f7-6b4812ce9386",
   "metadata": {},
   "source": [
    "#### for Uchida et al. (CEFR-LS) dataset (SR_option2b):\n",
    "If the lemmatized version of the substitute is found in the './cefr/uchida_pos.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr/uchida_pos.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f822a1c-269e-4508-af6a-f9bdd3f71571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_ls/uchida_pos.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new TSV file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2bCEFR_ls_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955588b1-4d8a-43cf-973d-e6901edf9ed7",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "565a5d28-0f34-40ce-8dbe-182ef27c1715",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2bCEFR_ls_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3444\n",
    "MAP@5 = 0.2326\n",
    "MAP@10 = 0.1529\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86be84-3a42-4c9a-9ec3-91ee39d1b888",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on most frequent CEFR level (SR_option2c):\n",
    "If the lemmatized version of the substitute is found in the ./cefr_efllex/EFLLex_mostfreq.tsv' file, and\n",
    "If the POS tag of that word (as listed in './cefr_efllex/EFLLex_mostfreq.tsv) matches the POS tag of the original substitute word (as determined by parsing the sentence where the complex word is replaced by the original substitute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e27321-a677-4184-a3d3-6fe6d9b7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_mostfreq.tsv', sep='\\t', header=None, names=['word', 'pos', 'cefr'])\n",
    "\n",
    "# define a mapping from CEFR levels to numerical values\n",
    "cefr_level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "\n",
    "# map the CEFR levels in the dataframe to numerical values using the mapping\n",
    "cefr_df['cefr'] = cefr_df['cefr'].map(cefr_level_mapping)\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in cefrj_all_treebank.tsv AND the POS tag of that word (in cefrj_all_treebank.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['cefr'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "         \n",
    "   \n",
    "\n",
    "    # sort the substitutes based on their CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print(f\"ranked_cefr_subs: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde1cd5-ed4c-4246-a04c-2c1a9c717041",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c2a61ed-3f0f-4ea1-ad61-50f0a45d8b75",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2cCEFR_efl_mostfreq_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.2611\n",
    "MAP@5 = 0.2096\n",
    "MAP@10 = 0.1382\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56de14-b948-41e4-9724-055372c88fdc",
   "metadata": {},
   "source": [
    "#### EFFLEX dataset: based on weighted average across CEFR levels (SR_option2d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "002bb78b-fc32-461e-b082-eb36d97e982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_efllex/EFLLex_weighted.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "    # print(f\"sentence: {sentence}\\n\")\n",
    "    # print(f\"complex_word: {complex_word}\\n\")\n",
    "\n",
    "     # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "    # print(f\"Substitutes with their lemmas: {substitutes_lemmas}\\n\")\n",
    "    \n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in EFLLex_weighted.tsv AND the POS tag of that word (in EFLLex_weighted.tsv) is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            substitutes_cefr.append((original, cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]))\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "\n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    # print (f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2dCEFR_efl_weigthed_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8152afe-4740-4d42-8c86-d70aae3e415c",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a2109b-4842-4bdc-a1f7-01d6a8740052",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2dCEFR_efl_weigthed_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3277\n",
    "MAP@5 = 0.2406\n",
    "MAP@10 = 0.1512\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfb49f-5de4-4c0a-a5ba-7f8fe2c16b23",
   "metadata": {},
   "source": [
    "### all CEFR datasets combined: unique word-postag groups with duplicate CEFR scores have been averaged (SR_option 2e):\n",
    "Code includes:\n",
    "1. average number of substitutes per row not found in the combined CEFR database.\n",
    "2. average cefr levels across all substitutes found in that CEFR database.\n",
    "3. Average CEFR level for complex words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fbe2485-3e56-4c5f-9ddc-99c74839d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase'\n",
      "\n",
      "On average, there were 7.2 substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\n",
      "Average CEFR level over all rows (except level 7): 3.9283364819216544\n",
      "Average CEFR level for complex words: 4.75\n"
     ]
    }
   ],
   "source": [
    "# read the CEFR levels file into a dataframe\n",
    "cefr_df = pd.read_csv('./cefr_all/cefr_all_combined.tsv', sep='\\t', header=None, names=['word', 'pos', 'Weighted CEFR'])\n",
    "\n",
    "# read the predictions file into a dataframe\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions dataframe, map each substitute to its weighted CEFR level, sort them, and save them into a new list\n",
    "predictions_cefr = []\n",
    "count_7 = 0  # counter of substitutes with level 7\n",
    "total_rows = 0  # counter of total number of rows\n",
    "\n",
    "total_cefr_levels = []  # to put all cefr levels across all rows in\n",
    "complex_word_cefr_levels = []  # list to store CEFR levels of the complex words\n",
    "\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    cefr_levels = []  # to put the cefr levels for the current row in\n",
    "    \n",
    "    # replace the complex word in the sentence with the substitute, and parse it to get the pos tag of the substitute\n",
    "    substitute_pos = []\n",
    "    for substitute in substitutes:\n",
    "        replaced_sentence = sentence.replace(complex_word, substitute)\n",
    "        doc = nlp(replaced_sentence)\n",
    "        pos = [token.pos_ for token in doc if token.text == substitute][0]\n",
    "        substitute_pos.append((substitute, pos))\n",
    "    \n",
    "    # get the lemma of the substitute based on its pos tag\n",
    "    substitutes_lemmas = []\n",
    "    for sub_pos in substitute_pos:\n",
    "        substitute, pos_spacy = sub_pos\n",
    "        pos_substitute_wordnet = map_pos_spacy_wordnet(pos_spacy)\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_ if substitute in [token.text for token in doc_substitute] else substitute\n",
    "        substitutes_lemmas.append((substitute, substitute_lemma))\n",
    "\n",
    "    # map each lemmatized substitute to its weighted CEFR level, or to a high number if it doesn't have a CEFR level\n",
    "    substitutes_cefr = []\n",
    "    for original, lemmatized in substitutes_lemmas:\n",
    "        # get the pos of the original substitute by parsing the sentence where the complex word is replaced by the substitute\n",
    "        sub_sentence = sentence.replace(complex_word, original)\n",
    "        sub_pos = dict(pos_tag(word_tokenize(sub_sentence))).get(original)\n",
    "        # if the lemmatized substitute equals a word that is found in './cefr_all/cefr_all_combined.tsv' AND the POS tag of that word (in './cefr_all/cefr_all_combined.tsv') is the same as the POS tag of the substitute:\n",
    "        if lemmatized in cefr_df['word'].values and cefr_df[cefr_df['word'] == lemmatized]['pos'].values[0] == sub_pos:\n",
    "            cefr_level = cefr_df[cefr_df['word'] == lemmatized]['Weighted CEFR'].values[0]\n",
    "            substitutes_cefr.append((original, cefr_level))\n",
    "            cefr_levels.append(cefr_level)\n",
    "        else:\n",
    "            substitutes_cefr.append((original, 7))  # assign a high value if it doesn't have a CEFR level or if pos don't match\n",
    "            count_7 += 1  # add 1 to the counter per substitute with level 7\n",
    "\n",
    "    total_rows += 1  # add 1 to the total number of rows\n",
    "    \n",
    "    \n",
    "    # sort the substitutes based on their weighted CEFR levels\n",
    "    ranked_cefr_subs = sorted(substitutes_cefr, key=lambda x: x[1])\n",
    "    \n",
    "    # print(f\"substitutes_cefr ranked on weighted average: {ranked_cefr_subs}\\n\")\n",
    "    sorted_cefr_levels = sorted(cefr_levels)\n",
    "    # print(f\"sorted_cefr_levels: {sorted_cefr_levels}\\n\")\n",
    "\n",
    "    # append the sorted list of substitutes to the new lists, keeping original form\n",
    "    predictions_cefr.append([sentence, complex_word] + [sub for sub, _ in ranked_cefr_subs])\n",
    "\n",
    "    total_cefr_levels.extend(cefr_levels)  # add cefr levels of the current row to the total cefr levels\n",
    "    \n",
    "    \n",
    "     # get the POS of the complex word and its lemma\n",
    "    complex_word_pos = dict(pos_tag(word_tokenize(sentence))).get(complex_word)\n",
    "    complex_word_lemma = nlp(complex_word)[0].lemma_ if complex_word in [token.text for token in nlp(complex_word)] else complex_word\n",
    "\n",
    "    # get the CEFR level of the complex word and append it to complex_word_cefr_levels\n",
    "    if complex_word_lemma in cefr_df['word'].values and cefr_df[cefr_df['word'] == complex_word_lemma]['pos'].values[0] == complex_word_pos:\n",
    "        complex_word_cefr = cefr_df[cefr_df['word'] == complex_word_lemma]['Weighted CEFR'].values[0]\n",
    "        complex_word_cefr_levels.append(complex_word_cefr)\n",
    "\n",
    "    \n",
    "# create a new dataframe from the new lists and write it to a new tsv file\n",
    "new_df = pd.DataFrame(predictions_cefr)\n",
    "new_df.to_csv('./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"SS_no3_SR_option2eCEFR_all_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase'\\n\")\n",
    "\n",
    "# calculate the average number of substitutes with level 7\n",
    "average_7 = count_7 / total_rows\n",
    "print(f\"On average, there were {average_7} substitutes of the provided 10 substitutes that had not been found in the combined CEFR dataset.\")\n",
    "\n",
    "# calculate average CEFR level over all rows\n",
    "if total_cefr_levels:\n",
    "    average_cefr_level_total = sum(total_cefr_levels) / len(total_cefr_levels)\n",
    "\n",
    "print(f\"Average CEFR level over all rows (except level 7): {average_cefr_level_total}\")\n",
    "\n",
    "# calculate average CEFR level of the complex words\n",
    "if complex_word_cefr_levels:\n",
    "    average_complex_word_cefr = sum(complex_word_cefr_levels) / len(complex_word_cefr_levels)\n",
    "    print(f\"Average CEFR level for complex words: {average_complex_word_cefr}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79407d44-8c29-4535-b463-07d011d65a53",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf8f4a3b-82a5-4f2b-9e0b-ba143f54275f",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option2eCEFR_all_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.7\n",
    "\n",
    "MAP@3 = 0.3333\n",
    "MAP@5 = 0.242\n",
    "MAP@10 = 0.1569\n",
    "\n",
    "Potential@3 = 0.9\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.7\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8cad8-2a2f-4bfb-8e5d-73aeb934babd",
   "metadata": {},
   "source": [
    "### Results:\n",
    "Based on the accumulated scores, the best model per strategy proceeds to the final evaluation on the test set:\n",
    "For the strategy 'CEFR Levels', this is the model based on the combined CEFR dataset: SS_no3_SR_option2eCEFR_all_robertabase (accum. score: 6.0322)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca4c90-41fe-41b0-a85a-10428151a7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
