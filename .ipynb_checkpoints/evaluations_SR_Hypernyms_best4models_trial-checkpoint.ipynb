{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22097079-bddc-4066-bcd6-981db0cde8bc",
   "metadata": {},
   "source": [
    "## trial set: Substitute Ranking (SR) step with hypernym-hyponym relations:\n",
    "#### Performed on best 3 models after SS step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65663d1d-adb2-4a02-a542-2e1d9d99853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97dbc521-0099-456e-8f52-437923e652da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed971dc5-6e74-41fc-9340-c0e79b82197f",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option2bHyps2first_robertabase (No. 1 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b8644-ed00-4149-9dcc-f8c12c4cca9d",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c403be9f-7b80-4122-83b2-9326ed88cd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1aShared1first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no1_SR_option1aShared1first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no1_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1aShared1first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e295e-d97c-4e0f-a794-ac7a44349b52",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option1aShared1first_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option1aShared1first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe69bdb7-05ae-44cf-9177-148e0024f6ff",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option1aShared1first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option1aShared1first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3444\n",
    "MAP@5 = 0.3076\n",
    "MAP@10 = 0.1753\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85fb0a-cb9f-4862-9e2d-ae9e2db740e5",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbcd3f7c-4fd5-4d1c-99ae-4d0bddcbc92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1bShared2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no1_SR_option1bShared2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no1_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1bShared2first_robertabase.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c022aae-afbb-4e67-a567-a6775df2cfbf",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option1bShared2first_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option1bShared2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cdeddf4-6753-4181-b2d6-1eb9caf57b00",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option1bShared2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option1bShared2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2946\n",
    "MAP@10 = 0.1642\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56e612-c047-48ae-82ce-10a9932fe7a0",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46bd8dcf-f304-41af-a022-8f36ca6031db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no1_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option2bHyps2first_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no1_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d993f18-0eac-4efd-ba4e-6bb1f612e82d",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv --output_file ./output/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c213ae40-7da0-4b3f-b11e-d7ff6187cd37",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no1_SR_option1cShared1+2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.3\n",
    "\n",
    "MAP@3 = 0.3277\n",
    "MAP@5 = 0.2976\n",
    "MAP@10 = 0.1703\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a500086-a02b-4050-8438-d84b489f6c77",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option1Synsfirst_robertabase (No. 2 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee60a3-fc80-4f8f-bb3c-65cafba8cb6e",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27d7702-53a0-4bfe-947f-491b26a7791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1aShared1first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no2_SR_option1aShared1first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no2_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1aShared1first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b2263-fa12-48e2-b7ac-38cb88937cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option1aShared1first_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option1aShared1first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99e3f395-00b1-4525-936e-a4449d0e6e0c",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option1aShared1first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option1aShared1first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.3166\n",
    "MAP@5 = 0.224\n",
    "MAP@10 = 0.1412\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f80e79-7cee-4211-9981-753c3d225a2a",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e83295-2bb2-4fd6-accf-80f23e27c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1bShared2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no2_SR_option1bShared2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no2_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1bShared2first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c2da4-357f-48ee-9027-adecc1a0ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option1bShared2first_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option1bShared2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71f9c6be-6666-42ee-a539-275bd0da8984",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option1bShared2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option1bShared2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.3111\n",
    "MAP@5 = 0.2256\n",
    "MAP@10 = 0.147\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77687ef6-b574-4e03-869d-11778a1f6a42",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b7b711-d16d-4ee1-82c2-ff04c1cf353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no2_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option1Synsfirst_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no2_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b78509-0811-4efc-8e13-8c9e475398a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76f334-486d-41f3-9075-d9e3443eff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv --output_file ./output/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44496da7-59af-49ee-893f-1a0140427d82",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no2_SR_option1cShared1+2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.2\n",
    "\n",
    "MAP@3 = 0.2888\n",
    "MAP@5 = 0.2173\n",
    "MAP@10 = 0.1379\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 1.0\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1\n",
    "Accuracy@2@top_gold_1 = 0.4\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd4634-296b-422a-8295-52c65ab45101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b46391-24c8-4269-bcfe-7cfd36ee1d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53ed8a8-6947-400c-8070-148c5c7c36b7",
   "metadata": {},
   "source": [
    "### for model SS_bsRobertalarge_robertabase (No. 3 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2ae89-efce-4c9c-b9bc-b274c26a7777",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98b0bdd-cd9d-433e-be88-14049e435e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1aShared1first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no3_SR_option1aShared1first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no3_SR_option1aShared1first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1aShared1first_robertabase.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eb14d-97e3-4304-9dee-c2dfba9ef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option1aShared1first_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option1aShared1first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49ac3c1b-acf6-4187-9c8c-e725b6fff271",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option1aShared1first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option1aShared1first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2388\n",
    "MAP@5 = 0.2023\n",
    "MAP@10 = 0.1327\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afdab-6fc9-4374-963a-ae454047f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663737d1-38cd-4e67-baef-c7d110d2dbdb",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16999ad8-89ca-4bd2-8e30-0afb31254cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1bShared2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no3_SR_option1bShared2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no3_SR_option1bShared2first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1bShared2first_robertabase.tsv'}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a7854-d8ff-42a7-8bc0-f5b12445998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option1bShared2first_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option1bShared2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a8855f9-256f-4b81-9922-1e674ef40e5e",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option1bShared2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option1bShared2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5\n",
    "\n",
    "MAP@3 = 0.3444\n",
    "MAP@5 = 0.2376\n",
    "MAP@10 = 0.1559\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.7\n",
    "Accuracy@3@top_gold_1 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b993e57-4b9f-4050-9a3c-3a3bd3d07a42",
   "metadata": {},
   "source": [
    "### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04db1e47-f676-4c97-b298-f7e2bab06e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no3_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_robertabase.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no3_SR_option1cShared1+2first_robertabase exported to csv in path './predictions/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379ed3f-34d7-4c30-8b34-a4cffdb5a2a3",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv --output_file ./output/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ee41713-aa30-4236-87e5-1a8046fa4ecf",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv\n",
    "OUTPUT file = ./output/trial/SS_no3_SR_option1cShared1+2first_robertabase.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4\n",
    "\n",
    "MAP@3 = 0.2555\n",
    "MAP@5 = 0.2043\n",
    "MAP@10 = 0.138\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 0.8\n",
    "Potential@10 = 1.0\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.3\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03d035-6e6f-4c9a-8cb9-c38755392280",
   "metadata": {},
   "source": [
    "### for model SS_phase2_option3f_BSrobertalarge_electralarge (No. 4 ranked after SS step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e9275-3527-4234-8bf5-b8734f1c8f1c",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1a:  rank the substitutes that are a 1-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f5f76b-236e-42f5-b6b8-fb269099bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option1aShared1first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1aShared1first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-a, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 1st level hypernyms of the complex word\n",
    "    intersection_1_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_1 = set(complex_word_hypernyms_1_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1:\n",
    "            intersection_1_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are the same as the first level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1a, step c): list of substitutes of which their synsets are NOT the same as the one-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_substitutes + other_substitutes\n",
    "#     print(f\"Substitute Ranking (SR) option 1a, step d): substitutes sorted on whether they belong to a synset that is the same as the first level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no4_SR_option1aShared1first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no4_SR_option1aShared1first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1aShared1first_electralarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d815d1-e54b-4c2b-aa2f-06770be41b8b",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option1aShared1first_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option1aShared1first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a594adf6-3847-487d-b410-bea100c363b1",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option1aShared1first_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option1aShared1first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.6\n",
    "\n",
    "MAP@3 = 0.3388\n",
    "MAP@5 = 0.2763\n",
    "MAP@10 = 0.177\n",
    "\n",
    "Potential@3 = 0.7\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.5\n",
    "Accuracy@3@top_gold_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779909e-d99d-4119-92a4-08473802796f",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1b:  rank the substitutes that are a 2-level up hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89113828-85fd-4c4b-9dbb-54907edde2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option1bShared2first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1bShared2first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-b, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with the 2nd level hypernyms of the complex word\n",
    "    intersection_2_substitutes = []\n",
    "    other_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st level hypernym of the complex word\n",
    "        intersection_2 = set(complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_2:\n",
    "            intersection_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are the same as the second level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_2_substitutes}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR) option 1b, step c): list of substitutes of which their synsets are NOT the same as the second-level hypernym of the complex word '{complex_word}' in Wordnet: {other_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_2_substitutes + other_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1b, step d): substitutes sorted on whether they belong to a synset that is the same as the second level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no4_SR_option1bShared2first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no4_SR_option1bShared2first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1bShared2first_electralarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6e84e-4ea1-4571-831a-84f9f421c4e9",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option1bShared2first_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option1bShared2first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3118adc3-7b34-4956-a3b4-33c960c76e7a",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option1bShared2first_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option1bShared2first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.7\n",
    "\n",
    "MAP@3 = 0.3722\n",
    "MAP@5 = 0.2903\n",
    "MAP@10 = 0.182\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80532cfc-ac54-45d9-baa8-1d4ab9ebcbb5",
   "metadata": {},
   "source": [
    "#### Substitute Ranking option 1c:  rank the substitutes that are either a first level or a second level hypernym of the complex word first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74698449-6edf-42a8-b12b-0f04b59b0e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_no4_SR_option1cShared1+2first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predictions file into a df\n",
    "pred_df = pd.read_csv('./predictions/trial/SS_phase2_option3f_BSrobertalarge_electralarge.tsv', sep='\\t', header=None)\n",
    "\n",
    "# for each row in the predictions df:\n",
    "for index, row in pred_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    complex_word = row[1]\n",
    "    substitutes = row[2:12]\n",
    "\n",
    "    # step a: get the complex word lemma, the complex word synsets, and its first level hypernym\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    complex_word_synsets = wn.synsets(complex_word_lemma)\n",
    "    complex_word_hypernyms_1 = [h for syn in complex_word_synsets for h in syn.hypernyms()]\n",
    "    complex_word_hypernyms_1_lemmas = [lemma for h in complex_word_hypernyms_1 for lemma in h.lemma_names()]\n",
    "    complex_word_hypernyms_2 = [h2 for h1 in complex_word_hypernyms_1 for h2 in h1.hypernyms()]\n",
    "    complex_word_hypernyms_2_lemmas = [lemma for h in complex_word_hypernyms_2 for lemma in h.lemma_names()]\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (1st level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_1_lemmas}\\n\")\n",
    "#     print(f\"Substitute Ranking (SR), option 1-c, step a): complex_word_hypernyms_lemmas (2nd level hypernyms) for complex word '{complex_word}': {complex_word_hypernyms_2_lemmas}\\n\")\n",
    "   \n",
    "    \n",
    "\n",
    "    # step b: get the lemma and synsets of the substitutes, and store the original substitutes with the lemmas and synsets\n",
    "    substitute_lemmas_synsets = []\n",
    "    for substitute in substitutes:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        substitute_synsets = wn.synsets(substitute_lemma)\n",
    "        substitute_lemmas_synsets.append((substitute, substitute_lemma, substitute_synsets)) \n",
    "    # print(f\"Substitute Ranking (SR), option 1-c, step b): substitute lemmas synsets: {substitute_lemmas_synsets}\\n\")\n",
    "       \n",
    "\n",
    "    ## step c: get the intersection of the substitute synsets with: the 1st and the 2nd level hypernyms of the complex word\n",
    "    intersection_1_2_substitutes = []\n",
    "    other_1_2_substitutes = []\n",
    "\n",
    "    for substitute, substitute_lemma, substitute_synsets in substitute_lemmas_synsets:\n",
    "        # get the lemmas of the substitute synsets\n",
    "        substitute_synsets_lemmas = [lemma for syn in substitute_synsets for lemma in syn.lemma_names()] \n",
    "\n",
    "        # check if the substitute belongs to a synset that is the same as the 1st or 2nd level hypernym of the complex word\n",
    "        intersection_1_2 = set(complex_word_hypernyms_1_lemmas + complex_word_hypernyms_2_lemmas).intersection(set(substitute_synsets_lemmas))\n",
    "        if intersection_1_2:\n",
    "            intersection_1_2_substitutes.append(substitute)  # append original substitute\n",
    "        else:\n",
    "            other_1_2_substitutes.append(substitute)  # append original substitute\n",
    "            \n",
    "            \n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are the same as the 1st or the 2nd level hypernyms of the complex word '{complex_word}' in Wordnet: {intersection_1_2_substitutes}\\n\")\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step c): list of substitutes of which their synsets are NOT the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet: {other_1_2_substitutes}\\n\")     \n",
    "\n",
    "      \n",
    "    ## step d: create the final list, by putting the intersection first in the list, appending the list with the other substitutes\n",
    "    final_list = intersection_1_2_substitutes + other_1_2_substitutes\n",
    "    # print(f\"Substitute Ranking (SR) option 1c, step d): substitutes sorted on whether they belong to a synset that is the same as the 1st or the 2nd level hypernym of the complex word '{complex_word}' in Wordnet first:  {final_list}\\n\")\n",
    "\n",
    "    # print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    pred_df.loc[index] = [sentence, complex_word] +  final_list\n",
    "\n",
    "    \n",
    "# export the dataframe to tsv for evaluation\n",
    "pred_df.to_csv(\"./predictions/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"SS_no4_SR_option1cShared1+2first_electralarge exported to csv in path './predictions/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b18f11-a24d-42a9-9c6e-3470d3e91400",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/trial/tsar2022_en_trial_gold_no_noise.tsv --predictions_file ./predictions/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv --output_file ./output/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96f5b854-685a-40f6-9ea9-bacdc2df4fc5",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/trial/tsar2022_en_trial_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv\n",
    "OUTPUT file = ./output/trial/SS_no4_SR_option1cShared1+2first_electralarge.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.7\n",
    "\n",
    "MAP@3 = 0.4055\n",
    "MAP@5 = 0.3033\n",
    "MAP@10 = 0.1892\n",
    "\n",
    "Potential@3 = 0.8\n",
    "Potential@5 = 0.9\n",
    "Potential@10 = 0.9\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.4\n",
    "Accuracy@2@top_gold_1 = 0.6\n",
    "Accuracy@3@top_gold_1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841e376-2a72-49bb-ab4f-78941d852782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ed5e9-404a-4906-a70c-c181817b21a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c72512-3add-4698-8a09-661b9c92a112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df86a26-8e3b-42a3-83f4-9b8f1df18ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b388-c5cd-4fdb-b0a3-20c83a5f6ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43d347-1108-4c00-8b3a-0e2892180c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
