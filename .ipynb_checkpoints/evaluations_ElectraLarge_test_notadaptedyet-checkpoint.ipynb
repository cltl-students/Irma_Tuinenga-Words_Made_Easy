{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0085fb59-3709-4063-b22e-73a011dbb414",
   "metadata": {},
   "source": [
    "#### Evaluations for Electra for the test set (373 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5683d06d-2380-4e93-bd91-fa8787d8f31c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForMaskedLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\transformers\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     33\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\transformers\\dependency_versions_check.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     expand_dims,\n\u001b[0;32m     42\u001b[0m     find_labels,\n\u001b[0;32m     43\u001b[0m     flatten_dict,\n\u001b[0;32m     44\u001b[0m     is_jax_tensor,\n\u001b[0;32m     45\u001b[0m     is_numpy_array,\n\u001b[0;32m     46\u001b[0m     is_tensor,\n\u001b[0;32m     47\u001b[0m     is_tf_tensor,\n\u001b[0;32m     48\u001b[0m     is_torch_device,\n\u001b[0;32m     49\u001b[0m     is_torch_tensor,\n\u001b[0;32m     50\u001b[0m     reshape,\n\u001b[0;32m     51\u001b[0m     squeeze,\n\u001b[0;32m     52\u001b[0m     tensor_size,\n\u001b[0;32m     53\u001b[0m     to_numpy,\n\u001b[0;32m     54\u001b[0m     to_py_obj,\n\u001b[0;32m     55\u001b[0m     transpose,\n\u001b[0;32m     56\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     60\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     send_example_telemetry,\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     89\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     90\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m     torch_version,\n\u001b[0;32m    164\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\transformers\\utils\\generic.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\__init__.py:144\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# NOTE: to be revisited following future namespace cleanup.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# See gh-14454 and gh-15672 for discussion.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\lib\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Private submodules\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# load module names. See https://github.com/networkx/networkx/issues/5838\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_check\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_tricks\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_base\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nanfunctions\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\lib\\index_tricks.py:12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumeric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     asarray, ScalarType, array, alltrue, cumprod, arange, ndim\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumerictypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_common_type, issubdtype\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrixlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmatrixlib\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diff\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ravel_multi_index, unravel_index\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\matrixlib\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Sub-package containing the matrix class and related functions.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defmatrix\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m defmatrix\u001b[38;5;241m.\u001b[39m__all__\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# While not in __all__, matrix_power used to be defined here, so we import\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# it for backward compatibility.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m matrix_power\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_from_string\u001b[39m(data):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\linalg\\__init__.py:73\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m``numpy.linalg``\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# To get sub-modules\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     76\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39m__all__\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\linalg\\linalg.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwodim_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m triu, eye\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _umath_linalg\n\u001b[0;32m     37\u001b[0m array_function_dispatch \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m     38\u001b[0m     overrides\u001b[38;5;241m.\u001b[39marray_function_dispatch, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy.linalg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# read the tsv file\n",
    "filename = \"./data/test/tsar2022_en_test_none_no_noise.tsv\"\n",
    "data = pd.read_csv(filename, sep='\\t', header=None, names=[\"sentence\", \"complex_word\"])\n",
    "\n",
    "# create an empty dataframe to store the top-10 substitutes for evaluation\n",
    "substitutes_df = pd.DataFrame(columns=[\"sentence\", \"complex_word\"] + [f\"substitute_{i+1}\" for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca8ccf-1610-4639-9442-7f0cb211dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc147290-d88e-4c2b-94bc-9e21f266759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used for morphological adjustments in step MA\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbbc20-f729-4e8e-82bf-67373cba2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used when word embeddings are used in step SS\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c75960-7777-4ac2-a607-15725beaedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below is used when Bertscore is used in step SS \n",
    "import bert_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527d02f-e481-4940-ace4-3f3b81786576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display.max_rows option to None to display all rows instead of limiting it to 50 \n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa3af3-857e-4db9-99b4-38c34eff6a23",
   "metadata": {},
   "source": [
    "#### Electra-base-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f19cfd-81d2-4e7a-b664-c070d9df5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer and the model\n",
    "lm_tokenizer = AutoTokenizer.from_pretrained(\"google/electra-large-generator\")\n",
    "lm_model = AutoModelForMaskedLM.from_pretrained(\"google/electra-large-generator\")\n",
    "\n",
    "\n",
    "# Instantiate the fill-mask pipeline with the ELECTRA model\n",
    "fill_mask = pipeline(\"fill-mask\", lm_model, tokenizer = lm_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0727fad-27a5-4bd5-bbbe-247db133f613",
   "metadata": {},
   "source": [
    "#### Substitute Generation and Morphological Adaptation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039b4f1-3fff-4647-9327-188e9d861160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    # print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token) # this is different per model (this code line applies to Electra)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "    \n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "    \n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) \n",
    "                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    \n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "    \n",
    "    \n",
    "    ## d) remove antonyms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word_no_antonym = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        syn = wn.synsets(complex_word_lemma)\n",
    "        if syn:\n",
    "            syn = syn[0]\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms() and lemma.name() == substitute_lemma:\n",
    "                    print(f\"Antonym removed (lemma): {lemma.antonyms()[0].name()}\")\n",
    "                    break\n",
    "            else:\n",
    "                substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "        else:\n",
    "            substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_dupl_complex_word_no_antonym}\\n\")\n",
    "    \n",
    "        \n",
    "    # limit the substitutes to the 10 highest ranked ones for evaluation\n",
    "    top_10_substitutes = substitutes_no_dupl_complex_word_no_antonym[:10]\n",
    "    #print(f\"top-10 substitutes SG and MA: {top_10_substitutes}\\n\")\n",
    "    \n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "    # pad the list with None until it has 10 elements\n",
    "    top_10_substitutes += [None] * (required_for_dataframe - len(top_10_substitutes))\n",
    "\n",
    "    # # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + top_10_substitutes\n",
    "    \n",
    "    \n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/ElectraLarge_SG_MA.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "print(\"ElectraBaseLarge_SG_MA exported to csv in path './predictions/test/ElectraLarge_SG_MA.tsv'}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521ab03-d88c-4212-8a4a-46bc92a3699c",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/ElectraLarge_SG_MA.tsv --output_file ./output/test/ElectraLarge_SG_MA.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "012771d6-44b1-4e15-8cbd-c7a7d90350ef",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/ElectraLarge_SG_MA.tsv\n",
    "OUTPUT file = ./output/test/ElectraLarge_SG_MA.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5483\n",
    "\n",
    "MAP@3 = 0.387\n",
    "MAP@5 = 0.2918\n",
    "MAP@10 = 0.1787\n",
    "\n",
    "Potential@3 = 0.8037\n",
    "Potential@5 = 0.8548\n",
    "Potential@10 = 0.9301\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.258\n",
    "Accuracy@2@top_gold_1 = 0.4112\n",
    "Accuracy@3@top_gold_1 = 0.4811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e64340-9f3e-4bfd-9ec8-ee86ddf13d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87194791-fed3-4237-8357-9b05e0b9690e",
   "metadata": {},
   "source": [
    "#### Substitute Generation, Morphological Adaptation, and Contextualized embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68773cb5-3017-4491-a3d3-bae5e3bf282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates similarity between the original sentence and the sentences with candidate substitutes that were retrieved in the SG step \n",
    "# creates a list with sentences with substitute words filled in (commented out for oversight purposes)\n",
    "\n",
    "\n",
    "def calculate_similarity_scores(sentence, sentence_with_substitutes):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/electra-large-generator\")\n",
    "    tf_model = TFAutoModel.from_pretrained(\"google/electra-large-generator\")\n",
    "\n",
    "    def embed_text(text):\n",
    "        tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "        outputs = tf_model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = tf.nn.l2_normalize(embeddings, axis=1)\n",
    "        return embeddings\n",
    "\n",
    "    original_sentence_embedding = embed_text(sentence)\n",
    "    substitute_sentence_embeddings = embed_text(sentence_with_substitutes)\n",
    "\n",
    "    cosine_similarity = np.inner(original_sentence_embedding, substitute_sentence_embeddings)\n",
    "    similarity_scores = cosine_similarity[0]\n",
    "\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18125eff-6860-452c-918e-f7ce027e6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    # print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token) # this is different per model (this code line applies to Electra)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "    \n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "    \n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) \n",
    "                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "    \n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "    \n",
    "    \n",
    "    ## d) remove antonyms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word_no_antonym = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        syn = wn.synsets(complex_word_lemma)\n",
    "        if syn:\n",
    "            syn = syn[0]\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms() and lemma.name() == substitute_lemma:\n",
    "                    print(f\"Antonym removed (lemma): {lemma.antonyms()[0].name()}\")\n",
    "                    break\n",
    "            else:\n",
    "                substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "        else:\n",
    "            substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_dupl_complex_word_no_antonym}\\n\")\n",
    "       \n",
    "    \n",
    "    \n",
    "     #3: Substitute Selection (SS) by contextualized embeddings and cosine similarity scores:  \n",
    "          \n",
    "    # create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_dupl_complex_word_no_antonym]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "    \n",
    "    \n",
    "    ##  calculate cosine similarity scores, and rank the substitutes based on their similarity score\n",
    "      \n",
    "    \n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        similarity_scores = calculate_similarity_scores(sentence, sentence_with_substitutes)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        embeddings_ranked_substitutes_with_scores = sorted(zip(substitutes_no_dupl_complex_word_no_antonym, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "        embeddings_ranked_substitutes_only = [substitute for substitute, score in embeddings_ranked_substitutes_with_scores]\n",
    "        #print(f\"SS step: Ranked substitutes, based on embedding scores in context: {embeddings_ranked_substitutes_only}\\n\")   \n",
    "        \n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        #print(f\" SS step: top-10 substitutes based on embedding scores in context: {embeddings_top_10_substitutes}\\n\")\n",
    "        embeddings_top_10_substitutes = embeddings_ranked_substitutes_only[:10]\n",
    "    else:\n",
    "        embeddings_top_10_substitutes = []\n",
    "    \n",
    "        \n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "    # pad the list with None until it has 10 elements\n",
    "    embeddings_top_10_substitutes += [None] * (required_for_dataframe - len(embeddings_top_10_substitutes))\n",
    "    \n",
    "    # add the sentence, complex_word, and the substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + embeddings_top_10_substitutes\n",
    "    \n",
    "        \n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/ElectraLarge_SG_MA_SS_ce.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "print(\"ElectraLarge_SG_MA_SS_ce exported to csv in path './predictions/test/ElectraLarge_SG_MA_SS_ce.tsv'}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b874a-0f9e-4cb2-b37c-889337fbc778",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/ElectraLarge_SG_MA_SS_ce.tsv --output_file ./output/test/ElectraLarge_SG_MA_SS_ce.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "037cd769-f7f0-4182-9ca3-6f63efbed39f",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/ElectraLarge_SG_MA_SS_ce.tsv\n",
    "OUTPUT file = ./output/test/ElectraLarge_SG_MA_SS_ce.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.4166\n",
    "\n",
    "MAP@3 = 0.2629\n",
    "MAP@5 = 0.1944\n",
    "MAP@10 = 0.1275\n",
    "\n",
    "Potential@3 = 0.672\n",
    "Potential@5 = 0.7688\n",
    "Potential@10 = 0.9112\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.1693\n",
    "Accuracy@2@top_gold_1 = 0.2715\n",
    "Accuracy@3@top_gold_1 = 0.3602"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc036c5-6a1e-4e71-a184-b2201ac7baf7",
   "metadata": {},
   "source": [
    "#### Substitute Generation, Morphological Adaptation, and BertScore:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34e0321f-543d-4a12-9607-da7b26365895",
   "metadata": {},
   "source": [
    "##### BErtscore based on ELECTRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504ebf4-576b-4370-8345-2492ac65fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token) # this is different per model (this code line applies to Electra)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) \n",
    "                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "    ## d) remove antonyms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word_no_antonym = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        syn = wn.synsets(complex_word_lemma)\n",
    "        if syn:\n",
    "            syn = syn[0]\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms() and lemma.name() == substitute_lemma:\n",
    "                    print(f\"Antonym removed (lemma): {lemma.antonyms()[0].name()}\")\n",
    "                    break\n",
    "            else:\n",
    "                substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "        else:\n",
    "            substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_dupl_complex_word_no_antonym}\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bertscores: \n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_dupl_complex_word_no_antonym]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='google/electra-large-generator', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "    \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_dupl_complex_word_no_antonym, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    ## fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    ## pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "   \n",
    "\n",
    "\n",
    "    ## add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/ElectraLarge_SG_MA_SS_bsElectra.tsv\", sep=\"\\t\", index=False, header=False)  \n",
    "print(\"ElectraLarge_SG_MA_SS_bsElectra exported to csv in path './predictions/test/ElectraLarge_SG_MA_SS_bsElectra.tsv'}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dbfa50-213d-4d26-afce-1df5ce5b539a",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/ElectraLarge_SG_MA_SS_bsElectra.tsv --output_file ./output/test/ElectraLarge_SG_MA_SS_bsElectra.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43ccf33f-6a06-4f66-9c98-1c5557070111",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/ElectraLarge_SG_MA_SS_bsElectra.tsv\n",
    "OUTPUT file = ./output/test/ElectraLarge_SG_MA_SS_bsElectra.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5241\n",
    "\n",
    "MAP@3 = 0.3514\n",
    "MAP@5 = 0.2681\n",
    "MAP@10 = 0.1671\n",
    "\n",
    "Potential@3 = 0.7822\n",
    "Potential@5 = 0.8709\n",
    "Potential@10 = 0.9354\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2204\n",
    "Accuracy@2@top_gold_1 = 0.3118\n",
    "Accuracy@3@top_gold_1 = 0.4112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d089071-4a94-49ff-be81-11f0c004df91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69eb271b-c6c6-4b14-93b8-eb55ce45dbf0",
   "metadata": {},
   "source": [
    "##### BErtscore based on Bert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d19c2-70f8-4f0a-90e7-523b74891dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token) # this is different per model (this code line applies to Electra)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) \n",
    "                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "    ## d) remove antonyms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word_no_antonym = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        syn = wn.synsets(complex_word_lemma)\n",
    "        if syn:\n",
    "            syn = syn[0]\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms() and lemma.name() == substitute_lemma:\n",
    "                    print(f\"Antonym removed (lemma): {lemma.antonyms()[0].name()}\")\n",
    "                    break\n",
    "            else:\n",
    "                substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "        else:\n",
    "            substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_dupl_complex_word_no_antonym}\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_dupl_complex_word_no_antonym]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='bert-large-uncased', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_dupl_complex_word_no_antonym, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/ElectraLarge_SG_MA_SS_bsBERT.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"ElectraLarge_SG_MA_SS_bsBert exported to csv in path './predictions/test/ElectraLarge_SG_MA_SS_bsBert.tsv'}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385aef2-04d1-4a94-a2cd-b0635a8fd616",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/ElectraLarge_SG_MA_SS_bsBERT.tsv --output_file ./output/test/ElectraLarge_SG_MA_SS_bsBERT.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200eac4b-d783-4c11-a56e-61cbf22cc7c6",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/ElectraLarge_SG_MA_SS_bsBERT.tsv\n",
    "OUTPUT file = ./output/test/ElectraLarge_SG_MA_SS_bsBERT.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5994\n",
    "\n",
    "MAP@3 = 0.4236\n",
    "MAP@5 = 0.3113\n",
    "MAP@10 = 0.1944\n",
    "\n",
    "Potential@3 = 0.836\n",
    "Potential@5 = 0.9005\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.258\n",
    "Accuracy@2@top_gold_1 = 0.3844\n",
    "Accuracy@3@top_gold_1 = 0.4973"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f0b63-a1f4-4d37-9d3e-6070ccb440c3",
   "metadata": {},
   "source": [
    "#### Bertscore based on Roberta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35948f-3e35-49c8-9c6f-d42899b604cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, for each complex word: \n",
    "for index, row in data.iterrows():\n",
    "       \n",
    "    # 1. Substitute Generation (SG): perform masking and generate substitutes:\n",
    "    \n",
    "    ## print the sentence and the complex word\n",
    "    sentence, complex_word = row[\"sentence\"], row[\"complex_word\"]\n",
    "    #print(f\"Sentence: {sentence}\")\n",
    "    #print(f\"Complex word: {complex_word}\")\n",
    "\n",
    "    ## in the sentence, replace the complex word with a masked word\n",
    "    sentence_masked_word = sentence.replace(complex_word, lm_tokenizer.mask_token) # this is different per model (this code line applies to Electra)\n",
    "\n",
    "    ## concatenate the original sentence and the masked sentence\n",
    "    sentences_concat = f\"{sentence} {lm_tokenizer.sep_token} {sentence_masked_word}\"\n",
    "\n",
    "    ## generate and rank candidate substitutes for the masked word using the fill_mask pipeline (removing elements without token_str key; as this gave errors in the ELECTRA models) .\n",
    "    top_k = 30\n",
    "    result = fill_mask(sentences_concat, top_k=top_k)\n",
    "    substitutes = [substitute[\"token_str\"] for substitute in result if \"token_str\" in substitute]\n",
    "    #print(f\"Substitute Generation step: initial substitute list: {substitutes}\\n\")\n",
    "\n",
    "\n",
    "    #2: Morphological Generation and Context Adaptation (Morphological Adaptation):  \n",
    "    ## a) remove noise in the substitutes, by ignoring generated substitutes that are empty or that have unwanted punctuation characters or that start with '##' (this returned errors with the ELECTRA model), and lowercase the substitutes (as some models don't lowercase by default)\n",
    "    ## and lowercase all substitutes. Use try/except statement to prevent other character-related problems to happen\n",
    "\n",
    "    punctuation_set = set(string.punctuation) - set('-') # retained hyphens in case tokenizers don't split on hyphenated compounds\n",
    "    punctuation_set.update({'“','”'})   # as these curly quotes appeared in the Electra (SG step) results but were not part of the string set\n",
    "\n",
    "    try:\n",
    "        substitutes = [substitute[\"token_str\"].lower() for substitute in result if not any(char in punctuation_set for char in substitute[\"token_str\"]) \n",
    "                       and not substitute[\"token_str\"].startswith('##') and substitute[\"token_str\"].strip() != \"\"]\n",
    "        # print(f\"Morphological Adaptation step a): substitute list without unwanted punctuation characters: {substitutes}\\n\")\n",
    "    except TypeError as error:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ## b) remove duplicates within the substitute list from the substitute list (duplicates are likely for models that did not lowercase by default)\n",
    "    ## the last mentioned duplicate is removed on purpose, as this may probably be the (previously) uppercased variant of the lowercased substitute (lowercased subs are most likely higher ranked by the model)\n",
    "    substitutes_no_dupl = []\n",
    "    for sub in substitutes:\n",
    "        if sub not in substitutes_no_dupl:\n",
    "            substitutes_no_dupl.append(sub)\n",
    "    #print(f\"Morphological Adaptation step b): substitute list without duplicates of substitutes: {substitutes_no_dupl}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    ## c) remove duplicates and inflected forms of the complex word from the substitute list\n",
    "\n",
    "    ## first Lemmatize the complex word with spaCy, in order to compare it with the lemmatized substitute later to see if their mutual lemmas are the same\n",
    "    doc_complex_word = nlp(complex_word)\n",
    "    complex_word_lemma = doc_complex_word[0].lemma_\n",
    "    #print(f\"complex_word_lemma for complex word '{complex_word}': {complex_word_lemma}\\n\")\n",
    "\n",
    "\n",
    "    ## then, remove duplicates and inflected forms of the complex word from the list with substitutes\n",
    "    substitutes_no_dupl_complex_word = []\n",
    "    for substitute in substitutes_no_dupl:\n",
    "        doc_substitute = nlp(substitute)\n",
    "        substitute_lemma = doc_substitute[0].lemma_\n",
    "        if substitute_lemma != complex_word_lemma:\n",
    "            substitutes_no_dupl_complex_word.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step c): substitute list without duplicates of the complex word nor inflected forms of the complex word: {substitutes_no_dupl_complex_word}\\n\")\n",
    "\n",
    "\n",
    "    ## d) remove antonyms of the complex word from the substitute list\n",
    "    substitutes_no_dupl_complex_word_no_antonym = []\n",
    "    for substitute in substitutes_no_dupl_complex_word:\n",
    "        syn = wn.synsets(complex_word_lemma)\n",
    "        if syn:\n",
    "            syn = syn[0]\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms() and lemma.name() == substitute_lemma:\n",
    "                    print(f\"Antonym removed (lemma): {lemma.antonyms()[0].name()}\")\n",
    "                    break\n",
    "            else:\n",
    "                substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "        else:\n",
    "            substitutes_no_dupl_complex_word_no_antonym.append(substitute)\n",
    "    #print(f\"Morphological Adaptation step d): substitute list without antonyms of the complex word: {substitutes_no_dupl_complex_word_no_antonym}\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3: Substitute Selection (SS) by calculating Bert scores: \n",
    "\n",
    "    ## create sentence with the complex word replaced by the substitutes\n",
    "    sentence_with_substitutes = [sentence.replace(complex_word, sub) for sub in substitutes_no_dupl_complex_word_no_antonym]\n",
    "    #print(f\"List with sentences where complex word is substituted: {sentence_with_substitutes}\\n\")\n",
    "\n",
    "\n",
    "    ## calculate BERTScores, and rank the substitutes based on these scores\n",
    "    if len(sentence_with_substitutes) > 0: # to make sure the list with substitutes is always filled\n",
    "        logging.getLogger('transformers').setLevel(logging.ERROR)  # to prevent the same warnings from being printed x times \n",
    "        scores = bert_score.score([sentence]*len(sentence_with_substitutes), sentence_with_substitutes, lang=\"en\", model_type='roberta-large', verbose=False)\n",
    "        logging.getLogger('transformers').setLevel(logging.WARNING) # to reset the logging level back to printing warnings\n",
    "        \n",
    "        # create a list of tuples, each tuple containing a substitute and its score\n",
    "        substitute_score_pairs = list(zip(substitutes_no_dupl_complex_word_no_antonym, scores[0].tolist()))\n",
    "\n",
    "        # sort the list of tuples by the scores (the second element of each tuple), in descending order\n",
    "        sorted_substitute_score_pairs = sorted(substitute_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # # print each substitute with its score\n",
    "        # for substitute, score in sorted_substitute_score_pairs:\n",
    "        #     print(f\"Substitute: {substitute}, BertScore: {score}\")\n",
    "\n",
    "        # extract the list of substitutes from the sorted pairs\n",
    "        bertscore_ranked_substitutes_only = [substitute for substitute, _ in sorted_substitute_score_pairs]\n",
    "        #print(f\"substitutes based on bertscores in context: {bertscore_ranked_substitutes_only}\\n\")\n",
    "\n",
    "        # limit the substitutes to the 10 first ones for evaluation\n",
    "        bertscore_top_10_substitutes = bertscore_ranked_substitutes_only[:10]\n",
    "        #print(f\"top-10 substitutes based on bertscores in context: {bertscore_top_10_substitutes}\\n\")\n",
    "\n",
    "    else:\n",
    "        bertscore_top_10_substitutes = []\n",
    "\n",
    "\n",
    "    ## add the results to the dataframe\n",
    "    # fill the dataframe with 10 elements even if there are less than 10 in the previous list\n",
    "    required_for_dataframe = 10\n",
    "\n",
    "    # pad the list with None until it has 10 elements\n",
    "    bertscore_top_10_substitutes += [None] * (required_for_dataframe - len(bertscore_top_10_substitutes))\n",
    "  \n",
    "\n",
    "\n",
    "    # add the sentence, complex_word, and substitutes to the dataframe \n",
    "    substitutes_df.loc[index] = [sentence, complex_word] + bertscore_top_10_substitutes\n",
    "\n",
    "    #print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "   \n",
    "    \n",
    "    \n",
    "# export the dataframe to a tsv file for evaluation\n",
    "substitutes_df.to_csv(\"./predictions/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"ElectraLarge_SG_MA_SS_bsRoberta exported to csv in path './predictions/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv'}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee89f8-c479-45eb-9a3e-d5cef6f3dec2",
   "metadata": {},
   "source": [
    "python tsar_eval.py --gold_file ./data/test/tsar2022_en_test_gold_no_noise.tsv --predictions_file ./predictions/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv --output_file ./output/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e71b4dd7-06b5-4b76-807d-585be11664a4",
   "metadata": {},
   "source": [
    "=========   EVALUATION config.=========\n",
    "GOLD file = ./data/test/tsar2022_en_test_gold_no_noise.tsv\n",
    "PREDICTION LABELS file = ./predictions/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv\n",
    "OUTPUT file = ./output/test/ElectraLarge_SG_MA_SS_bsRoberta.tsv\n",
    "===============   RESULTS  =============\n",
    "\n",
    "MAP@1/Potential@1/Precision@1 = 0.5967\n",
    "\n",
    "MAP@3 = 0.4277\n",
    "MAP@5 = 0.3262\n",
    "MAP@10 = 0.2009\n",
    "\n",
    "Potential@3 = 0.8467\n",
    "Potential@5 = 0.8978\n",
    "Potential@10 = 0.9462\n",
    "\n",
    "Accuracy@1@top_gold_1 = 0.2607\n",
    "Accuracy@2@top_gold_1 = 0.4139\n",
    "Accuracy@3@top_gold_1 = 0.4865\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c506071-4e81-451a-b203-609abda5a06e",
   "metadata": {},
   "source": [
    "#### Conclusion (based on Map1):\n",
    "1.ElectraLarge_SG_MA_SS_bsBERT (0.5994) # highest (slightly) MAP1 score of all electramodels, also on other scores\n",
    "2.ElectraLarge_SG_MA_SS_bsRoberta (0.5967)\n",
    "3.ElectraLarge_SG_MA (0.5483)\n",
    "4.ElectraLarge_SG_MA_SS_bsElectra (0.5241)\n",
    "5.ElectraLarge_SG_MA_SS_ce (0.4166)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
